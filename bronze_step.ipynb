{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "from imports import *\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Available Stations ID List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all stations and some metadata as a Pandas DataFrame\n",
    "stations_df = api.stations()\n",
    "# parse the response as a dictionary\n",
    "stations_df = api.stations(as_df=True)\n",
    "\n",
    "print(len(stations_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Buoys by Remarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_error_url_list = []\n",
    "\n",
    "# Liste de mots √† rechercher dans la colonne \"Remark\"\n",
    "blacklist = [\"Failure\", \"ceased\", \"failed\", \"recovered\", \"stopped\", 'adrift']\n",
    "stations_id_set = set()\n",
    "\n",
    "print(f'Avant Filtre: {stations_df.shape[0]}')\n",
    "\n",
    "# Liste pour collecter les indices √† supprimer\n",
    "indices_a_supprimer = []\n",
    "\n",
    "# Parcours des lignes de la DataFrame\n",
    "for idx, row in stations_df.iterrows():\n",
    "    station_id = row[\"Station\"]\n",
    "    station_Location = row[\"Hull No./Config and Location\"]  # Extraire la valeur de la cellule pour chaque ligne\n",
    "    \n",
    "    # Extraction du nom de la station si un \")\" est trouv√©\n",
    "    if \")\" in station_Location:\n",
    "        station_name = station_Location.split(')')[1].rstrip(\" )\")  # On enl√®ve l'espace et la parenth√®se en fin de cha√Æne\n",
    "    else:\n",
    "        station_name = station_Location.strip()  # Si pas de \")\", on garde toute la cha√Æne\n",
    "\n",
    "    station_name = station_name.rstrip(\" )\").replace(\"(\", \"\").replace(\")\", \"\").strip()\n",
    "\n",
    "    # Nettoyage final pour enlever toute parenth√®se ou espace en fin de nom\n",
    "    station_name = station_name.rstrip(\" )\")\n",
    "\n",
    "    # V√©rifier si \"Remark\" n'est pas NaN et si un des √©l√©ments de blacklist est dans \"Remark\"\n",
    "    if isinstance(row[\"Remark\"], str) and any(blacklist_word.lower() in row[\"Remark\"].lower() for blacklist_word in blacklist):\n",
    "        # Ajouter l'index √† la liste\n",
    "        indices_a_supprimer.append(idx)\n",
    "        url = get_buoy_url(station_id)\n",
    "        access_error_url_list.append(url)\n",
    "    else:\n",
    "        pass\n",
    "# Supprimer les lignes apr√®s la boucle\n",
    "stations_df.drop(index=indices_a_supprimer, inplace=True)\n",
    "\n",
    "print(f'Apr√®s Filtre: {stations_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Buoys_datas Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire pour stocker les DataFrames, cl√© : ID de la bou√©e, valeur : DataFrame\n",
    "buoy_datas = {}\n",
    "buoy_list = []\n",
    "\n",
    "# Parcours de chaque bou√©e dans stations_df\n",
    "for index, row in stations_df.iterrows():\n",
    "    buoy_id = row['Station']\n",
    "    metadata = get_station_metadata(buoy_id)\n",
    "\n",
    "    # ‚úÖ R√©cup√©rer les donn√©es sous forme de dictionnaire\n",
    "    buoy_info = parse_buoy_json(metadata)\n",
    "\n",
    "    # ‚úÖ Stocker directement les donn√©es dans buoy_datas\n",
    "    buoy_datas[buoy_id] = buoy_info\n",
    "    buoy_list.append(buoy_id)\n",
    "\n",
    "# Affichage du nombre de bou√©es r√©ussies et √©chou√©es\n",
    "print(f\"Nombre de bou√©es trait√©es : {len(buoy_datas)}\\n\")\n",
    "\n",
    "# Afficher le contenu de buoy_datas\n",
    "\n",
    "first_key =next(iter(buoy_datas))\n",
    "first_key\n",
    "buoy_datas[first_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecte de donn√©es marines et m√©t√©os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ D√©marrage du processus\n",
    "print(\"\\nüöÄ D√©marrage du processus de collecte des donn√©es...\\n\")\n",
    "\n",
    "# Initialisation des compteurs\n",
    "marine_data_collected_successfully = marine_data_collected_failed = 0\n",
    "meteo_data_collected_successfully = meteo_data_collected_failed = 0\n",
    "\n",
    "success = False\n",
    "total_stations = stations_df.shape[0]\n",
    "count = 0\n",
    "\n",
    "# üîÑ Parcours des bou√©es / stations\n",
    "for idx, row in stations_df.iterrows():\n",
    "    buoy_id = row[\"Station\"]\n",
    "\n",
    "    ######### üåä MARINE DATA #########\n",
    "    try:\n",
    "        df_marine = NDBC.realtime_observations(buoy_id)\n",
    "        if df_marine is None or df_marine.empty:\n",
    "            marine_data_collected_failed += 1\n",
    "            continue\n",
    "\n",
    "        marine_data_collected_successfully += 1\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur collecte marine {buoy_id}: {e}\")\n",
    "        marine_data_collected_failed += 1\n",
    "        continue\n",
    "\n",
    "    # Ajout des m√©tadonn√©es\n",
    "    try:\n",
    "        buoy_info = buoy_datas.get(buoy_id, {})\n",
    "        Lat, Lon = buoy_info.get('lat_buoy'), buoy_info.get('lon_buoy')\n",
    "        if Lat is None or Lon is None:\n",
    "            raise ValueError(f\"Donn√©es manquantes pour {buoy_id}\")\n",
    "\n",
    "        df_marine['Lat'] = Lat\n",
    "        df_marine['Lon'] = Lon\n",
    "        df_marine['Water_depth'] = buoy_info.get('Water_depth', None)\n",
    "        df_marine.columns = ['Datetime' if 'date' in col.lower() or 'time' in col.lower() else col for col in df_marine.columns]\n",
    "        df_marine['Datetime'] = df_marine['Datetime'].dt.tz_localize(None)\n",
    "\n",
    "        buoy_datas[buoy_id][\"Marine\"] = df_marine\n",
    "\n",
    "        station_zone = safe_get(parse_buoy_json(get_station_metadata(buoy_id)), \"station_zone\")\n",
    "        Bronze_Marine_table_Name = f\"br_{buoy_id}_marine_{station_zone}\".replace('.', '_').replace('-', '_').replace(' ', '_').lower()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur m√©tadonn√©es marine {buoy_id}: {e}\")\n",
    "        marine_data_collected_failed += 1\n",
    "        continue\n",
    "\n",
    "    ######### ‚õÖ METEO DATA #########\n",
    "    try:\n",
    "        df_meteo = meteo_api_request([Lat, Lon])\n",
    "        if df_meteo is None or df_meteo.empty:\n",
    "            meteo_data_collected_failed += 1\n",
    "            continue\n",
    "        \n",
    "        rename_columns(df_meteo, {'date':'Datetime'})\n",
    "        df_meteo.columns = ['Datetime' if 'date' in col.lower() or 'time' in col.lower() else col for col in df_meteo.columns]\n",
    "        df_meteo['Datetime'] = df_meteo['Datetime'].dt.tz_localize(None)\n",
    "    \n",
    "        buoy_datas[buoy_id][\"Meteo\"] = df_meteo\n",
    "        meteo_data_collected_successfully += 1\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur collecte m√©t√©o {buoy_id}: {e}\")\n",
    "        meteo_data_collected_failed += 1\n",
    "        continue\n",
    "\n",
    "# Retirer les bou√©es avec des DataFrames vides ou None\n",
    "buoy_datas = {buoy_id: data for buoy_id, data in buoy_datas.items() \n",
    "              if \"Marine\" in data and data[\"Marine\"] is not None and not data[\"Marine\"].empty\n",
    "              and \"Meteo\" in data and data[\"Meteo\"] is not None and not data[\"Meteo\"].empty}\n",
    "\n",
    "# üîö R√©sum√© final\n",
    "\n",
    "print(\"\\nüìù R√©sum√© final :\")\n",
    "print(f\"üåä Marine - Collecte ‚úÖ {marine_data_collected_successfully} ‚ùå {marine_data_collected_failed}\")\n",
    "print(f\"‚õÖ M√©t√©o - Collecte ‚úÖ {meteo_data_collected_successfully} ‚ùå {meteo_data_collected_failed}\")\n",
    "\n",
    "# Afficher la longueur du dictionnaire (nombre de bou√©es avec des donn√©es valides)\n",
    "print(f\"\\nüìä Nombre de bou√©es avec des donn√©es valides : {len(buoy_datas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_buoys_missing_df_counts(buoy_datas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Enrichment with MetaDatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_not_include = ['lon_buoy', \"lat_buoy\", \"url\"]\n",
    "for buoy_id, value in buoy_datas.items():\n",
    "    print(f\"\\nüîç Traitement de la Station ID: {buoy_id}\")\n",
    "\n",
    "    marine_df = buoy_datas[buoy_id][\"Marine\"]\n",
    "    meteo_df = buoy_datas[buoy_id][\"Meteo\"]\n",
    "\n",
    "    try:\n",
    "        # R√©cup√©rer les m√©tadonn√©es de la station\n",
    "        buoy_metadata = get_station_metadata(buoy_id)\n",
    "        parsed_data = parse_buoy_json(buoy_metadata)\n",
    "\n",
    "        # Mise √† jour du dictionnaire avec les m√©tadonn√©es\n",
    "        data = buoy_datas[buoy_id]\n",
    "        data.update(parsed_data)\n",
    "        \n",
    "        # Ajouter les m√©tadonn√©es comme nouvelles colonnes dans marine_df\n",
    "        if marine_df is not None:\n",
    "            marine_df[\"Station ID\"] = str(buoy_id)\n",
    "            for key, value in parsed_data.items():\n",
    "                # V√©rifier si la cl√© n'est pas dans la liste des exclusions\n",
    "                if key not in list_not_include:\n",
    "                    marine_df[key] = value\n",
    "                    print(f\"‚úÖ Colonne '{key}' ajout√©e au DataFrame de la station {buoy_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur pour la station {buoy_id}: {e}\")\n",
    "\n",
    "# V√©rification de l'ajout des colonnes en prenant un id au hasard\n",
    "station_id = random.choice(list(buoy_datas.keys()))\n",
    "marine_df = buoy_datas[station_id][\"Marine\"]\n",
    "\n",
    "if marine_df is not None:\n",
    "    print(\"\\nColonnes ajout√©es au DataFrame de la station\", station_id)\n",
    "    print(marine_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_buoys_missing_df_counts(buoy_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_marine.columns)\n",
    "display(df_meteo.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_columns_oceanography = [\n",
    "    'wind_direction',             \n",
    "    'wind_speed',                 \n",
    "    'wave_height',                   \n",
    "    'pressure',                   \n",
    "    'air_temperature',            \n",
    "    'water_temperature',          \n",
    "    'Datetime',\n",
    "    'Lat',\n",
    "    'Lon'                 \n",
    "]\n",
    "\n",
    "important_columns_meteorology = [\n",
    "    'temperature_2m',             \n",
    "    'relative_humidity_2m',       \n",
    "    'dew_point_2m',               \n",
    "    'precipitation',              \n",
    "    'pressure_msl',               \n",
    "    'cloud_cover',                \n",
    "    'wind_speed_10m',             \n",
    "    'Datetime'\n",
    "]\n",
    "\n",
    "stations_depart = len(buoy_datas)\n",
    "ignored_buoys = {}  # Dictionary to track ignored buoys and their reasons\n",
    "\n",
    "for station_id, data in buoy_datas.items():\n",
    "    print(f\"\\nüîÑ Nettoyage des donn√©es pour la station {station_id}\")\n",
    "\n",
    "    marine_df = data.get(\"Marine\")\n",
    "    meteo_df = data.get(\"Meteo\")\n",
    "\n",
    "    if marine_df is None or meteo_df is None:\n",
    "        ignored_buoys[station_id] = \"Marine DataFrame ou Meteo DataFrame manquant(e)\"\n",
    "        print(f\"‚ö†Ô∏è Station {station_id} ignor√©e: Marine DataFrame ou Meteo DataFrame manquant(e)\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Nettoyage des DataFrames\n",
    "        cleaned_marine_df = handle_null_values(marine_df)\n",
    "        cleaned_meteo_df = handle_null_values(meteo_df)\n",
    "        # V√©rification des colonnes importantes apr√®s nettoyage\n",
    "        marine_columns_ok = all(col in cleaned_marine_df.columns for col in important_columns_oceanography)\n",
    "        meteo_columns_ok = all(col in cleaned_meteo_df.columns for col in important_columns_meteorology)\n",
    "\n",
    "        # Track which columns are missing\n",
    "        missing_marine_columns = [col for col in important_columns_oceanography if col not in cleaned_marine_df.columns]\n",
    "        missing_meteo_columns = [col for col in important_columns_meteorology if col not in cleaned_meteo_df.columns]\n",
    "\n",
    "        if missing_marine_columns or missing_meteo_columns:\n",
    "            ignored_buoys[station_id] = f\"Colonnes manquantes: Marine: {missing_marine_columns}, Meteo: {missing_meteo_columns}\"\n",
    "            print(f\"‚ö†Ô∏è Station {station_id} ignor√©e: Colonnes manquantes - Marine: {missing_marine_columns}, Meteo: {missing_meteo_columns}\")\n",
    "            continue\n",
    "\n",
    "        # Ajouter le DataFrame nettoy√© au dictionnaire des r√©sultats\n",
    "        buoy_datas[station_id]['Cleaned Marine'] = cleaned_marine_df\n",
    "        buoy_datas[station_id]['Cleaned Meteo'] = cleaned_meteo_df\n",
    "        print(f\"‚úÖ Nettoyage r√©ussi pour la station {station_id} ({cleaned_marine_df.shape[0]} lignes)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        ignored_buoys[station_id] = f\"Erreur lors du nettoyage: {e}\"\n",
    "        print(f\"‚ùå Erreur lors du nettoyage pour {station_id}: {e}\")\n",
    "\n",
    "# üî• Suppression des stations ignor√©es du dictionnaire principal\n",
    "for station_id in ignored_buoys:\n",
    "    buoy_datas.pop(station_id, None)\n",
    "\n",
    "len_cleaned_data = len([data for data in buoy_datas.values() if 'Cleaned Marine' in data and 'Cleaned Meteo' in data])\n",
    "\n",
    "# R√©sum√© final du nettoyage\n",
    "print(\"\\nüìä R√âSUM√â DU NETTOYAGE:\")\n",
    "print(f\"üìå Stations au d√©part : {stations_depart}\")\n",
    "print(f\"‚úÖ Stations nettoy√©es : {len_cleaned_data}\")\n",
    "print(f\"üèÅ Stations restantes apr√®s filtrage :\")\n",
    "\n",
    "for station_id, reason in ignored_buoys.items():\n",
    "    print(f\"üõë Station {station_id} ignor√©e: {reason}\")\n",
    "\n",
    "print(f\"\\nüßπ Cl√©s restantes dans buoy_datas apr√®s purge : {len(buoy_datas)} (attendu : {len_cleaned_data})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_buoys_missing_df_counts(buoy_datas, prefix=\"Cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusionner les df_meteo et df_marine sur 'Datetime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusion des DataFrames nettoy√©s\n",
    "print(\"\\nüîó FUSION DES DONN√âES MARINE + METEO PAR STATION\")\n",
    "\n",
    "merged_success_count = 0  # Compteur de fusions r√©ussies\n",
    "total_merged_rows = 0     # Total de lignes fusionn√©es\n",
    "\n",
    "for station_id, data in buoy_datas.items():\n",
    "    print(f\"\\nüîÑ Fusion des donn√©es pour la station {station_id}\")\n",
    "\n",
    "    cleaned_marine_df = data.get(\"Cleaned Marine\")\n",
    "    cleaned_meteo_df = data.get(\"Cleaned Meteo\")\n",
    "\n",
    "    if cleaned_marine_df is None or cleaned_meteo_df is None:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        merged_df = pd.merge(cleaned_marine_df, cleaned_meteo_df, on=\"Datetime\", how=\"inner\")\n",
    "\n",
    "        if merged_df.empty:\n",
    "            print(f\"‚ö†Ô∏è Station {station_id} fusionn√©e, mais r√©sultat vide apr√®s inner merge sur 'Datetime'\")\n",
    "        else:\n",
    "            buoy_datas[station_id][\"Merged\"] = merged_df\n",
    "            merged_success_count += 1\n",
    "            total_merged_rows += len(merged_df)\n",
    "            print(f\"‚úÖ Fusion r√©ussie pour la station {station_id} ({merged_df.shape[0]} lignes)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de la fusion pour {station_id}: {e}\")\n",
    "\n",
    "# R√©sum√© des fusions\n",
    "print(f\"\\nüì¶ Fusions r√©ussies : {merged_success_count}/{len_cleaned_data} stations\")\n",
    "print(f\"üìä Total de lignes fusionn√©es : {total_merged_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat√©nation des DataFrames fusionn√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat√©nation des DataFrames fusionn√©s\n",
    "print(\"\\nüß¨ CONCAT√âNATION DES DONN√âES FUSIONN√âES EN UN SEUL DATAFRAME\")\n",
    "\n",
    "final_merged_df_list = []\n",
    "concat_success_count = 0\n",
    "concat_total_rows = 0\n",
    "\n",
    "for station_id, data in buoy_datas.items():\n",
    "    merged_df = data.get(\"Merged\")\n",
    "\n",
    "    if merged_df is None:\n",
    "        print(f\"‚ö†Ô∏è Station {station_id} ignor√©e pour concat√©nation: Donn√©es fusionn√©es manquantes\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        final_merged_df_list.append(merged_df)\n",
    "        concat_success_count += 1\n",
    "        concat_total_rows += len(merged_df)\n",
    "        print(f\"‚úÖ Concat√©nation r√©ussie pour la station {station_id} ({len(merged_df)} lignes)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de la concat√©nation pour {station_id}: {e}\")\n",
    "\n",
    "# Cr√©ation du DataFrame final unique\n",
    "try:\n",
    "    df_final = pd.concat(final_merged_df_list, ignore_index=True)\n",
    "    print(f\"\\nüßæ DataFrame final cr√©√© avec succ√®s ({df_final.shape[0]} lignes, {df_final.shape[1]} colonnes)\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Erreur lors de la cr√©ation du DataFrame final: {e}\")\n",
    "    df_final = None\n",
    "\n",
    "# R√©sum√©\n",
    "print(f\"\\nüì¶ Concat√©nations r√©ussies : {concat_success_count}/{merged_success_count}\")\n",
    "print(f\"üìä Total de lignes dans le DataFrame final : {concat_total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_final.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hour Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_final['Datetime'] = pd.to_datetime(df_final['Datetime'])\n",
    "\n",
    "    df_final = df_final[df_final['Datetime'].dt.minute == 0]\n",
    "    # Affichage pour v√©rifier le r√©sultat\n",
    "    print(f\"üöÄ DataFrame filtr√©e pour ne garder que les lignes √† l'heure pile: {df_final.shape[0]} lignes\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ÔøΩÔøΩ Erreur lors du filtrage des lignes √† l'heure pile: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_datetime = pd.DataFrame(df_final[['Datetime']], dtype='datetime64[ns]')\n",
    "final_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = handle_null_values(df_final)\n",
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['41008' '41044' '41049' '42001' '42002' '42036' '42056' '42058' '44007'\n",
      " '44020' '44065' '46006' '46014' '46025' '46027' '46072' '46078' '46084'\n",
      " '46086' '46087' '46088' '51000' '51001' '51002']\n"
     ]
    }
   ],
   "source": [
    "# get station id distinct values\n",
    "station_ids = df_final['Station ID'].unique()\n",
    "print(station_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns = [col.strip() for col in df_final.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire de renommage des colonnes\n",
    "col_to_rename = {'temperature_2m': 'T¬∞(C¬∞)', \n",
    "                 'relative_humidity_2m': 'Relative Humidity (%)',\n",
    "                 'dew_point_2m': 'Dew Point (¬∞C)', \n",
    "                 'precipitation': 'Precipitations (mm)',  \n",
    "                 'pressure_msl':'Sea Level Pressure (hPa)', \n",
    "                 'cloud_cover_low':'Low Clouds (%)',\n",
    "                 'cloud_cover_mid' : 'Middle Clouds (%)', \n",
    "                 'cloud_cover_high' : 'High Clouds (%)', \n",
    "                 'visibility' : 'Visibility (km)', \n",
    "                 'wind_direction': 'Wind Direction (¬∞)',\n",
    "                 'wind_speed': 'Wind Speed (km/h)', \n",
    "                 'wind_gust': 'Wind Gusts (km/h)',\n",
    "                 'wind_speed_10m':'Wind Speed (10m)', \n",
    "                 'surface_pressure': 'Surface Pressure',\n",
    "                 'wave_height': 'Wave Height (m)', \n",
    "                 'average_wave_period': 'Average Wave Period (s)',\n",
    "                 'dominant_wave_direction': 'Dominant Wave Direction (¬∞)', \n",
    "                 'pressure': 'Pressure (hPa)',\n",
    "                 'air_temperature': 'Air T¬∞', \n",
    "                 'water_temperature': 'Water T¬∞', \n",
    "                 'Water_depth': 'Water Depth (m)', \n",
    "                 \"Air_temp_height\": \"Air T¬∞ Height\", \n",
    "                 \"Anemometer_height\": \"Anemometer Height (m)\", \n",
    "                 \"station_zone\": \"Station Zone\",\n",
    "                 \"Barometer_elevation\": \"Barometer Elevation\", \n",
    "                 \"sea_temp_depth\" : \"Sea Temperature Depth (m)\"}\n",
    "\n",
    "# Liste des colonnes √† supprimer\n",
    "cols_to_delete = ['soil_temperature_0cm', 'lat_buoy','lon_buoy', 'rain', \n",
    "                  'showers', 'is_day', 'soil_moisture_0_to_1cm', \n",
    "                  'cloud_cover']\n",
    "\t\n",
    "\n",
    "# Renommer les colonnes d'abord\n",
    "df_final = rename_columns(df_final, col_to_rename)\n",
    "# Ensuite, supprimer les colonnes non d√©sir√©es\n",
    "df_final = drop_columns_if_exist(df_final, cols_to_delete)\n",
    "try:\n",
    "    df_final[\"Visibility (km)\"].map(lambda x: x/1000)\n",
    "    df_final[\"T¬∞(C¬∞)\"] = round(df_final[\"T¬∞(C¬∞)\"], 2)\n",
    "    df_final[\"Wind Speed (10m)\"] = round(df_final[\"Wind Speed (10m)\"], 2)\n",
    "except Exception as e:\n",
    "    print(f\"ÔøΩÔøΩ Erreur lors du traitement des colonnes :\\n {e}\")\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "print(\"\\nColonnes apr√®s renommage et suppression :\")\n",
    "print(df_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    display_row_values(df_final, columns=[\"Visibility (km)\", \"T¬∞(C¬∞)\"])\n",
    "#    df_final[\"Visibility (km)\"] = df_final[\"Visibility (km)\"].map(lambda x: x / 1000)\n",
    "    \n",
    "    df_final[\"T¬∞(C¬∞)\"] = df_final[\"T¬∞(C¬∞)\"].round(2)\n",
    "    display_row_values(df_final, [\"Visibility (km)\", \"T¬∞(C¬∞)\"])\n",
    "except Exception as e:\n",
    "    print(f\"ÔøΩÔøΩ Erreur : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_final[['Lat', 'Lon']] = df_final.apply(\n",
    "        lambda row: pd.Series(convert_coordinates(row['Lat'], row['Lon'])),\n",
    "        axis=1\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors de la conversion des coordonn√©es : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  T¬∞(C¬∞) = (Air T¬∞ + T¬∞(C¬∞))/2 et virer Air T¬∞\n",
    "try:\n",
    "    df_final['T¬∞(C¬∞)'] = (df_final['Air T¬∞'] + df_final['T¬∞(C¬∞)']) / 2\n",
    "    df_final.drop(columns=['Air T¬∞'], inplace=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Sea Level Pressure (hPa) = (Sea Level Pressure (hPa) +Surface Pressure)/2 et virer Surface Pressure\n",
    "\n",
    "try:\n",
    "    df_final['Sea Level Pressure (hPa)'] = (df_final['Sea Level Pressure (hPa)'] + df_final['Surface Pressure']) / 2\n",
    "    df_final.drop(columns=['Surface Pressure'], inplace=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# virer le m dans Water Depth avec regex lambda et passer la colonne en float\n",
    "df_final['Water Depth (m)'] = df_final['Water Depth (m)'].apply(lambda x: re.sub(r'\\D', '', str(x)).strip())\n",
    "df_final['Water Depth (m)'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check truth about Wind Speed Using another API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_42058 = df_final[df_final['Station ID'] == \"42058\"]\n",
    "df_42058.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Chargement de la cl√© API ----\n",
    "vc_api_key_path = r\"c:\\Credentials\\visual_crossing_weather_api.json\"\n",
    "with open(vc_api_key_path, 'r') as file:\n",
    "    content = json.load(file)\n",
    "    vc_api_key = content[\"api_key\"]\n",
    "\n",
    "# ---- Extraire les coordonn√©es depuis la premi√®re ligne du DataFrame ----\n",
    "lat_42058, lon_42058 = None, None\n",
    "if not df_42058.empty:\n",
    "    first_row = df_42058.iloc[0]\n",
    "    lat_42058, lon_42058 = first_row[\"Lat\"], first_row[\"Lon\"]\n",
    "\n",
    "# ---- D√©finir les dates pour la requ√™te ----\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "last_month = (datetime.now() - timedelta(days=31)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# ---- Cr√©er le dossier de cache si n√©cessaire ----\n",
    "cache_dir = \"api_call_files\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# ---- D√©finir le fichier cache selon la position ----\n",
    "cache_file = os.path.join(cache_dir, f\"vc_meteo_{lat_42058}_{lon_42058}.csv\")\n",
    "\n",
    "# ---- V√©rifier si un cache r√©cent existe (moins de 24h) ----\n",
    "use_cache = False\n",
    "if os.path.exists(cache_file):\n",
    "    last_modified = datetime.fromtimestamp(os.path.getmtime(cache_file))\n",
    "    if datetime.now() - last_modified < timedelta(hours=24):\n",
    "        print(f\"üì¶ Cache d√©tect√© ({cache_file}), modifi√© le {last_modified.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        vc_meteo_df = pd.read_csv(cache_file)\n",
    "        print(\"‚úÖ Donn√©es m√©t√©o recharg√©es depuis le cache.\")\n",
    "        use_cache = True\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Cache trouv√© mais p√©rim√© (plus de 24h) ‚Üí nouvelle requ√™te API.\")\n",
    "\n",
    "# ---- Appel API si pas de cache valide ----\n",
    "if not use_cache and lat_42058 is not None and lon_42058 is not None:\n",
    "    url = f\"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/{lat_42058},{lon_42058}/{last_month}/{today}?unitGroup=metric&key={vc_api_key}&contentType=json\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            vc_meteo_data = response.json()\n",
    "            print(\"üåç Donn√©es m√©t√©o r√©cup√©r√©es depuis l'API Visual Crossing.\")\n",
    "\n",
    "            # ---- Extraire les donn√©es journali√®res et sauvegarder ----\n",
    "            if \"days\" in vc_meteo_data:\n",
    "                vc_meteo_df = pd.json_normalize(vc_meteo_data[\"days\"])\n",
    "                vc_meteo_df.to_csv(cache_file, index=False)\n",
    "                print(f\"üíæ Donn√©es sauvegard√©es dans le cache : {cache_file}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Le champ 'days' est absent de la r√©ponse API.\")\n",
    "        else:\n",
    "            print(f\"‚ùå √âchec de l‚Äôappel API ‚Äî code de statut : {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception lev√©e lors de la requ√™te API : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÅ Charger les donn√©es du CSV Visual Crossing\n",
    "vc_csv_path = f\"api_call_files/vc_meteo_{lat_42058}_{lon_42058}.csv\"\n",
    "df_vc_meteo = pd.read_csv(vc_csv_path)\n",
    "df_vc_meteo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßº Nettoyage et transformation\n",
    "import ast  # Permet de convertir les strings repr√©sentant des listes en objets Python\n",
    "all_hours = []\n",
    "\n",
    "# Parcours de chaque ligne du fichier m√©t√©o\n",
    "for i, row in df_vc_meteo.iterrows():\n",
    "    try:\n",
    "        # Convertir la colonne \"hours\" en liste de dictionnaires\n",
    "        hours_list = ast.literal_eval(row['hours'])\n",
    "\n",
    "        # Pour chaque heure dans la journ√©e, on ajoute une entr√©e au tableau final\n",
    "        for hour_data in hours_list:\n",
    "            hour_data['Date'] = row['datetime']  # On ajoute aussi la date du jour correspondant\n",
    "            all_hours.append(hour_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur parsing ligne {i}: {e}\")  # En cas d'erreur de parsing (souvent probl√®me de format)\n",
    "\n",
    "# ‚úÖ On transforme la liste aplanie en DataFrame\n",
    "df_vc_flat = pd.DataFrame(all_hours)\n",
    "\n",
    "# üïí Conversion de l'heure/temps\n",
    "df_vc_flat[\"Datetime\"] = pd.to_datetime(df_vc_flat[\"datetimeEpoch\"], unit=\"s\").dt.strftime(\"%Y-%m-%d-%H\")\n",
    "\n",
    "# üóìÔ∏è Filtrer sur les 30 derniers jours\n",
    "today = datetime.now()\n",
    "thirty_days_ago = today - timedelta(days=30)\n",
    "\n",
    "today_str = today.strftime(\"%Y-%m-%d\")\n",
    "thirty_days_ago_str = thirty_days_ago.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# On filtre le DataFrame pour ne garder que les lignes entre ces deux dates\n",
    "df_vc_last_month = df_vc_flat[\n",
    "    (df_vc_flat['Date'] >= thirty_days_ago_str) & \n",
    "    (df_vc_flat['Date'] <= today_str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_row_values(df_vc_last_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct syntaxe pour filtrer les colonnes sp√©cifiques\n",
    "df_vc_last_month = df_vc_last_month[[\"Datetime\", \"temp\", \"humidity\", \"precip\", \"dew\", \"windgust\", \"windspeed\", \"winddir\", \"pressure\", \"visibility\"]]\n",
    "\n",
    "for col in df_vc_last_month.columns:\n",
    "    if not col == \"Datetime\":\n",
    "        rename_columns(df_vc_last_month, {col: f\"VC_{col}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Datetime' column to datetime type in df_vc_last_month\n",
    "df_vc_last_month['Datetime'] = pd.to_datetime(df_vc_last_month['Datetime'], errors='coerce')\n",
    "\n",
    "\n",
    "# Check if the Datetime column is correctly converted\n",
    "print(df_42058['Datetime'].dtype)\n",
    "print(df_vc_last_month['Datetime'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now perform the merge\n",
    "df_compare = pd.merge(df_42058, df_vc_last_month, on=\"Datetime\", how=\"inner\")\n",
    "\n",
    "df_compare.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_windspeed_compare = df_compare[['Wind Speed (km/h)', 'Wind Speed (10m)','VC_windspeed']]\n",
    "\n",
    "    df_pressure_compare = df_compare[['Sea Level Pressure (hPa)','Pressure (hPa)', 'VC_pressure']]\n",
    "    df_dew_compare = df_compare[['dewpoint','Dew Point (¬∞C)', 'VC_dew']]\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparaison Wind Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_windspeed_compare.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_col_to_delete = ['Wind Speed (km/h)', 'Anemometer Height (m)']\n",
    "df_final = drop_columns_if_exist(df_final, wind_col_to_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparaison Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pressure_compare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = drop_columns_if_exist(df_final, ['Pressure (hPa)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dew_compare.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    # Calcul des distances absolues entre chaque paire de mesures pour chaque ligne\n",
    "    # Ces distances nous permettent de savoir quelle mesure est la plus proche des autres\n",
    "    df_compare['dist_dewpoint_DewPoint'] = np.abs(df_compare['dewpoint'] - df_compare['Dew Point (¬∞C)'])\n",
    "    df_compare['dist_dewpoint_VC'] = np.abs(df_compare['dewpoint'] - df_compare['VC_dew'])\n",
    "    df_compare['dist_DewPoint_VC'] = np.abs(df_compare['Dew Point (¬∞C)'] - df_compare['VC_dew'])\n",
    "\n",
    "    # Pour chaque ligne, on d√©termine quelle mesure est la plus proche des deux autres :\n",
    "    # - Si 'dewpoint' est plus proche des autres mesures que 'Dew Point (¬∞C)' et 'VC_dew', alors 'dewpoint' est marqu√© comme plus proche.\n",
    "    df_compare['dewpoint_closer'] = (df_compare['dist_dewpoint_VC'] < df_compare['dist_dewpoint_DewPoint']) & (df_compare['dist_dewpoint_VC'] < df_compare['dist_DewPoint_VC'])\n",
    "\n",
    "    # - Si 'Dew Point (¬∞C)' est plus proche des autres mesures que 'dewpoint' et 'VC_dew', alors 'Dew Point (¬∞C)' est marqu√© comme plus proche.\n",
    "    df_compare['DewPoint_closer'] = (df_compare['dist_dewpoint_DewPoint'] < df_compare['dist_dewpoint_VC']) & (df_compare['dist_dewpoint_DewPoint'] < df_compare['dist_DewPoint_VC'])\n",
    "\n",
    "    # - Si 'VC_dew' est plus proche des autres mesures que 'dewpoint' et 'Dew Point (¬∞C)', alors 'VC_dew' est marqu√© comme plus proche.\n",
    "    df_compare['VC_closer'] = (df_compare['dist_DewPoint_VC'] < df_compare['dist_dewpoint_VC']) & (df_compare['dist_DewPoint_VC'] < df_compare['dist_dewpoint_DewPoint'])\n",
    "\n",
    "    # Calculer les probabilit√©s que chaque mesure soit la plus proche des autres sur l'ensemble des lignes :\n",
    "    # La probabilit√© est simplement la proportion de fois o√π une mesure a √©t√© plus proche des autres.\n",
    "    prob_dewpoint_closer = df_compare['dewpoint_closer'].mean().round(3)\n",
    "    prob_dewpoint_c_closer = df_compare['DewPoint_closer'].mean().round(3)\n",
    "    prob_vc_closer = df_compare['VC_closer'].mean().round(3)\n",
    "\n",
    "    # Afficher les r√©sultats\n",
    "    # Ces r√©sultats indiquent la probabilit√© que chaque mesure soit la plus proche des autres sur toutes les lignes de donn√©es\n",
    "    print(f\"Probability that 'dewpoint' is closer to the truth: {prob_dewpoint_closer}\")\n",
    "    print(f\"Probability that 'Dew Point (¬∞C)' is closer to the truth: {prob_dewpoint_c_closer}\")\n",
    "    print(f\"Probability that 'VC_dew' is closer to the truth: {prob_vc_closer}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_final['Dew Point (¬∞C)'] = df_final['dewpoint']\n",
    "    df_final = drop_columns_if_exist(df_final, ['dewpoint'])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = add_hour_day_period_and_month(df_final)\n",
    "print(df_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_final.info())\n",
    "display(df_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les informations de connexion MySQL\n",
    "path_to_mysql_creds = r\"c:\\Credentials\\mysql_creds.json\"\n",
    "with open(path_to_mysql_creds, 'r') as file:\n",
    "    content = json.load(file)\n",
    "    mysql_user = content[\"user\"]\n",
    "    password = content[\"password\"]\n",
    "    host = content[\"host\"]\n",
    "    port = content[\"port\"]\n",
    "    database = \"oceanography_data_analysis\"\n",
    "    silver_table = \"silver table\"\n",
    "\n",
    "# Connexion √† MySQL avec SQLModel\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{mysql_user}:{password}@{host}:{port}/{database}\", isolation_level='AUTOCOMMIT')\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_final.rename(columns={'Air T¬∞ Height': 'Air T¬∞ Height (m)',\n",
    "                             'Barometer Elevation': 'Barometer Elevation (m)',\n",
    "                             'Water T¬∞': 'Water T¬∞ (¬∞C)'}, inplace=True)\n",
    "\n",
    "    print(\"Column 'Air T¬∞ Height' renamed to 'Air T¬∞ Height (m)'\")\n",
    "\n",
    "    print(\"Column 'Barometer Elevation' renamed to 'Barometer Elevation (m)'\")\n",
    "    print(\"Column 'Water T¬∞' renamed to 'Water T¬∞ (¬∞C)'\")\n",
    "    df_final['T¬∞(C¬∞)'] = df_final['T¬∞(C¬∞)'].round(2)\n",
    "\n",
    "    print(\"Column 'T¬∞(C¬∞)' rounded to 2 decimal places\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir 'Station ID' en cha√Æne de caract√®res et concat√©ner avec la date sous forme de 'YYYYMMDDHH'\n",
    "df_final['ID Unique'] = df_final['Station ID'].astype(str) + df_final['Datetime'].dt.strftime('%Y%m%d%H')\n",
    "\n",
    "# Obtenir les paires uniques de 'ID Unique' et 'Station ID'\n",
    "df_uniques_and_station_id = df_final[['ID Unique', 'Station ID']].drop_duplicates()\n",
    "# R√©organiser les colonnes pour que 'ID Unique' soit en premier\n",
    "cols = ['ID Unique'] + [col for col in df_final.columns if col != 'ID Unique']\n",
    "df_final = df_final[cols]\n",
    "\n",
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_in_mysql(df=df_final, engine=engine, table_name=silver_table)\n",
    "insert_new_rows(df=df_final, engine=engine, silver_table=silver_table, ref_column='ID Unique')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total DF code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### DimStation ##################################################################################################\n",
    "df_station = df_final[['Station ID', 'Station Zone', 'Lat', 'Lon']].drop_duplicates()\n",
    "df_station = df_station[['Station ID', 'Station Zone', 'Lat', 'Lon']]\n",
    "\n",
    "print(f\"\\nNul count in df_station:\\n{df_station.isnull().sum()}\")\n",
    "\n",
    "###################################### DimTime ##################################################################################################\n",
    "df_final['Year'] = df_final['Datetime'].dt.year\n",
    "df_final['Day'] = df_final['Datetime'].dt.day\n",
    "df_final['DayOfWeek'] = df_final['Datetime'].dt.weekday\n",
    "df_final['Hour'] = df_final['Datetime'].dt.hour\n",
    "df_time = df_final[['Year', 'Month', 'Day', 'Hour', 'DayOfWeek', 'DayPeriod']].drop_duplicates()\n",
    "df_time['Date ID'] = df_time.apply(lambda row: int(f\"{row.Year:04d}{row.Month:02d}{row.Day:02d}{row.Hour:02d}\"), axis=1)\n",
    "df_time = df_time[['Date ID', 'Year', 'Month', 'Day', 'Hour', 'DayOfWeek', 'DayPeriod']]\n",
    "\n",
    "print(f\"\\nNull count in df_time:\\n{df_time.isnull().sum()}\")\n",
    "\n",
    "########################################## Facts Meteo #########################################################################\n",
    "\n",
    "df_facts_meteo = df_final[['ID Unique', 'T¬∞(C¬∞)', 'Relative Humidity (%)', 'Dew Point (¬∞C)',\n",
    "    'Precipitations (mm)', 'Sea Level Pressure (hPa)', 'Low Clouds (%)',\n",
    "    'Middle Clouds (%)', 'High Clouds (%)', 'Visibility (km)', 'Wind Speed (10m)',\n",
    "    'Wind Direction (¬∞)', 'Wind Gusts (km/h)', 'Barometer Elevation (m)',\n",
    "    'Air T¬∞ Height (m)', 'Station ID', 'Year', 'Month', 'Day', 'Hour'\n",
    "]].copy()\n",
    "df_facts_meteo['Date ID'] = df_facts_meteo.apply(lambda row: int(f\"{row.Year:04d}{row.Month:02d}{row.Day:02d}{row.Hour:02d}\"), axis=1)\n",
    "\n",
    "# R√©arrangement pour mettre les cl√©s √©trang√®res √† la fin\n",
    "df_facts_meteo = df_facts_meteo[[\n",
    "    'ID Unique',  # Cl√© unique en premier\n",
    "    'T¬∞(C¬∞)', 'Relative Humidity (%)', 'Dew Point (¬∞C)', 'Precipitations (mm)',\n",
    "    'Sea Level Pressure (hPa)', 'Low Clouds (%)', 'Middle Clouds (%)', 'High Clouds (%)',\n",
    "    'Visibility (km)', 'Wind Speed (10m)', 'Wind Direction (¬∞)', 'Wind Gusts (km/h)',\n",
    "    'Barometer Elevation (m)', 'Air T¬∞ Height (m)', 'Date ID', 'Station ID'\n",
    "]]  # Cl√©s √©trang√®res √† la fin\n",
    "\n",
    "# print nul count in the df\n",
    "print(f\"\\nNull count in df_facts_meteo:\\n{df_facts_meteo.isnull().sum()}\")\n",
    "\n",
    "########################################## Facts Ocean #########################################################################\n",
    "\n",
    "df_facts_ocean = df_final[[\n",
    "    'ID Unique', \n",
    "    'Wave Height (m)', 'Average Wave Period (s)', 'Dominant Wave Direction (¬∞)',\n",
    "    'Water T¬∞ (¬∞C)', 'Water Depth (m)', 'Sea Temperature Depth (m)', 'Station ID', 'Year', 'Month', 'Day', 'Hour'\n",
    "]].copy()\n",
    "df_facts_ocean['Date ID'] = df_facts_ocean.apply(lambda row: int(f\"{row.Year:04d}{row.Month:02d}{row.Day:02d}{row.Hour:02d}\"), axis=1)\n",
    "\n",
    "df_facts_ocean = df_facts_ocean[[\n",
    "    'ID Unique',  # Cl√© unique en premier\n",
    "    'Wave Height (m)', 'Average Wave Period (s)', 'Dominant Wave Direction (¬∞)',\n",
    "    'Water T¬∞ (¬∞C)', 'Water Depth (m)', 'Sea Temperature Depth (m)', 'Date ID', 'Station ID'\n",
    "]]  # Cl√©s √©trang√®res √† la fin\n",
    "\n",
    "# print nul count in the df\n",
    "print(f\"\\nNull count in df_facts_ocean:\\n{df_facts_ocean.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier les doublons\n",
    "print(f\"Doublons dans df_station : {df_station.duplicated().sum()}\")\n",
    "print(f\"Doublons dans df_time : {df_time.duplicated().sum()}\")\n",
    "print(f\"Doublons dans df_facts_meteo : {df_facts_meteo.duplicated().sum()}\")\n",
    "print(f\"Doublons dans df_facts_ocean : {df_facts_ocean.duplicated().sum()}\")\n",
    "\n",
    "\n",
    "# V√©rifier si les cl√©s √©trang√®res existent dans les dimensions\n",
    "print(f\"Stations ID dans df_facts_meteo qui ne sont pas dans df_station : {df_facts_meteo[~df_facts_meteo['Station ID'].isin(df_station['Station ID'])].shape[0]}\")\n",
    "print(f\"Date ID dans df_facts_meteo qui ne sont pas dans df_time : {df_facts_meteo[~df_facts_meteo['Date ID'].isin(df_time['Date ID'])].shape[0]}\")\n",
    "\n",
    "print(f\"Stations ID dans df_facts_ocean qui ne sont pas dans df_station : {df_facts_ocean[~df_facts_ocean['Station ID'].isin(df_station['Station ID'])].shape[0]}\")\n",
    "print(f\"Date ID dans df_facts_ocean qui ne sont pas dans df_time : {df_facts_ocean[~df_facts_ocean['Date ID'].isin(df_time['Date ID'])].shape[0]}\")\n",
    "\n",
    "# V√©rification des valeurs de base\n",
    "print(f\"\\nDescription des donn√©es de df_station : \\n{df_station.describe()}\")\n",
    "print(f\"\\nDescription des donn√©es de df_time : \\n{df_time.describe()}\")\n",
    "print(f\"\\nDescription des donn√©es de df_facts_meteo : \\n{df_facts_meteo.describe()}\")\n",
    "print(f\"\\nDescription des donn√©es de df_facts_ocean : \\n{df_facts_ocean.describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique values\n",
    "print(f\"{df_station.shape[0]}\\n\\n{df_station.nunique()}\")\n",
    "display_row_values(df_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    create_table_in_mysql(df=df_station, engine=engine, table_name='dim_station')\n",
    "    insert_new_rows(df=df_station, engine=engine, table='dim_station', ref_column='Station ID')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{df_time.shape[0]}\\n\\n{df_time.nunique()}\")\n",
    "display_row_values(df_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_new_rows(df: pd.DataFrame, engine, table: str, ref_column: str):\n",
    "    \"\"\"\n",
    "    Ins√®re les nouvelles lignes dans la table MySQL apr√®s avoir v√©rifi√© si les IDs uniques existent d√©j√†.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): Le DataFrame contenant les nouvelles donn√©es.\n",
    "    - engine (SQLAlchemy Engine): L'engine SQLAlchemy pour se connecter √† la base de donn√©es.\n",
    "    - table (str): Le nom de la table dans laquelle ins√©rer les donn√©es.\n",
    "    - ref_column (str): Le nom de la colonne √† utiliser comme r√©f√©rence (ID unique).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"üîç Connexion √† la base de donn√©es en cours...\")\n",
    "\n",
    "        with engine.connect() as connection:\n",
    "            # V√©rifier si la table est vide\n",
    "            print(\"üßÆ V√©rification si la table est vide...\")\n",
    "            check_empty_sql = f\"SELECT COUNT(*) FROM `{table}`\"\n",
    "            result = connection.execute(text(check_empty_sql))\n",
    "            row_count = result.fetchone()[0]\n",
    "\n",
    "            if row_count == 0:\n",
    "                # La table est vide, ins√©rer toutes les donn√©es avec compteur\n",
    "                print(\"‚ö° La table est vide. Insertion de toutes les donn√©es avec suivi...\")\n",
    "                total = len(df)\n",
    "                inserted = 0\n",
    "                for _, row in df.iterrows():\n",
    "                    row_df = pd.DataFrame([row])\n",
    "                    row_df.to_sql(table, con=engine, if_exists='append', index=False)\n",
    "                    inserted += 1\n",
    "                    sys.stdout.write(f\"\\r‚è≥ Insertion en cours : {inserted}/{total}\")\n",
    "                    sys.stdout.flush()\n",
    "                print(\"\\n‚úÖ Toutes les lignes ont √©t√© ins√©r√©es dans la table.\")\n",
    "            else:\n",
    "                # Sinon, r√©cup√©rer les IDs existants dans la table\n",
    "                print(f\"üîé R√©cup√©ration des {ref_column} existants dans la table...\")\n",
    "                check_existing_ids_sql = f\"SELECT `{ref_column}` FROM `{table}`\"\n",
    "                result = connection.execute(text(check_existing_ids_sql))\n",
    "                existing_ids = {row[0] for row in result.fetchall()}\n",
    "                print(f\"üßë‚Äçüíª {len(existing_ids)} {ref_column} existants ont √©t√© trouv√©s dans la table.\")\n",
    "\n",
    "                # Filtrage des lignes du DataFrame\n",
    "                print(\"üîÑ Filtrage des nouvelles lignes √† ins√©rer...\")\n",
    "                new_rows_df = df[~df[ref_column].isin(existing_ids)]\n",
    "\n",
    "                if not new_rows_df.empty:\n",
    "                    total = len(new_rows_df)\n",
    "                    print(f\"üöÄ Insertion de {total} nouvelles lignes...\")\n",
    "                    inserted = 0\n",
    "                    for _, row in new_rows_df.iterrows():\n",
    "                        row_df = pd.DataFrame([row])\n",
    "                        row_df.to_sql(table, con=engine, if_exists='append', index=False)\n",
    "                        inserted += 1\n",
    "                        sys.stdout.write(f\"\\r‚è≥ Insertion en cours : {inserted}/{total}\")\n",
    "                        sys.stdout.flush()\n",
    "                    print(\"\\n‚úÖ Insertion termin√©e avec succ√®s.\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Aucune nouvelle ligne √† ins√©rer, tous les {ref_column} sont d√©j√† pr√©sents.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de l'insertion dans la table '{table}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    create_table_in_mysql(df=df_time, engine=engine, table_name='dim_time')\n",
    "    insert_new_rows(df=df_time, engine=engine, table='dim_time', ref_column='Date ID')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{df_facts_meteo.shape[0]}\\n\\n{df_facts_meteo.nunique()}\")\n",
    "display_row_values(df_facts_meteo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    create_table_in_mysql(df=df_facts_meteo, engine=engine, table_name='facts_meteo')\n",
    "    insert_new_rows(df=df_facts_meteo, engine=engine, table='facts_meteo', ref_column='ID Unique')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{df_facts_ocean.shape[0]}\\n\\n{df_facts_ocean.nunique()}\")\n",
    "display_row_values(df_facts_ocean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    create_table_in_mysql(df=df_facts_ocean, engine=engine, table_name='facts_ocean')\n",
    "    insert_new_rows(df=df_facts_ocean, engine=engine, table='facts_ocean', ref_column='ID Unique')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
