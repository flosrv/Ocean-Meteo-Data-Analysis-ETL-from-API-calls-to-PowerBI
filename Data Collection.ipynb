{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database import (\n",
    "    engine_DW,\n",
    "    engine_staging,\n",
    "    db_staging,\n",
    "    db_DW,\n",
    "\n",
    "    table_staging_marine_name,\n",
    "    table_staging_meteo_name,\n",
    "\n",
    "    table_facts_marine_name,\n",
    "    table_facts_meteo_name,\n",
    "\n",
    "    table_dim_station_name,\n",
    "    table_dim_time_name)\n",
    "\n",
    "from imports import *\n",
    "from functions import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Available Stations ID List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all stations and some metadata as a Pandas DataFrame\n",
    "stations_df = api.stations()\n",
    "# parse the response as a dictionary\n",
    "stations_df = api.stations(as_df=True)\n",
    "\n",
    "print(len(stations_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter Buoys by Remarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_error_url_list = []\n",
    "\n",
    "# Liste de mots √† rechercher dans la colonne \"Remark\"\n",
    "blacklist = [\"Failure\", \"ceased\", \"failed\", \"recovered\", \"stopped\", 'adrift']\n",
    "stations_id_set = set()\n",
    "\n",
    "print(f'Avant Filtre: {stations_df.shape[0]}')\n",
    "\n",
    "# Liste pour collecter les indices √† supprimer\n",
    "indices_a_supprimer = []\n",
    "\n",
    "# Parcours des lignes de la DataFrame\n",
    "for idx, row in stations_df.iterrows():\n",
    "    # Creation de l'url via la station ID\n",
    "    station_id = row[\"Station\"]\n",
    "    url = r\"https://www.ndbc.noaa.gov/station_page.php?station=\"+str(station_id)\n",
    "    #creation\n",
    "    station_Location = row[\"Hull No./Config and Location\"]  # Extraire la valeur de la cellule pour chaque ligne\n",
    "    \n",
    "    # Extraction du nom de la station si un \")\" est trouv√©\n",
    "    if \")\" in station_Location:\n",
    "        station_name = station_Location.split(')')[1].rstrip(\" )\")  # On enl√®ve l'espace et la parenth√®se en fin de cha√Æne\n",
    "    else:\n",
    "        station_name = station_Location.strip()  # Si pas de \")\", on garde toute la cha√Æne\n",
    "\n",
    "    station_name = station_name.rstrip(\" )\").replace(\"(\", \"\").replace(\")\", \"\").strip()\n",
    "\n",
    "    # Nettoyage final pour enlever toute parenth√®se ou espace en fin de nom\n",
    "    station_name = station_name.rstrip(\" )\")\n",
    "\n",
    "    # V√©rifier si \"Remark\" n'est pas NaN et si un des √©l√©ments de blacklist est dans \"Remark\"\n",
    "    if isinstance(row[\"Remark\"], str) and any(blacklist_word.lower() in row[\"Remark\"].lower() for blacklist_word in blacklist):\n",
    "        # Ajouter l'index √† la liste\n",
    "        indices_a_supprimer.append(idx)\n",
    "        url = get_buoy_url(station_id)\n",
    "        access_error_url_list.append(url)\n",
    "    else:\n",
    "        pass\n",
    "# Supprimer les lignes apr√®s la boucle\n",
    "stations_df.drop(index=indices_a_supprimer, inplace=True)\n",
    "\n",
    "print(\"\\nüåä Bou√©es rejet√©es (URLs):\")\n",
    "for url in access_error_url_list:\n",
    "    print(url)\n",
    "\n",
    "print(f'Apr√®s Filtre: {stations_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get HTML from buoys that failed the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_SECTIONS = {}\n",
    "\n",
    "for url in access_error_url_list:\n",
    "    try:\n",
    "        # Extraction de l'identifiant de la bou√©e depuis l'URL\n",
    "        buoy_id = url.split(\"=\")[1]\n",
    "        DICT_SECTIONS[buoy_id] = {}\n",
    "\n",
    "        # Requ√™te HTTP vers la page de la bou√©e\n",
    "        response = requests.get(url, timeout=5)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Recherche de la section sp√©cifique contenant les m√©tadonn√©es\n",
    "        section_metadata = soup.find(\"section\", id=\"stationmetadata\", class_=\"metadata\")\n",
    "        \n",
    "        if section_metadata:\n",
    "            print(f\"‚úÖ Metadata trouv√©e pour {buoy_id}\")\n",
    "            DICT_SECTIONS[buoy_id][\"html\"] = section_metadata\n",
    "        else:\n",
    "            print(f\"‚ùå Aucune metadata trouv√©e pour {buoy_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur pour {buoy_id} : {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choisir un buoy_id au hasard dans le dictionnaire\n",
    "random_buoy_id = random.choice(list(DICT_SECTIONS.keys()))\n",
    "section_html = DICT_SECTIONS[random_buoy_id].get(\"html\")\n",
    "\n",
    "# URL de base pour les images\n",
    "base_url = f\"https://www.ndbc.noaa.gov/station_page.php?station={random_buoy_id}\"\n",
    "\n",
    "if section_html:\n",
    "    # Convertir en objet BeautifulSoup si ce n‚Äôest pas d√©j√† fait\n",
    "    soup = BeautifulSoup(str(section_html), \"html.parser\")\n",
    "\n",
    "    # Convertir tous les liens <img src=\"...\"> en absolu\n",
    "    for img in soup.find_all(\"img\"):\n",
    "        if img.has_attr(\"src\"):\n",
    "            img[\"src\"] = urljoin(base_url, img[\"src\"])\n",
    "\n",
    "    print(f\"üéØ Bou√©e s√©lectionn√©e : {random_buoy_id}\")\n",
    "    display(HTML(str(soup)))\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Pas de contenu HTML pour la bou√©e {random_buoy_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Buoys_datas Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire pour stocker les DataFrames, cl√© : ID de la bou√©e, valeur : DataFrame\n",
    "buoy_datas = {}\n",
    "buoy_list = []\n",
    "\n",
    "# Parcours de chaque bou√©e dans stations_df\n",
    "for index, row in stations_df.iterrows():\n",
    "    buoy_id = row['Station']\n",
    "    metadata = get_station_metadata(buoy_id)\n",
    "\n",
    "    # ‚úÖ R√©cup√©rer les donn√©es sous forme de dictionnaire\n",
    "    buoy_info = parse_buoy_json(metadata)\n",
    "\n",
    "    # ‚úÖ Stocker directement les donn√©es dans buoy_datas\n",
    "    buoy_datas[buoy_id] = buoy_info\n",
    "    buoy_list.append(buoy_id)\n",
    "\n",
    "# Affichage du nombre de bou√©es r√©ussies et √©chou√©es\n",
    "print(f\"Nombre de bou√©es trait√©es : {len(buoy_datas)}\\n\")\n",
    "\n",
    "# Afficher le contenu de buoy_datas\n",
    "\n",
    "first_key =next(iter(buoy_datas))\n",
    "first_key\n",
    "buoy_datas[first_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "import folium\n",
    "\n",
    "# Liste pour stocker les coordonn√©es\n",
    "coordinates = []\n",
    "\n",
    "# Remplir la liste avec les coordonn√©es converties\n",
    "for buoy_id, value in buoy_datas.items():\n",
    "    Water_depth = value[\"Water_depth\"]\n",
    "    station_zone = value[\"station_zone\"]\n",
    "    lat = value[\"lat_buoy\"]\n",
    "    lon = value[\"lon_buoy\"]\n",
    "    \n",
    "    lat, lon = convert_to_decimal(lat, lon)\n",
    "    coordinates.append((lat, lon, Water_depth, station_zone, buoy_id))\n",
    "\n",
    "# Calcul du centre de la carte\n",
    "avg_lat = sum(coord[0] for coord in coordinates) / len(coordinates)\n",
    "avg_lon = sum(coord[1] for coord in coordinates) / len(coordinates)\n",
    "\n",
    "# Cr√©ation de la carte avec zoom 2.5, sans attribution visible\n",
    "map_center = folium.Map(\n",
    "    location=[avg_lat, avg_lon],\n",
    "    zoom_start=2.5,\n",
    "    tiles=None\n",
    ")\n",
    "\n",
    "# Ajouter les tuiles Esri sans attribution visible\n",
    "folium.TileLayer(\n",
    "    tiles=\"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\",\n",
    "    attr=\"&nbsp;\",  # Astuce pour √©viter l'affichage de l'attribution\n",
    "    name=\"Esri Imagery\",\n",
    "    control=False\n",
    ").add_to(map_center)\n",
    "\n",
    "# Ajouter les marqueurs\n",
    "for lat, lon, Water_depth, station_zone, buoy_id in coordinates:\n",
    "    popup = folium.Popup(f\"ID: {buoy_id}<br>Zone: {station_zone}<br>Water Depth: {Water_depth} meters\", max_width=300)\n",
    "    folium.Marker([lat, lon], popup=popup).add_to(map_center)\n",
    "\n",
    "# CSS pour masquer l'attribution\n",
    "hide_leaflet_css = \"\"\"\n",
    "<style>\n",
    ".leaflet-control-attribution {\n",
    "    display: none !important;\n",
    "}\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "# Injecter le CSS et afficher la carte\n",
    "display(HTML(hide_leaflet_css))\n",
    "display(map_center)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecte des donn√©es marines et m√©t√©os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ D√©marrage du processus\n",
    "print(\"\\nüöÄ D√©marrage du processus de collecte des donn√©es...\\n\")\n",
    "\n",
    "# Initialisation des compteurs\n",
    "marine_data_collected_successfully = marine_data_collected_failed = 0\n",
    "meteo_data_collected_successfully = meteo_data_collected_failed = 0\n",
    "\n",
    "success = False\n",
    "total_stations = stations_df.shape[0]\n",
    "count = 0\n",
    "\n",
    "# üîÑ Parcours des bou√©es / stations\n",
    "for idx, row in stations_df.iterrows():\n",
    "    buoy_id = row[\"Station\"]\n",
    "\n",
    "    ######### üåä MARINE DATA #########\n",
    "    try:\n",
    "        df_marine = NDBC.realtime_observations(buoy_id)\n",
    "        if df_marine is None or df_marine.empty:\n",
    "            marine_data_collected_failed += 1\n",
    "            continue\n",
    "\n",
    "        marine_data_collected_successfully += 1\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur collecte marine {buoy_id}: {e}\")\n",
    "        marine_data_collected_failed += 1\n",
    "        continue\n",
    "\n",
    "    # Ajout des m√©tadonn√©es\n",
    "    try:\n",
    "        buoy_info = buoy_datas.get(buoy_id, {})\n",
    "        Lat, Lon = buoy_info.get('lat_buoy'), buoy_info.get('lon_buoy')\n",
    "        if Lat is None or Lon is None:\n",
    "            raise ValueError(f\"Donn√©es manquantes pour {buoy_id}\")\n",
    "\n",
    "        df_marine['Lat'] = Lat\n",
    "        df_marine['Lon'] = Lon\n",
    "        df_marine['Water_depth'] = buoy_info.get('Water_depth', None)\n",
    "        df_marine.columns = ['Datetime' if 'date' in col.lower() or 'time' in col.lower() else col for col in df_marine.columns]\n",
    "        df_marine['Datetime'] = df_marine['Datetime'].dt.tz_localize(None)\n",
    "\n",
    "        buoy_datas[buoy_id][\"Marine\"] = df_marine\n",
    "\n",
    "        station_zone = safe_get(parse_buoy_json(get_station_metadata(buoy_id)), \"station_zone\")\n",
    "        Bronze_Marine_table_Name = f\"br_{buoy_id}_marine_{station_zone}\".replace('.', '_').replace('-', '_').replace(' ', '_').lower()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur m√©tadonn√©es marine {buoy_id}: {e}\")\n",
    "        marine_data_collected_failed += 1\n",
    "        continue\n",
    "\n",
    "    ######### ‚õÖ METEO DATA #########\n",
    "    try:\n",
    "        df_meteo = meteo_api_request([Lat, Lon])\n",
    "        if df_meteo is None or df_meteo.empty:\n",
    "            meteo_data_collected_failed += 1\n",
    "            continue\n",
    "        \n",
    "        rename_columns(df_meteo, {'date':'Datetime'})\n",
    "        df_meteo.columns = ['Datetime' if 'date' in col.lower() or 'time' in col.lower() else col for col in df_meteo.columns]\n",
    "        df_meteo['Datetime'] = df_meteo['Datetime'].dt.tz_localize(None)\n",
    "    \n",
    "        buoy_datas[buoy_id][\"Meteo\"] = df_meteo\n",
    "        meteo_data_collected_successfully += 1\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur collecte m√©t√©o {buoy_id}: {e}\")\n",
    "        meteo_data_collected_failed += 1\n",
    "        continue\n",
    "\n",
    "# Retirer les bou√©es avec des DataFrames vides ou None\n",
    "buoy_datas = {buoy_id: data for buoy_id, data in buoy_datas.items() \n",
    "              if \"Marine\" in data and data[\"Marine\"] is not None and not data[\"Marine\"].empty\n",
    "              and \"Meteo\" in data and data[\"Meteo\"] is not None and not data[\"Meteo\"].empty}\n",
    "\n",
    "# üîö R√©sum√© final\n",
    "\n",
    "print(\"\\nüìù R√©sum√© final :\")\n",
    "print(f\"üåä Marine - Collecte ‚úÖ {marine_data_collected_successfully} ‚ùå {marine_data_collected_failed}\")\n",
    "print(f\"‚õÖ M√©t√©o - Collecte ‚úÖ {meteo_data_collected_successfully} ‚ùå {meteo_data_collected_failed}\")\n",
    "\n",
    "# Afficher la longueur du dictionnaire (nombre de bou√©es avec des donn√©es valides)\n",
    "print(f\"\\nüìä Nombre de bou√©es avec des donn√©es valides : {len(buoy_datas)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Enrichment with MetaDatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_not_include = ['lon_buoy', \"lat_buoy\", \"url\"]\n",
    "\n",
    "list_df_marine = []\n",
    "list_df_meteo = []\n",
    "\n",
    "for buoy_id, value in buoy_datas.items():\n",
    "    print(f\"\\nüîç Traitement de la Station ID: {buoy_id}\")\n",
    "\n",
    "    marine_df = buoy_datas[buoy_id][\"Marine\"]\n",
    "    list_df_marine.append(marine_df)\n",
    "    meteo_df = buoy_datas[buoy_id][\"Meteo\"]\n",
    "    list_df_meteo.append(meteo_df)\n",
    "\n",
    "    try:\n",
    "        # R√©cup√©rer les m√©tadonn√©es de la station\n",
    "        buoy_metadata = get_station_metadata(buoy_id)\n",
    "        parsed_data = parse_buoy_json(buoy_metadata)\n",
    "\n",
    "        # Mise √† jour du dictionnaire avec les m√©tadonn√©es\n",
    "        data = buoy_datas[buoy_id]\n",
    "        data.update(parsed_data)\n",
    "        \n",
    "        # Ajouter les m√©tadonn√©es comme nouvelles colonnes dans marine_df\n",
    "        if marine_df is not None:\n",
    "            marine_df[\"Station ID\"] = str(buoy_id)\n",
    "            for key, value in parsed_data.items():\n",
    "                # V√©rifier si la cl√© n'est pas dans la liste des exclusions\n",
    "                if key not in list_not_include:\n",
    "                    marine_df[key] = value\n",
    "                    print(f\"‚úÖ Colonne '{key}' ajout√©e au DataFrame de la station {buoy_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur pour la station {buoy_id}: {e}\")\n",
    "\n",
    "# V√©rification de l'ajout des colonnes en prenant un id au hasard\n",
    "station_id = random.choice(list(buoy_datas.keys()))\n",
    "marine_df = buoy_datas[station_id][\"Marine\"]\n",
    "\n",
    "if marine_df is not None:\n",
    "    print(\"\\nColonnes ajout√©es au DataFrame de la station\", station_id)\n",
    "    print(marine_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_buoys_missing_df_counts(buoy_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_marine.columns)\n",
    "display(df_meteo.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marine_random = random.choice(list_df_marine)\n",
    "\n",
    "\n",
    "# Marine null values heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.heatmap(df_marine_random.isnull(), cbar=False, cmap='coolwarm')\n",
    "\n",
    "plt.title(f'Null Values Heatmap {df_marine_random['Station ID'][0]}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©lectionner une DataFrame al√©atoire et r√©cup√©rer son index\n",
    "random_index = random.randint(0, len(list_df_meteo) - 1)\n",
    "df_meteo_random = list_df_meteo[random_index]\n",
    "\n",
    "# Marine null values heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df_meteo_random.isnull(), cbar=False, cmap='coolwarm')\n",
    "plt.title(f'Null Values Heatmap (DataFrame #{random_index})')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_columns_oceanography = [\n",
    "    'wind_direction',             \n",
    "    'wind_speed',                 \n",
    "    'wave_height',                   \n",
    "    'pressure',                   \n",
    "    'air_temperature',            \n",
    "    'water_temperature',          \n",
    "    'Datetime',\n",
    "    'Lat',\n",
    "    'Lon'                 \n",
    "]\n",
    "\n",
    "important_columns_meteorology = [\n",
    "    'temperature_2m',             \n",
    "    'relative_humidity_2m',       \n",
    "    'dew_point_2m',               \n",
    "    'precipitation',              \n",
    "    'pressure_msl',               \n",
    "    'cloud_cover',                \n",
    "    'wind_speed_10m',             \n",
    "    'Datetime'\n",
    "]\n",
    "\n",
    "initial_meteo_data_count = 0\n",
    "initial_marine_data_count = 0\n",
    "\n",
    "marine_data_after_cleaning = 0\n",
    "meteo_data_after_cleaning = 0\n",
    "\n",
    "stations_depart = len(buoy_datas)\n",
    "ignored_buoys = {}  # Dictionary to track ignored buoys and their reasons\n",
    "\n",
    "for station_id, data in buoy_datas.items():\n",
    "    print(f\"\\nüîÑ Nettoyage des donn√©es pour la station {station_id}\")\n",
    "\n",
    "    marine_df = data.get(\"Marine\")\n",
    "    meteo_df = data.get(\"Meteo\")\n",
    "\n",
    "    if marine_df is None or meteo_df is None:\n",
    "        ignored_buoys[station_id] = \"Marine DataFrame ou Meteo DataFrame manquant(e)\"\n",
    "        print(f\"‚ö†Ô∏è Station {station_id} ignor√©e: Marine DataFrame ou Meteo DataFrame manquant(e)\")\n",
    "        continue\n",
    "\n",
    "    initial_marine_data_count += marine_df.shape[0]\n",
    "    initial_meteo_data_count += meteo_df.shape[0]\n",
    "\n",
    "    try:\n",
    "        # Nettoyage des DataFrames\n",
    "        cleaned_marine_df = handle_null_values(marine_df)\n",
    "        cleaned_meteo_df = handle_null_values(meteo_df)\n",
    "        # V√©rification des colonnes importantes apr√®s nettoyage\n",
    "        marine_columns_ok = all(col in cleaned_marine_df.columns for col in important_columns_oceanography)\n",
    "        meteo_columns_ok = all(col in cleaned_meteo_df.columns for col in important_columns_meteorology)\n",
    "\n",
    "        # Track which columns are missing\n",
    "        missing_marine_columns = [col for col in important_columns_oceanography if col not in cleaned_marine_df.columns]\n",
    "        missing_meteo_columns = [col for col in important_columns_meteorology if col not in cleaned_meteo_df.columns]\n",
    "\n",
    "        if missing_marine_columns or missing_meteo_columns:\n",
    "            ignored_buoys[station_id] = f\"Colonnes manquantes: Marine: {missing_marine_columns}, Meteo: {missing_meteo_columns}\"\n",
    "            print(f\"‚ö†Ô∏è Station {station_id} ignor√©e: Colonnes manquantes - Marine: {missing_marine_columns}, Meteo: {missing_meteo_columns}\")\n",
    "            continue\n",
    "\n",
    "        # Ajouter le DataFrame nettoy√© au dictionnaire des r√©sultats\n",
    "        buoy_datas[station_id]['Cleaned Marine'] = cleaned_marine_df\n",
    "        buoy_datas[station_id]['Cleaned Meteo'] = cleaned_meteo_df\n",
    "        print(f\"‚úÖ Nettoyage r√©ussi pour la station {station_id} ({cleaned_marine_df.shape[0]} lignes)\")\n",
    "\n",
    "        marine_data_after_cleaning += cleaned_marine_df.shape[0]\n",
    "        meteo_data_after_cleaning += cleaned_meteo_df.shape[0]\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        ignored_buoys[station_id] = f\"Erreur lors du nettoyage: {e}\"\n",
    "        print(f\"‚ùå Erreur lors du nettoyage pour {station_id}: {e}\")\n",
    "\n",
    "# üî• Suppression des stations ignor√©es du dictionnaire principal\n",
    "for station_id in ignored_buoys:\n",
    "    buoy_datas.pop(station_id, None)\n",
    "\n",
    "len_cleaned_data = len([data for data in buoy_datas.values() if 'Cleaned Marine' in data and 'Cleaned Meteo' in data])\n",
    "\n",
    "# R√©sum√© final du nettoyage\n",
    "print(\"\\nüìä R√âSUM√â DU NETTOYAGE:\")\n",
    "print(f\"üìå Stations au d√©part : {stations_depart}\")\n",
    "print(f\"‚úÖ Stations nettoy√©es : {len_cleaned_data}\")\n",
    "print(f\"üèÅ Stations restantes apr√®s filtrage :\")\n",
    "\n",
    "for station_id, reason in ignored_buoys.items():\n",
    "    print(f\"üõë Station {station_id} ignor√©e: {reason}\")\n",
    "\n",
    "print(f\"\\nüßπ Cl√©s restantes dans buoy_datas apr√®s purge : {len(buoy_datas)} (attendu : {len_cleaned_data})\")\n",
    "\n",
    "\n",
    "print(f\"\\nüìä Nombre de donn√©es initiales : Marine: {initial_marine_data_count}, M√©t√©o: {initial_meteo_data_count}\")\n",
    "print(f\"üìä Nombre de donn√©es apr√®s nettoyage : Marine: {marine_data_after_cleaning}, M√©t√©o: {meteo_data_after_cleaning}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_buoys_missing_df_counts(buoy_datas, prefix=\"Cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_row_values(df_meteo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fusionner les df_meteo et df_marine sur 'Datetime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusion des DataFrames nettoy√©s\n",
    "print(\"\\nüîó FUSION DES DONN√âES MARINE + METEO PAR STATION\")\n",
    "\n",
    "merged_success_count = 0  # Compteur de fusions r√©ussies\n",
    "total_merged_rows = 0     # Total de lignes fusionn√©es\n",
    "\n",
    "for station_id, data in buoy_datas.items():\n",
    "    print(f\"\\nüîÑ Fusion des donn√©es pour la station {station_id}\")\n",
    "\n",
    "    cleaned_marine_df = data.get(\"Cleaned Marine\")\n",
    "    cleaned_meteo_df = data.get(\"Cleaned Meteo\")\n",
    "\n",
    "    if cleaned_marine_df is None or cleaned_meteo_df is None:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        merged_df = pd.merge(cleaned_marine_df, cleaned_meteo_df, on=\"Datetime\", how=\"inner\")\n",
    "\n",
    "        if merged_df.empty:\n",
    "            print(f\"‚ö†Ô∏è Station {station_id} fusionn√©e, mais r√©sultat vide apr√®s inner merge sur 'Datetime'\")\n",
    "        else:\n",
    "            buoy_datas[station_id][\"Merged\"] = merged_df\n",
    "            merged_success_count += 1\n",
    "            total_merged_rows += len(merged_df)\n",
    "            print(f\"‚úÖ Fusion r√©ussie pour la station {station_id} ({merged_df.shape[0]} lignes)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de la fusion pour {station_id}: {e}\")\n",
    "\n",
    "# R√©sum√© des fusions\n",
    "print(f\"\\nüì¶ Fusions r√©ussies : {merged_success_count}/{len_cleaned_data} stations\")\n",
    "print(f\"üìä Total de lignes fusionn√©es : {total_merged_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_marine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concat√©nation des DataFrames fusionn√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat√©nation des DataFrames fusionn√©s\n",
    "print(\"\\nüß¨ CONCAT√âNATION DES DONN√âES FUSIONN√âES EN UN SEUL DATAFRAME\")\n",
    "\n",
    "final_merged_df_list = []\n",
    "concat_success_count = 0\n",
    "concat_total_rows = 0\n",
    "\n",
    "for station_id, data in buoy_datas.items():\n",
    "    merged_df = data.get(\"Merged\")\n",
    "\n",
    "    if merged_df is None:\n",
    "        print(f\"‚ö†Ô∏è Station {station_id} ignor√©e pour concat√©nation: Donn√©es fusionn√©es manquantes\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        final_merged_df_list.append(merged_df)\n",
    "        concat_success_count += 1\n",
    "        concat_total_rows += len(merged_df)\n",
    "        print(f\"‚úÖ Concat√©nation r√©ussie pour la station {station_id} ({len(merged_df)} lignes)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de la concat√©nation pour {station_id}: {e}\")\n",
    "\n",
    "# Cr√©ation du DataFrame final unique\n",
    "try:\n",
    "    df_final = pd.concat(final_merged_df_list, ignore_index=True)\n",
    "    print(f\"\\nüßæ DataFrame final cr√©√© avec succ√®s ({df_final.shape[0]} lignes, {df_final.shape[1]} colonnes)\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Erreur lors de la cr√©ation du DataFrame final: {e}\")\n",
    "    df_final = None\n",
    "\n",
    "# R√©sum√©\n",
    "print(f\"\\nüì¶ Concat√©nations r√©ussies : {concat_success_count}/{merged_success_count}\")\n",
    "print(f\"üìä Total de lignes dans le DataFrame final : {concat_total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null values heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null value heatmap with sns\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "sns.heatmap(df_final.isnull(), cbar=False, cmap='viridis')\n",
    "\n",
    "plt.title('Null Values Heatmap', fontdict={'size': 20})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = handle_null_values(df_final)\n",
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hour Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_final = df_final[['Datetime', 'Lat', 'Lon'] + [col for col in df_final.columns if col not in ['Datetime', 'Lat', 'Lon']]]\n",
    "    # placer la colonne Datetime en %Y-%m-%d %H\n",
    "    \n",
    "    print(f\"üöÄ DataFrame filtr√©e pour ne garder que les lignes √† l'heure pile: {df_final.shape[0]} lignes\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "finally:\n",
    "    display(df_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns = [col.strip() for col in df_final.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming and deleting useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire de renommage des colonnes\n",
    "col_to_rename = {'temperature_2m': 'T¬∞(C¬∞)', \n",
    "                 'relative_humidity_2m': 'Relative Humidity (%)',\n",
    "                 'dew_point_2m': 'Dew Point (¬∞C)', \n",
    "                 'precipitation': 'Precipitations (mm)',  \n",
    "                 'pressure_msl':'Sea Level Pressure (hPa)', \n",
    "                 'cloud_cover_low':'Low Clouds (%)',\n",
    "                 'cloud_cover_mid' : 'Middle Clouds (%)', \n",
    "                 'cloud_cover_high' : 'High Clouds (%)', \n",
    "                 'visibility' : 'Visibility (km)', \n",
    "                 'wind_direction': 'Wind Direction (¬∞)',\n",
    "                 'wind_speed': 'Wind Speed (km/h)', \n",
    "                 'wind_gust': 'Wind Gusts (km/h)',\n",
    "                 'wind_speed_10m':'Wind Speed (10m)', \n",
    "                 'surface_pressure': 'Surface Pressure',\n",
    "                 'wave_height': 'Wave Height (m)', \n",
    "                 'average_wave_period': 'Average Wave Period (s)',\n",
    "                 'dominant_wave_direction': 'Dominant Wave Direction (¬∞)', \n",
    "                 'pressure': 'Pressure (hPa)',\n",
    "                 'air_temperature': 'Air T¬∞', \n",
    "                 'water_temperature': 'Water T¬∞', \n",
    "                 'Water_depth': 'Water Depth (m)', \n",
    "                 \"Air_temp_height\": \"Air T¬∞ Height\", \n",
    "                 \"Anemometer_height\": \"Anemometer Height (m)\", \n",
    "                 \"station_zone\": \"Station Zone\",\n",
    "                 \"Barometer_elevation\": \"Barometer Elevation\", \n",
    "                 \"sea_temp_depth\" : \"Sea Temperature Depth (m)\",\n",
    "                 \"cloud_cover\": \"Cloud Cover (%)\"\n",
    "                 }\n",
    "\n",
    "# Liste des colonnes √† supprimer\n",
    "cols_to_delete = ['soil_temperature_0cm', 'lat_buoy','lon_buoy', 'rain', \n",
    "                  'showers', 'is_day', 'soil_moisture_0_to_1cm']\n",
    "\t\n",
    "# Renommer les colonnes d'abord\n",
    "df_final = rename_columns(df_final, col_to_rename)\n",
    "# Ensuite, supprimer les colonnes non d√©sir√©es\n",
    "df_final = drop_columns_if_exist(df_final, cols_to_delete)\n",
    "try:\n",
    "    if df_final['Visibility (km)'].mean() > 1000:\n",
    "        df_final['Visibility (km)'] = df_final['Visibility (km)'] / 1000\n",
    "        print(\"Conversion de la visibilit√© de m√®tres √† kilom√®tres\")\n",
    "    df_final[\"T¬∞(C¬∞)\"] = round(df_final[\"T¬∞(C¬∞)\"], 2)\n",
    "    df_final[\"Wind Speed (10m)\"] = round(df_final[\"Wind Speed (10m)\"], 2)\n",
    "except Exception as e:\n",
    "    print(f\"ÔøΩÔøΩ Erreur lors du traitement des colonnes :\\n {e}\")\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "print(\"\\nColonnes apr√®s renommage et suppression :\")\n",
    "print(\"\\n\")\n",
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer les coordonn√©es en format float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    df_final[['Lat', 'Lon']] = df_final.apply(\n",
    "        lambda row: pd.Series(convert_coordinates(row['Lat'], row['Lon'])),\n",
    "        axis=1\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Erreur : {e}\")\n",
    "finally:\n",
    "    display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pressure and air temperatures are very close\n",
    "We'll make the average of them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_final['T¬∞(C¬∞)'] = (df_final['Air T¬∞'] + df_final['T¬∞(C¬∞)']) / 2\n",
    "    df_final['T¬∞(C¬∞)'] = df_final['T¬∞(C¬∞)'].round(2)\n",
    "    df_final.drop(columns=['Air T¬∞'], inplace=True)\n",
    "    \n",
    "    df_final['Sea Level Pressure (hPa)'] = round((df_final['Sea Level Pressure (hPa)'] + df_final['Surface Pressure']) / 2, 2)\n",
    "    df_final.drop(columns=['Surface Pressure'], inplace=True)\n",
    "except Exception as e:\n",
    "    print(f\"Erreur :\\n {e}\")\n",
    "    \n",
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Categorical Time columns and delete the 'm' in Water Depth Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Cr√©er une colonne temporaire pour acc√©der √† .dt\n",
    "\n",
    "    df_final['Year'] = df_final['Datetime'].dt.year.astype(str)\n",
    "    df_final['Month'] = df_final['Datetime'].dt.month_name()\n",
    "    df_final['Day'] = df_final['Datetime'].dt.day.astype(str)\n",
    "    df_final['Hour'] = df_final['Datetime'].dt.hour.astype(str)\n",
    "    df_final['DayOfWeek'] = df_final['Datetime'].dt.day_name()\n",
    "    df_final['DayPeriod'] = df_final['Datetime'].apply(\n",
    "        lambda x: 'Morning' if 6 <= x.hour < 12 else\n",
    "                  'Afternoon' if 12 <= x.hour < 18 else\n",
    "                  'Evening' if 18 <= x.hour < 22 else\n",
    "                  'Night'\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erreur :\\n {e}\")\n",
    "\n",
    "try:\n",
    "    # virer le m dans Water Depth avec regex lambda et passer la colonne en float\n",
    "    df_final['Water Depth (m)'] = df_final['Water Depth (m)'].apply(lambda x: re.sub(r'\\D', '', str(x)).strip())\n",
    "    df_final['Water Depth (m)'] = df_final['Water Depth (m)'].astype(float)\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "\n",
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check truth about Wind Speed Using another API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_42058 = df_final[df_final['Station ID'] == \"42058\"]\n",
    "df_42058.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requ√™te √† l'API Visual Crossing pour les donn√©es de v√©rification (1 / 24h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Chargement de la cl√© API ----\n",
    "vc_api_key_path = r\"c:\\Credentials\\visual_crossing_weather_api.json\"\n",
    "with open(vc_api_key_path, 'r') as file:\n",
    "    content = json.load(file)\n",
    "    vc_api_key = content[\"api_key\"]\n",
    "\n",
    "# ---- Extraire les coordonn√©es depuis la premi√®re ligne du DataFrame ----\n",
    "lat_42058, lon_42058 = None, None\n",
    "\n",
    "if not df_42058.empty:\n",
    "    first_row = df_42058.iloc[0]\n",
    "    lat_42058, lon_42058 = first_row[\"Lat\"], first_row[\"Lon\"]\n",
    "\n",
    "# ---- D√©finir les dates pour la requ√™te ----\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "last_month = (datetime.now() - timedelta(days=31)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# ---- Cr√©er le dossier de cache si n√©cessaire ----\n",
    "cache_dir = \"api_call_files\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# ---- D√©finir le fichier cache selon la position ----\n",
    "cache_file = os.path.join(cache_dir, f\"vc_meteo_{lat_42058}_{lon_42058}.csv\")\n",
    "\n",
    "# ---- V√©rifier si un cache r√©cent existe (moins de 24h) ----\n",
    "use_cache = False\n",
    "if os.path.exists(cache_file):\n",
    "    last_modified = datetime.fromtimestamp(os.path.getmtime(cache_file))\n",
    "    if datetime.now() - last_modified < timedelta(hours=24):\n",
    "        print(f\"üì¶ Cache d√©tect√© ({cache_file}), modifi√© le {last_modified.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        vc_meteo_df = pd.read_csv(cache_file)\n",
    "        print(\"‚úÖ Donn√©es m√©t√©o recharg√©es depuis le cache.\")\n",
    "        use_cache = True\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Cache trouv√© mais p√©rim√© (plus de 24h) ‚Üí nouvelle requ√™te API.\")\n",
    "\n",
    "# ---- Appel API si pas de cache valide ----\n",
    "if not use_cache and lat_42058 is not None and lon_42058 is not None:\n",
    "    url = f\"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/{lat_42058},{lon_42058}/{last_month}/{today}?unitGroup=metric&key={vc_api_key}&contentType=json\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            vc_meteo_data = response.json()\n",
    "            print(\"üåç Donn√©es m√©t√©o r√©cup√©r√©es depuis l'API Visual Crossing.\")\n",
    "\n",
    "            # ---- Extraire les donn√©es journali√®res et sauvegarder ----\n",
    "            if \"days\" in vc_meteo_data:\n",
    "                vc_meteo_df = pd.json_normalize(vc_meteo_data[\"days\"])\n",
    "                vc_meteo_df.to_csv(cache_file, index=False)\n",
    "                print(f\"üíæ Donn√©es sauvegard√©es dans le cache : {cache_file}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Le champ 'days' est absent de la r√©ponse API.\")\n",
    "        else:\n",
    "            print(f\"‚ùå √âchec de l‚Äôappel API ‚Äî code de statut : {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception lev√©e lors de la requ√™te API : {e}\")\n",
    "        \n",
    "\n",
    "# üìÅ Charger les donn√©es du CSV Visual Crossing\n",
    "vc_csv_path = f\"vc_api_call_files/vc_meteo_{lat_42058}_{lon_42058}.csv\"\n",
    "df_vc_meteo = pd.read_csv(vc_csv_path)\n",
    "try:\n",
    "    # mettre Datetime en index\n",
    "    df_vc_meteo.rename(columns={\"datetime\": \"Datetime\"}, inplace=True)  \n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erreur :\\n {e}\")\n",
    "\n",
    "df_vc_meteo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage du DataFrame retourn√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßº Nettoyage et transformation\n",
    "all_hours = []\n",
    "\n",
    "for i, row in df_vc_meteo.iterrows():\n",
    "    try:\n",
    "        hours_list = ast.literal_eval(row['hours'])\n",
    "\n",
    "        for hour_data in hours_list:\n",
    "            hour_data['Date'] = i  # ‚úÖ On met l'index courant, i.e. la date du jour\n",
    "            all_hours.append(hour_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur parsing ligne {i}: {e}\")\n",
    "\n",
    "df_vc_flat = pd.DataFrame(all_hours)\n",
    "\n",
    "# üïí Convertir le timestamp en datetime string\n",
    "df_vc_flat[\"Datetime\"] = pd.to_datetime(df_vc_flat[\"datetimeEpoch\"], unit=\"s\").dt.strftime(\"%Y-%m-%d-%H\")\n",
    "\n",
    "# üóìÔ∏è Filtrer sur les 30 derniers jours\n",
    "today = datetime.now()\n",
    "thirty_days_ago = today - timedelta(days=30)\n",
    "\n",
    "df_vc_flat['Date'] = pd.to_datetime(df_vc_flat['Date'])  # üëà Assurer que c'est bien du datetime\n",
    "df_vc_last_month = df_vc_flat[\n",
    "    (df_vc_flat['Date'] >= thirty_days_ago) & \n",
    "    (df_vc_flat['Date'] <= today)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renommage des colonnes pour faciliter la comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtrer les colonnes n√©cessaires\n",
    "try:\n",
    "    df_vc_last_month = df_vc_last_month[[\"Datetime\", \"temp\", \"humidity\", \"precip\", \"dew\", \"windgust\", \n",
    "                                     \"windspeed\", \"winddir\", \"pressure\", \"visibility\"]]\n",
    "except Exception as e:\n",
    "        print(f\"Erreur lors du filtrage des colonnes:\\n {e}\\n\")\n",
    "\n",
    "try:\n",
    "        df_vc_last_month[\"Datetime\"] = pd.to_datetime(df_vc_last_month[\"Datetime\"], errors='coerce')\n",
    "except Exception as e:\n",
    "        print(f\"Erreur lors du reformatage de la colonne Datetime:\\n {e}\\n\")        \n",
    "\n",
    "for col in df_vc_last_month.columns:\n",
    "        try:\n",
    "            # \n",
    "            if not \"Datetime\" in col:\n",
    "                if not col.startswith(\"VC_\"):\n",
    "                        rename_columns(df_vc_last_month, {col: f\"VC_{col}\"})\n",
    "        except Exception as e:\n",
    "                print(f\"Erreur lors du renommage de la colonne {col}:\\n {e}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enforce Datetime Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the Datetime column is correctly converted\n",
    "print(df_42058[\"Datetime\"].dtype)\n",
    "print(df_vc_last_month[\"Datetime\"].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge and Compare DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    \n",
    "    df_compare = pd.merge(df_42058, df_vc_last_month, on=\"Datetime\", how=\"inner\")\n",
    "    display(df_compare.columns)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparaison Wind Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Wind Speed Comparison\n",
    "    df_windspeed_compare = df_compare[['Wind Speed (km/h)', 'Wind Speed (10m)','VC_windspeed']]\n",
    "    #  Pressure Comparison\n",
    "    df_pressure_compare = df_compare[['Sea Level Pressure (hPa)','Pressure (hPa)', 'VC_pressure']]\n",
    "    # Dew Point Comparison\n",
    "    df_dew_compare = df_compare[['dewpoint','Dew Point (¬∞C)', 'VC_dew']]\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "df_windspeed_compare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_col_to_delete = ['Wind Speed (km/h)', 'Anemometer Height (m)']\n",
    "df_final = drop_columns_if_exist(df_final, wind_col_to_delete)\n",
    "# Arrondir les valeurs de df_final\n",
    "\n",
    "df_final = df_final.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparaison Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pressure_compare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = drop_columns_if_exist(df_final, ['Pressure (hPa)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dew_compare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_final['Dew Point (¬∞C)'] = df_final['dewpoint']\n",
    "    df_final = drop_columns_if_exist(df_final, ['dewpoint'])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename Final Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_final.rename(columns={'Air T¬∞ Height': 'Air T¬∞ Height (m)',\n",
    "                             'Barometer Elevation': 'Barometer Elevation (m)',\n",
    "                             'Water T¬∞': 'Water T¬∞ (¬∞C)'}, inplace=True)\n",
    "\n",
    "    print(\"Column 'Air T¬∞ Height' renamed to 'Air T¬∞ Height (m)'\")\n",
    "\n",
    "    print(\"Column 'Barometer Elevation' renamed to 'Barometer Elevation (m)'\")\n",
    "    print(\"Column 'Water T¬∞' renamed to 'Water T¬∞ (¬∞C)'\")\n",
    "    df_final['T¬∞(C¬∞)'] = df_final['T¬∞(C¬∞)'].round(2)\n",
    "\n",
    "    print(\"Column 'T¬∞(C¬∞)' rounded to 2 decimal places\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction des types des colonnes\n",
    "try:\n",
    "    df_final['Lat'] = df_final['Lat'].astype(str)\n",
    "    df_final['Lon'] = df_final['Lon'].astype(str)\n",
    "    print(df_final.dtypes)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marine_cleaned = df_final[['Datetime', 'Lat', 'Lon',\n",
    "       'Wave Height (m)', 'Average Wave Period (s)',\n",
    "       'Dominant Wave Direction (¬∞)', 'Water T¬∞ (¬∞C)', 'Water Depth (m)',\n",
    "       'Station ID', 'Station Zone', 'Sea Temperature Depth (m)',\n",
    "       'Barometer Elevation (m)', \n",
    "       'Sea Level Pressure (hPa)', 'Year', 'Month', 'Day', 'Hour', 'DayOfWeek',\n",
    "       'DayPeriod']]\n",
    "df_meteo_cleaned = df_final[['Datetime', 'Lat', 'Lon', \n",
    "                             'Wind Direction (¬∞)', 'Wind Gusts (km/h)',\n",
    "       'Station ID', 'Station Zone', 'Sea Temperature Depth (m)',\n",
    "       'Barometer Elevation (m)', 'Air T¬∞ Height (m)', 'T¬∞(C¬∞)',\n",
    "\n",
    "       'Relative Humidity (%)', 'Dew Point (¬∞C)', 'Precipitations (mm)',\n",
    "       'Cloud Cover (%)', 'Low Clouds (%)',\n",
    "       'Middle Clouds (%)', 'High Clouds (%)', 'Visibility (km)',\n",
    "       'Wind Speed (10m)', 'Year', 'Month', 'Day', 'Hour', 'DayOfWeek',\n",
    "       'DayPeriod']]\n",
    "\n",
    "print(f\"DataFrame Marine Index: {df_marine_cleaned.index}\")\n",
    "print(f\"DataFrame Meteo Index: {df_meteo_cleaned.index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marine_table_name = f\"cleaned_marine_data\"\n",
    "create_table_in_mysql(table_name=table_staging_marine_name, \n",
    "                      engine=engine_staging, \n",
    "            df=df_marine_cleaned)\n",
    "\n",
    "insert_new_rows(table_name=table_staging_marine_name, \n",
    "                engine=engine_staging, \n",
    "        df=df_marine_cleaned, ref= 'Datetime')\n",
    "\n",
    "\n",
    "\n",
    "meteo_table_name = f\"cleaned_meteo_data\"\n",
    "create_table_in_mysql(table_name= table_staging_meteo_name, \n",
    "        engine=engine_staging, df=df_meteo_cleaned)\n",
    "\n",
    "insert_new_rows(table_name=table_staging_meteo_name, \n",
    "                engine=engine_staging, \n",
    "                df=df_meteo_cleaned, ref= 'Datetime')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_folder = 'csv'\n",
    "base_filename = \"Cleaned_Data_Ocean_Meteo_ETL\"\n",
    "\n",
    "# V√©rifier dans le dossier si de tels fichiers existent d√©j√† avec ce nom de base\n",
    "existing_files = os.listdir(csv_folder)\n",
    "\n",
    "# Filtrer ceux qui commencent par le nom de base et ont l'extension .csv\n",
    "matching_files = [f for f in existing_files if f.startswith(base_filename) and f.endswith('.csv')]\n",
    "\n",
    "if len(matching_files) > 0:\n",
    "    # Cr√©er une liste vide pour stocker les DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # Lire chaque fichier CSV et ajouter la DataFrame √† la liste\n",
    "    for file in matching_files:\n",
    "        file_path = os.path.join(csv_folder, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concat√©ner toutes les DataFrames en une seule\n",
    "    df_concat_existing_files = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Concat√©ner les donn√©es existantes avec df_final\n",
    "    concatenated_df = pd.concat([df_concat_existing_files, df_final], ignore_index=True)\n",
    "    \n",
    "    # Supprimer les doublons bas√©s sur la colonne 'Datetime'\n",
    "    concatenated_df = concatenated_df.drop_duplicates(subset='Datetime', keep='last')\n",
    "    concatenated_df['Datetime'] = pd.to_datetime(concatenated_df['Datetime'], errors='coerce')\n",
    "\n",
    "    # Sauvegarder le CSV concat√©n√© sans doublons\n",
    "    save_concat_csv(concatenated_df, csv_folder=csv_folder, base_filename=\"Cleaned_Data_Ocean_Meteo_ETL\")\n",
    "    \n",
    "    # Supprimer les fichiers CSV d'ingr√©dients\n",
    "    for file in matching_files:\n",
    "        os.remove(os.path.join(csv_folder, file))\n",
    "else:\n",
    "    # Si aucun fichier existant, juste sauvegarder df_final\n",
    "    save_concat_csv(df_final, csv_folder=csv_folder, base_filename=\"Cleaned_Data_Ocean_Meteo_ETL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index  |  Datetime             |  Lat    |  Lon      |  Wind Direction (¬∞)  |  Wind Gusts (km/h)  |  Wave Height (m)  |  Average Wave Period (s)  |  Dominant Wave Direction (¬∞)  |  Water T¬∞ (¬∞C)  |  Water Depth (m)  |  Station ID  |  Station Zone                              |  Sea Temperature Depth (m)  |  Barometer Elevation (m)  |  Air T¬∞ Height (m)  |  T¬∞(C¬∞)  |  Relative Humidity (%)  |  Dew Point (¬∞C)  |  Precipitations (mm)  |  Sea Level Pressure (hPa)  |  Cloud Cover (%)  |  Low Clouds (%)  |  Middle Clouds (%)  |  High Clouds (%)  |  Visibility (km)  |  Wind Speed (10m)  |  Year  |  Month  |  Day  |  Hour  |  DayOfWeek  |  DayPeriod\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "0      |  2025-04-29 20:00:00  |  32.5   |  -79.1    |  100.0               |  4.0                |  1.1              |  5.1                      |  128.0                        |  22.4           |  35.0             |  41004       |  edisto                                    |  1.5                        |  2.7                      |  3.7                |  21.93   |  62.0                   |  13.9            |  0.0                  |  1023.6                    |  1.0              |  1.0             |  0.0                |  0.0              |  30.3             |  12.29             |  2025  |  April  |  29   |  20    |  Tuesday    |  Evening  \n",
      "1      |  2025-04-29 19:00:00  |  32.5   |  -79.1    |  110.0               |  4.0                |  1.1              |  5.1                      |  128.0                        |  22.4           |  35.0             |  41004       |  edisto                                    |  1.5                        |  2.7                      |  3.7                |  21.9    |  62.0                   |  14.1            |  0.0                  |  1024.2                    |  0.0              |  0.0             |  0.0                |  0.0              |  29.9             |  10.11             |  2025  |  April  |  29   |  19    |  Tuesday    |  Evening  \n",
      "2      |  2025-04-29 18:00:00  |  32.5   |  -79.1    |  100.0               |  4.0                |  1.1              |  5.1                      |  128.0                        |  22.2           |  35.0             |  41004       |  edisto                                    |  1.5                        |  2.7                      |  3.7                |  21.8    |  60.0                   |  13.4            |  0.0                  |  1025.8                    |  0.0              |  0.0             |  0.0                |  0.0              |  31.1             |  13.4              |  2025  |  April  |  29   |  18    |  Tuesday    |  Evening  \n",
      "3      |  2025-04-29 17:00:00  |  32.5   |  -79.1    |  90.0                |  5.0                |  1.1              |  5.1                      |  128.0                        |  22.2           |  35.0             |  41004       |  edisto                                    |  1.5                        |  2.7                      |  3.7                |  21.7    |  62.0                   |  13.1            |  0.0                  |  1026.3                    |  0.0              |  0.0             |  0.0                |  0.0              |  29.9             |  10.82             |  2025  |  April  |  29   |  17    |  Tuesday    |  Afternoon\n",
      "4      |  2025-04-29 16:00:00  |  32.5   |  -79.1    |  90.0                |  5.0                |  1.1              |  5.1                      |  128.0                        |  22.1           |  35.0             |  41004       |  edisto                                    |  1.5                        |  2.7                      |  3.7                |  21.65   |  62.0                   |  13.3            |  0.0                  |  1026.4                    |  0.0              |  0.0             |  0.0                |  0.0              |  29.8             |  14.04             |  2025  |  April  |  29   |  16    |  Tuesday    |  Afternoon\n",
      "5      |  2025-04-29 15:00:00  |  32.5   |  -79.1    |  100.0               |  6.0                |  1.1              |  5.1                      |  128.0                        |  22.0           |  35.0             |  41004       |  edisto                                    |  1.5                        |  2.7                      |  3.7                |  21.75   |  63.0                   |  13.4            |  0.0                  |  1026.7                    |  0.0              |  0.0             |  0.0                |  0.0              |  29.5             |  18.73             |  2025  |  April  |  29   |  15    |  Tuesday    |  Afternoon\n",
      "6      |  2025-04-29 14:00:00  |  32.5   |  -79.1    |  100.0               |  6.0                |  1.1              |  5.1                      |  128.0                        |  21.8           |  35.0             |  41004       |  edisto                                    |  1.5                        |  2.7                      |  3.7                |  21.78   |  61.0                   |  13.5            |  0.0                  |  1026.4                    |  2.0              |  2.0             |  0.0                |  0.0              |  30.8             |  19.44             |  2025  |  April  |  29   |  14    |  Tuesday    |  Afternoon\n",
      "7      |  2025-04-29 13:00:00  |  32.5   |  -79.1    |  100.0               |  7.0                |  1.1              |  5.1                      |  128.0                        |  19.6           |  35.0             |  41004       |  edisto                                    |  1.5                        |  2.7                      |  3.7                |  21.7    |  63.0                   |  13.9            |  0.0                  |  1026.0                    |  1.0              |  1.0             |  0.0                |  0.0              |  29.3             |  19.85             |  2025  |  April  |  29   |  13    |  Tuesday    |  Afternoon\n",
      "8      |  2025-04-29 12:00:00  |  32.5   |  -79.1    |  100.0               |  7.0                |  1.1              |  5.1                      |  128.0                        |  19.6           |  35.0             |  41004       |  edisto                                    |  1.5                        |  2.7                      |  3.7                |  21.8    |  62.0                   |  13.9            |  0.0                  |  1025.7                    |  2.0              |  2.0             |  0.0                |  0.0              |  29.9             |  17.03             |  2025  |  April  |  29   |  12    |  Tuesday    |  Afternoon\n",
      "9      |  2025-04-29 11:00:00  |  32.5   |  -79.1    |  110.0               |  7.0                |  1.1              |  5.1                      |  128.0                        |  19.6           |  35.0             |  41004       |  edisto                                    |  1.5                        |  2.7                      |  3.7                |  21.72   |  63.0                   |  14.4            |  0.0                  |  1025.4                    |  14.0             |  14.0            |  0.0                |  0.0              |  29.2             |  18.36             |  2025  |  April  |  29   |  11    |  Tuesday    |  Morning  \n"
     ]
    }
   ],
   "source": [
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cr√©ation des DataFrames pour les tables du DW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colonnes d√©riv√©es\n",
    "df_finalfor_DW = df_final.copy()\n",
    "df_finalfor_DW['Date ID'] = df_final['Datetime'].dt.strftime('%Y%m%d%H')\n",
    "df_finalfor_DW['Unique ID'] = df_final['Datetime'].dt.strftime('%Y%m%d%H%M') + df_final['Station ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Datetime', 'Lat', 'Lon', 'Station ID', 'Station Zone',\n",
       "       'Sea Temperature Depth (m)', 'Barometer Elevation (m)',\n",
       "       'Air T¬∞ Height (m)', 'Year', 'Month', 'Day', 'Hour', 'DayOfWeek',\n",
       "       'DayPeriod', 'Date ID', 'Unique ID'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imprimer toutes les colonnes numeriquesq\n",
    "num_cols = df_finalfor_DW.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "#imprimer toutes les colonnes non-numeriques\n",
    "non_num_cols = df_finalfor_DW.select_dtypes(exclude=[np.number]).columns\n",
    "non_num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index  |  Datetime             |  Lat    |  Lon      |  Wind Direction (¬∞)  |  Wind Gusts (km/h)  |  Wave Height (m)  |  Average Wave Period (s)  |  Dominant Wave Direction (¬∞)  |  Water T¬∞ (¬∞C)  |  Water Depth (m)  |  Station ID  |  Station Zone                              |  Sea Temperature Depth (m)  |  Barometer Elevation (m)  |  Air T¬∞ Height (m)  |  T¬∞(C¬∞)  |  Relative Humidity (%)  |  Dew Point (¬∞C)  |  Precipitations (mm)  |  Sea Level Pressure (hPa)  |  Cloud Cover (%)  |  Low Clouds (%)  |  Middle Clouds (%)  |  High Clouds (%)  |  Visibility (km)  |  Wind Speed (10m)  |  Year  |  Month  |  Day  |  Hour  |  DayOfWeek  |  DayPeriod  |  Date ID     |  Unique ID        \n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "0      |  2025-04-29 20:00:00  |  32.5   |  -79.1    |  100.0               |  4.0                |  1.1              |  5.1                      |  128.0                        |  22.4           |  35.0             |  41004       |  edisto                                    |  1.5                        |  2.7                      |  3.7                |  21.93   |  62.0                   |  13.9            |  0.0                  |  1023.6                    |  1.0              |  1.0             |  0.0                |  0.0              |  30.3             |  12.29             |  2025  |  April  |  29   |  20    |  Tuesday    |  Evening    |  2025042920  |  20250429200041004\n",
      "1      |  2025-04-29 19:00:00  |  32.5   |  -79.1    |  110.0               |  4.0                |  1.1              |  5.1                      |  128.0                        |  22.4           |  35.0             |  41004       |  edisto                                    |  1.5                        |  2.7                      |  3.7                |  21.9    |  62.0                   |  14.1            |  0.0                  |  1024.2                    |  0.0              |  0.0             |  0.0                |  0.0              |  29.9             |  10.11             |  2025  |  April  |  29   |  19    |  Tuesday    |  Evening    |  2025042919  |  20250429190041004\n",
      "2      |  2025-04-29 18:00:00  |  32.5   |  -79.1    |  100.0               |  4.0                |  1.1              |  5.1                      |  128.0                        |  22.2           |  35.0             |  41004       |  edisto                                    |  1.5                        |  2.7                      |  3.7                |  21.8    |  60.0                   |  13.4            |  0.0                  |  1025.8                    |  0.0              |  0.0             |  0.0                |  0.0              |  31.1             |  13.4              |  2025  |  April  |  29   |  18    |  Tuesday    |  Evening    |  2025042918  |  20250429180041004\n",
      "3      |  2025-04-29 17:00:00  |  32.5   |  -79.1    |  90.0                |  5.0                |  1.1              |  5.1                      |  128.0                        |  22.2           |  35.0             |  41004       |  edisto                                    |  1.5                        |  2.7                      |  3.7                |  21.7    |  62.0                   |  13.1            |  0.0                  |  1026.3                    |  0.0              |  0.0             |  0.0                |  0.0              |  29.9             |  10.82             |  2025  |  April  |  29   |  17    |  Tuesday    |  Afternoon  |  2025042917  |  20250429170041004\n",
      "4      |  2025-04-29 16:00:00  |  32.5   |  -79.1    |  90.0                |  5.0                |  1.1              |  5.1                      |  128.0                        |  22.1           |  35.0             |  41004       |  edisto                                    |  1.5                        |  2.7                      |  3.7                |  21.65   |  62.0                   |  13.3            |  0.0                  |  1026.4                    |  0.0              |  0.0             |  0.0                |  0.0              |  29.8             |  14.04             |  2025  |  April  |  29   |  16    |  Tuesday    |  Afternoon  |  2025042916  |  20250429160041004\n",
      "5      |  2025-04-29 15:00:00  |  32.5   |  -79.1    |  100.0               |  6.0                |  1.1              |  5.1                      |  128.0                        |  22.0           |  35.0             |  41004       |  edisto                                    |  1.5                        |  2.7                      |  3.7                |  21.75   |  63.0                   |  13.4            |  0.0                  |  1026.7                    |  0.0              |  0.0             |  0.0                |  0.0              |  29.5             |  18.73             |  2025  |  April  |  29   |  15    |  Tuesday    |  Afternoon  |  2025042915  |  20250429150041004\n",
      "6      |  2025-04-29 14:00:00  |  32.5   |  -79.1    |  100.0               |  6.0                |  1.1              |  5.1                      |  128.0                        |  21.8           |  35.0             |  41004       |  edisto                                    |  1.5                        |  2.7                      |  3.7                |  21.78   |  61.0                   |  13.5            |  0.0                  |  1026.4                    |  2.0              |  2.0             |  0.0                |  0.0              |  30.8             |  19.44             |  2025  |  April  |  29   |  14    |  Tuesday    |  Afternoon  |  2025042914  |  20250429140041004\n",
      "7      |  2025-04-29 13:00:00  |  32.5   |  -79.1    |  100.0               |  7.0                |  1.1              |  5.1                      |  128.0                        |  19.6           |  35.0             |  41004       |  edisto                                    |  1.5                        |  2.7                      |  3.7                |  21.7    |  63.0                   |  13.9            |  0.0                  |  1026.0                    |  1.0              |  1.0             |  0.0                |  0.0              |  29.3             |  19.85             |  2025  |  April  |  29   |  13    |  Tuesday    |  Afternoon  |  2025042913  |  20250429130041004\n",
      "8      |  2025-04-29 12:00:00  |  32.5   |  -79.1    |  100.0               |  7.0                |  1.1              |  5.1                      |  128.0                        |  19.6           |  35.0             |  41004       |  edisto                                    |  1.5                        |  2.7                      |  3.7                |  21.8    |  62.0                   |  13.9            |  0.0                  |  1025.7                    |  2.0              |  2.0             |  0.0                |  0.0              |  29.9             |  17.03             |  2025  |  April  |  29   |  12    |  Tuesday    |  Afternoon  |  2025042912  |  20250429120041004\n",
      "9      |  2025-04-29 11:00:00  |  32.5   |  -79.1    |  110.0               |  7.0                |  1.1              |  5.1                      |  128.0                        |  19.6           |  35.0             |  41004       |  edisto                                    |  1.5                        |  2.7                      |  3.7                |  21.72   |  63.0                   |  14.4            |  0.0                  |  1025.4                    |  14.0             |  14.0            |  0.0                |  0.0              |  29.2             |  18.36             |  2025  |  April  |  29   |  11    |  Tuesday    |  Morning    |  2025042911  |  20250429110041004\n"
     ]
    }
   ],
   "source": [
    "display_row_values(df_finalfor_DW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### DimStation ##################################################################################################\n",
    "\n",
    "df_station = df_finalfor_DW[[\n",
    "    'Station ID', 'Station Zone', 'Lat', 'Lon'\n",
    "]].copy().drop_duplicates()\n",
    "\n",
    "###################################### DimTime ##################################################################################################\n",
    "\n",
    "df_time = df_finalfor_DW[['Datetime', 'Year', 'Month', 'DayOfWeek', 'Day', 'Hour', 'DayPeriod']].copy().drop_duplicates()\n",
    "\n",
    "########################################## Facts Meteo #########################################################################\n",
    "\n",
    "df_facts_meteo = df_finalfor_DW[[\n",
    "    'Unique ID', # PK\n",
    "    'T¬∞(C¬∞)', 'Relative Humidity (%)', 'Dew Point (¬∞C)', 'Precipitations (mm)',\n",
    "    'Sea Level Pressure (hPa)', 'Low Clouds (%)', 'Middle Clouds (%)', 'High Clouds (%)',\n",
    "    \"Cloud Cover (%)\", 'Visibility (km)', 'Wind Speed (10m)', 'Wind Direction (¬∞)',\n",
    "    'Wind Gusts (km/h)', 'Barometer Elevation (m)', 'Air T¬∞ Height (m)',\n",
    "    \n",
    "    'Station ID', # FK\n",
    "    'Datetime' # FK\n",
    "]].copy().drop_duplicates()\n",
    "\n",
    "########################################## Facts Ocean #########################################################################\n",
    "\n",
    "df_facts_ocean = df_finalfor_DW[[\n",
    "    'Unique ID', # PK\n",
    "    'Wave Height (m)', 'Average Wave Period (s)', 'Dominant Wave Direction (¬∞)',\n",
    "    'Water T¬∞ (¬∞C)', 'Water Depth (m)', 'Sea Temperature Depth (m)',\n",
    "    \n",
    "    'Station ID', # FK\n",
    "    'Datetime'  # FK\n",
    "]].copy().drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index  |  Datetime             |  Year  |  Month  |  DayOfWeek  |  Day  |  Hour  |  DayPeriod\n",
      "--------------------------------------------------------------------------------------\n",
      "0      |  2025-04-29 20:00:00  |  2025  |  April  |  Tuesday    |  29   |  20    |  Evening  \n",
      "1      |  2025-04-29 19:00:00  |  2025  |  April  |  Tuesday    |  29   |  19    |  Evening  \n",
      "2      |  2025-04-29 18:00:00  |  2025  |  April  |  Tuesday    |  29   |  18    |  Evening  \n",
      "3      |  2025-04-29 17:00:00  |  2025  |  April  |  Tuesday    |  29   |  17    |  Afternoon\n",
      "4      |  2025-04-29 16:00:00  |  2025  |  April  |  Tuesday    |  29   |  16    |  Afternoon\n",
      "5      |  2025-04-29 15:00:00  |  2025  |  April  |  Tuesday    |  29   |  15    |  Afternoon\n",
      "6      |  2025-04-29 14:00:00  |  2025  |  April  |  Tuesday    |  29   |  14    |  Afternoon\n",
      "7      |  2025-04-29 13:00:00  |  2025  |  April  |  Tuesday    |  29   |  13    |  Afternoon\n",
      "8      |  2025-04-29 12:00:00  |  2025  |  April  |  Tuesday    |  29   |  12    |  Afternoon\n",
      "9      |  2025-04-29 11:00:00  |  2025  |  April  |  Tuesday    |  29   |  11    |  Morning  \n"
     ]
    }
   ],
   "source": [
    "display_row_values(df_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index  |  Station ID  |  Station Zone                              |  Lat    |  Lon    \n",
      "-----------------------------------------------------------------------------------\n",
      "0      |  41004       |  edisto                                    |  32.5   |  -79.1  \n",
      "1094   |  41008       |  grays reef                                |  31.4   |  -80.87 \n",
      "2194   |  41044       |  ne st martin                              |  21.58  |  -58.63 \n",
      "3286   |  41046       |  east bahamas                              |  23.86  |  -68.34 \n",
      "3474   |  41049       |  south bermuda                             |  27.5   |  -62.27 \n",
      "4574   |  42001       |  mid gulf                                  |  25.93  |  -89.66 \n",
      "5673   |  42002       |  west gulf                                 |  25.95  |  -93.78 \n",
      "6770   |  42020       |  corpus christi, tx                        |  26.97  |  -96.68 \n",
      "7870   |  42036       |  west tampa                                |  28.5   |  -84.5  \n",
      "8969   |  42056       |  yucatan basin                             |  19.82  |  -84.98 \n"
     ]
    }
   ],
   "source": [
    "display_row_values(df_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index  |  Unique ID          |  T¬∞(C¬∞)  |  Relative Humidity (%)  |  Dew Point (¬∞C)  |  Precipitations (mm)  |  Sea Level Pressure (hPa)  |  Low Clouds (%)  |  Middle Clouds (%)  |  High Clouds (%)  |  Cloud Cover (%)  |  Visibility (km)  |  Wind Speed (10m)  |  Wind Direction (¬∞)  |  Wind Gusts (km/h)  |  Barometer Elevation (m)  |  Air T¬∞ Height (m)  |  Station ID  |  Datetime           \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "0      |  20250429200041004  |  21.93   |  62.0                   |  13.9            |  0.0                  |  1023.6                    |  1.0             |  0.0                |  0.0              |  1.0              |  30.3             |  12.29             |  100.0               |  4.0                |  2.7                      |  3.7                |  41004       |  2025-04-29 20:00:00\n",
      "1      |  20250429190041004  |  21.9    |  62.0                   |  14.1            |  0.0                  |  1024.2                    |  0.0             |  0.0                |  0.0              |  0.0              |  29.9             |  10.11             |  110.0               |  4.0                |  2.7                      |  3.7                |  41004       |  2025-04-29 19:00:00\n",
      "2      |  20250429180041004  |  21.8    |  60.0                   |  13.4            |  0.0                  |  1025.8                    |  0.0             |  0.0                |  0.0              |  0.0              |  31.1             |  13.4              |  100.0               |  4.0                |  2.7                      |  3.7                |  41004       |  2025-04-29 18:00:00\n",
      "3      |  20250429170041004  |  21.7    |  62.0                   |  13.1            |  0.0                  |  1026.3                    |  0.0             |  0.0                |  0.0              |  0.0              |  29.9             |  10.82             |  90.0                |  5.0                |  2.7                      |  3.7                |  41004       |  2025-04-29 17:00:00\n",
      "4      |  20250429160041004  |  21.65   |  62.0                   |  13.3            |  0.0                  |  1026.4                    |  0.0             |  0.0                |  0.0              |  0.0              |  29.8             |  14.04             |  90.0                |  5.0                |  2.7                      |  3.7                |  41004       |  2025-04-29 16:00:00\n",
      "5      |  20250429150041004  |  21.75   |  63.0                   |  13.4            |  0.0                  |  1026.7                    |  0.0             |  0.0                |  0.0              |  0.0              |  29.5             |  18.73             |  100.0               |  6.0                |  2.7                      |  3.7                |  41004       |  2025-04-29 15:00:00\n",
      "6      |  20250429140041004  |  21.78   |  61.0                   |  13.5            |  0.0                  |  1026.4                    |  2.0             |  0.0                |  0.0              |  2.0              |  30.8             |  19.44             |  100.0               |  6.0                |  2.7                      |  3.7                |  41004       |  2025-04-29 14:00:00\n",
      "7      |  20250429130041004  |  21.7    |  63.0                   |  13.9            |  0.0                  |  1026.0                    |  1.0             |  0.0                |  0.0              |  1.0              |  29.3             |  19.85             |  100.0               |  7.0                |  2.7                      |  3.7                |  41004       |  2025-04-29 13:00:00\n",
      "8      |  20250429120041004  |  21.8    |  62.0                   |  13.9            |  0.0                  |  1025.7                    |  2.0             |  0.0                |  0.0              |  2.0              |  29.9             |  17.03             |  100.0               |  7.0                |  2.7                      |  3.7                |  41004       |  2025-04-29 12:00:00\n",
      "9      |  20250429110041004  |  21.72   |  63.0                   |  14.4            |  0.0                  |  1025.4                    |  14.0            |  0.0                |  0.0              |  14.0             |  29.2             |  18.36             |  110.0               |  7.0                |  2.7                      |  3.7                |  41004       |  2025-04-29 11:00:00\n"
     ]
    }
   ],
   "source": [
    "display_row_values(df_facts_meteo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index  |  Unique ID          |  Wave Height (m)  |  Average Wave Period (s)  |  Dominant Wave Direction (¬∞)  |  Water T¬∞ (¬∞C)  |  Water Depth (m)  |  Sea Temperature Depth (m)  |  Station ID  |  Datetime           \n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "0      |  20250429200041004  |  1.1              |  5.1                      |  128.0                        |  22.4           |  35.0             |  1.5                        |  41004       |  2025-04-29 20:00:00\n",
      "1      |  20250429190041004  |  1.1              |  5.1                      |  128.0                        |  22.4           |  35.0             |  1.5                        |  41004       |  2025-04-29 19:00:00\n",
      "2      |  20250429180041004  |  1.1              |  5.1                      |  128.0                        |  22.2           |  35.0             |  1.5                        |  41004       |  2025-04-29 18:00:00\n",
      "3      |  20250429170041004  |  1.1              |  5.1                      |  128.0                        |  22.2           |  35.0             |  1.5                        |  41004       |  2025-04-29 17:00:00\n",
      "4      |  20250429160041004  |  1.1              |  5.1                      |  128.0                        |  22.1           |  35.0             |  1.5                        |  41004       |  2025-04-29 16:00:00\n",
      "5      |  20250429150041004  |  1.1              |  5.1                      |  128.0                        |  22.0           |  35.0             |  1.5                        |  41004       |  2025-04-29 15:00:00\n",
      "6      |  20250429140041004  |  1.1              |  5.1                      |  128.0                        |  21.8           |  35.0             |  1.5                        |  41004       |  2025-04-29 14:00:00\n",
      "7      |  20250429130041004  |  1.1              |  5.1                      |  128.0                        |  19.6           |  35.0             |  1.5                        |  41004       |  2025-04-29 13:00:00\n",
      "8      |  20250429120041004  |  1.1              |  5.1                      |  128.0                        |  19.6           |  35.0             |  1.5                        |  41004       |  2025-04-29 12:00:00\n",
      "9      |  20250429110041004  |  1.1              |  5.1                      |  128.0                        |  19.6           |  35.0             |  1.5                        |  41004       |  2025-04-29 11:00:00\n"
     ]
    }
   ],
   "source": [
    "display_row_values(df_facts_ocean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doublons dans df_station : 0\n",
      "Doublons dans df_time : 0\n",
      "Doublons dans df_facts_meteo : 0\n",
      "Doublons dans df_facts_ocean : 0\n"
     ]
    }
   ],
   "source": [
    "# V√©rifier les doublons\n",
    "print(f\"Doublons dans df_station : {df_station.duplicated().sum()}\")\n",
    "print(f\"Doublons dans df_time : {df_time.duplicated().sum()}\")\n",
    "print(f\"Doublons dans df_facts_meteo : {df_facts_meteo.duplicated().sum()}\")\n",
    "print(f\"Doublons dans df_facts_ocean : {df_facts_ocean.duplicated().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking dim_station DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "\n",
      "Station ID      25\n",
      "Station Zone    25\n",
      "Lat             24\n",
      "Lon             25\n",
      "dtype: int64\n",
      "Index  |  Station ID  |  Station Zone                              |  Lat    |  Lon    \n",
      "-----------------------------------------------------------------------------------\n",
      "0      |  41004       |  edisto                                    |  32.5   |  -79.1  \n",
      "1094   |  41008       |  grays reef                                |  31.4   |  -80.87 \n",
      "2194   |  41044       |  ne st martin                              |  21.58  |  -58.63 \n",
      "3286   |  41046       |  east bahamas                              |  23.86  |  -68.34 \n",
      "3474   |  41049       |  south bermuda                             |  27.5   |  -62.27 \n",
      "4574   |  42001       |  mid gulf                                  |  25.93  |  -89.66 \n",
      "5673   |  42002       |  west gulf                                 |  25.95  |  -93.78 \n",
      "6770   |  42020       |  corpus christi, tx                        |  26.97  |  -96.68 \n",
      "7870   |  42036       |  west tampa                                |  28.5   |  -84.5  \n",
      "8969   |  42056       |  yucatan basin                             |  19.82  |  -84.98 \n"
     ]
    }
   ],
   "source": [
    "# check unique values\n",
    "print(f\"{df_station.shape[0]}\\n\\n{df_station.nunique()}\")\n",
    "display_row_values(df_station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cr√©ation de la table dim_station et insertion des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ V√©rification de l'existence de la table...\n",
      "‚ö†Ô∏è La table 'dim_station' existe d√©j√†.\n",
      "üîé 25 valeurs d√©j√† pr√©sentes dans la colonne 'Station ID'.\n",
      "‚ö†Ô∏è Aucune nouvelle ligne √† ins√©rer.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    create_table_in_mysql(df=df_station, engine=engine_DW, table_name=table_dim_station_name)\n",
    "    insert_new_rows(df=df_station, engine=engine_DW, table_name=table_dim_station_name, ref='Station ID')\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking dim_time DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1101\n",
      "\n",
      "Datetime     1101\n",
      "Year            1\n",
      "Month           2\n",
      "DayOfWeek       7\n",
      "Day            31\n",
      "Hour           24\n",
      "DayPeriod       4\n",
      "dtype: int64\n",
      "Index  |  Datetime             |  Year  |  Month  |  DayOfWeek  |  Day  |  Hour  |  DayPeriod\n",
      "--------------------------------------------------------------------------------------\n",
      "0      |  2025-04-29 20:00:00  |  2025  |  April  |  Tuesday    |  29   |  20    |  Evening  \n",
      "1      |  2025-04-29 19:00:00  |  2025  |  April  |  Tuesday    |  29   |  19    |  Evening  \n",
      "2      |  2025-04-29 18:00:00  |  2025  |  April  |  Tuesday    |  29   |  18    |  Evening  \n",
      "3      |  2025-04-29 17:00:00  |  2025  |  April  |  Tuesday    |  29   |  17    |  Afternoon\n",
      "4      |  2025-04-29 16:00:00  |  2025  |  April  |  Tuesday    |  29   |  16    |  Afternoon\n",
      "5      |  2025-04-29 15:00:00  |  2025  |  April  |  Tuesday    |  29   |  15    |  Afternoon\n",
      "6      |  2025-04-29 14:00:00  |  2025  |  April  |  Tuesday    |  29   |  14    |  Afternoon\n",
      "7      |  2025-04-29 13:00:00  |  2025  |  April  |  Tuesday    |  29   |  13    |  Afternoon\n",
      "8      |  2025-04-29 12:00:00  |  2025  |  April  |  Tuesday    |  29   |  12    |  Afternoon\n",
      "9      |  2025-04-29 11:00:00  |  2025  |  April  |  Tuesday    |  29   |  11    |  Morning  \n"
     ]
    }
   ],
   "source": [
    "print(f\"{df_time.shape[0]}\\n\\n{df_time.nunique()}\")\n",
    "display_row_values(df_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cr√©ation de la table dim_time et insertion des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ V√©rification de l'existence de la table...\n",
      "‚ö†Ô∏è La table 'dim_time' existe d√©j√†.\n",
      "üîé 1099 valeurs d√©j√† pr√©sentes dans la colonne 'Datetime'.\n",
      "üöÄ Insertion de 2 lignes dans 'dim_time'...\n",
      "‚úÖ Insertion termin√©e avec succ√®s.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    create_table_in_mysql(df=df_time, engine=engine_DW, table_name=table_dim_time_name)\n",
    "    insert_new_rows(df=df_time, engine=engine_DW, table_name=table_dim_time_name, ref='Datetime')\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Meteo Facts DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26559\n",
      "\n",
      "Unique ID                   26559\n",
      "T¬∞(C¬∞)                       1700\n",
      "Relative Humidity (%)          73\n",
      "Dew Point (¬∞C)                364\n",
      "Precipitations (mm)            72\n",
      "Sea Level Pressure (hPa)      560\n",
      "Low Clouds (%)                101\n",
      "Middle Clouds (%)             101\n",
      "High Clouds (%)               101\n",
      "Cloud Cover (%)               101\n",
      "Visibility (km)              1221\n",
      "Wind Speed (10m)             3355\n",
      "Wind Direction (¬∞)             36\n",
      "Wind Gusts (km/h)              24\n",
      "Barometer Elevation (m)         3\n",
      "Air T¬∞ Height (m)               3\n",
      "Station ID                     25\n",
      "Datetime                     1101\n",
      "dtype: int64\n",
      "Index  |  Unique ID          |  T¬∞(C¬∞)  |  Relative Humidity (%)  |  Dew Point (¬∞C)  |  Precipitations (mm)  |  Sea Level Pressure (hPa)  |  Low Clouds (%)  |  Middle Clouds (%)  |  High Clouds (%)  |  Cloud Cover (%)  |  Visibility (km)  |  Wind Speed (10m)  |  Wind Direction (¬∞)  |  Wind Gusts (km/h)  |  Barometer Elevation (m)  |  Air T¬∞ Height (m)  |  Station ID  |  Datetime           \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "0      |  20250429200041004  |  21.93   |  62.0                   |  13.9            |  0.0                  |  1023.6                    |  1.0             |  0.0                |  0.0              |  1.0              |  30.3             |  12.29             |  100.0               |  4.0                |  2.7                      |  3.7                |  41004       |  2025-04-29 20:00:00\n",
      "1      |  20250429190041004  |  21.9    |  62.0                   |  14.1            |  0.0                  |  1024.2                    |  0.0             |  0.0                |  0.0              |  0.0              |  29.9             |  10.11             |  110.0               |  4.0                |  2.7                      |  3.7                |  41004       |  2025-04-29 19:00:00\n",
      "2      |  20250429180041004  |  21.8    |  60.0                   |  13.4            |  0.0                  |  1025.8                    |  0.0             |  0.0                |  0.0              |  0.0              |  31.1             |  13.4              |  100.0               |  4.0                |  2.7                      |  3.7                |  41004       |  2025-04-29 18:00:00\n",
      "3      |  20250429170041004  |  21.7    |  62.0                   |  13.1            |  0.0                  |  1026.3                    |  0.0             |  0.0                |  0.0              |  0.0              |  29.9             |  10.82             |  90.0                |  5.0                |  2.7                      |  3.7                |  41004       |  2025-04-29 17:00:00\n",
      "4      |  20250429160041004  |  21.65   |  62.0                   |  13.3            |  0.0                  |  1026.4                    |  0.0             |  0.0                |  0.0              |  0.0              |  29.8             |  14.04             |  90.0                |  5.0                |  2.7                      |  3.7                |  41004       |  2025-04-29 16:00:00\n",
      "5      |  20250429150041004  |  21.75   |  63.0                   |  13.4            |  0.0                  |  1026.7                    |  0.0             |  0.0                |  0.0              |  0.0              |  29.5             |  18.73             |  100.0               |  6.0                |  2.7                      |  3.7                |  41004       |  2025-04-29 15:00:00\n",
      "6      |  20250429140041004  |  21.78   |  61.0                   |  13.5            |  0.0                  |  1026.4                    |  2.0             |  0.0                |  0.0              |  2.0              |  30.8             |  19.44             |  100.0               |  6.0                |  2.7                      |  3.7                |  41004       |  2025-04-29 14:00:00\n",
      "7      |  20250429130041004  |  21.7    |  63.0                   |  13.9            |  0.0                  |  1026.0                    |  1.0             |  0.0                |  0.0              |  1.0              |  29.3             |  19.85             |  100.0               |  7.0                |  2.7                      |  3.7                |  41004       |  2025-04-29 13:00:00\n",
      "8      |  20250429120041004  |  21.8    |  62.0                   |  13.9            |  0.0                  |  1025.7                    |  2.0             |  0.0                |  0.0              |  2.0              |  29.9             |  17.03             |  100.0               |  7.0                |  2.7                      |  3.7                |  41004       |  2025-04-29 12:00:00\n",
      "9      |  20250429110041004  |  21.72   |  63.0                   |  14.4            |  0.0                  |  1025.4                    |  14.0            |  0.0                |  0.0              |  14.0             |  29.2             |  18.36             |  110.0               |  7.0                |  2.7                      |  3.7                |  41004       |  2025-04-29 11:00:00\n"
     ]
    }
   ],
   "source": [
    "print(f\"{df_facts_meteo.shape[0]}\\n\\n{df_facts_meteo.nunique()}\")\n",
    "display_row_values(df_facts_meteo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cr√©ation de la table facts_meteo et insertion des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ V√©rification de l'existence de la table...\n",
      "‚ö†Ô∏è La table 'facts_meteo' existe d√©j√†.\n",
      "üîé 1099 valeurs d√©j√† pr√©sentes dans la colonne 'Datetime'.\n",
      "üöÄ Insertion de 50 lignes dans 'facts_meteo'...\n",
      "‚úÖ Insertion termin√©e avec succ√®s.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    create_table_in_mysql(df=df_facts_meteo, engine=engine_DW, table_name=table_facts_meteo_name)\n",
    "    insert_new_rows(df=df_facts_meteo, engine=engine_DW, table_name=table_facts_meteo_name, ref='Datetime')\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Ocean Facts DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Datetime', 'Lat', 'Lon', 'Wind Direction (¬∞)', 'Wind Gusts (km/h)',\n",
       "       'Wave Height (m)', 'Average Wave Period (s)',\n",
       "       'Dominant Wave Direction (¬∞)', 'Water T¬∞ (¬∞C)', 'Water Depth (m)',\n",
       "       'Station ID', 'Station Zone', 'Sea Temperature Depth (m)',\n",
       "       'Barometer Elevation (m)', 'Air T¬∞ Height (m)', 'T¬∞(C¬∞)',\n",
       "       'Relative Humidity (%)', 'Dew Point (¬∞C)', 'Precipitations (mm)',\n",
       "       'Sea Level Pressure (hPa)', 'Cloud Cover (%)', 'Low Clouds (%)',\n",
       "       'Middle Clouds (%)', 'High Clouds (%)', 'Visibility (km)',\n",
       "       'Wind Speed (10m)', 'Year', 'Month', 'Day', 'Hour', 'DayOfWeek',\n",
       "       'DayPeriod', 'Date ID', 'Unique ID'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_finalfor_DW.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26559\n",
      "\n",
      "Unique ID                      26559\n",
      "Wave Height (m)                   38\n",
      "Average Wave Period (s)           57\n",
      "Dominant Wave Direction (¬∞)      180\n",
      "Water T¬∞ (¬∞C)                    250\n",
      "Water Depth (m)                   25\n",
      "Sea Temperature Depth (m)          3\n",
      "Station ID                        25\n",
      "Datetime                        1101\n",
      "dtype: int64\n",
      "Index  |  Unique ID          |  Wave Height (m)  |  Average Wave Period (s)  |  Dominant Wave Direction (¬∞)  |  Water T¬∞ (¬∞C)  |  Water Depth (m)  |  Sea Temperature Depth (m)  |  Station ID  |  Datetime           \n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "0      |  20250429200041004  |  1.1              |  5.1                      |  128.0                        |  22.4           |  35.0             |  1.5                        |  41004       |  2025-04-29 20:00:00\n",
      "1      |  20250429190041004  |  1.1              |  5.1                      |  128.0                        |  22.4           |  35.0             |  1.5                        |  41004       |  2025-04-29 19:00:00\n",
      "2      |  20250429180041004  |  1.1              |  5.1                      |  128.0                        |  22.2           |  35.0             |  1.5                        |  41004       |  2025-04-29 18:00:00\n",
      "3      |  20250429170041004  |  1.1              |  5.1                      |  128.0                        |  22.2           |  35.0             |  1.5                        |  41004       |  2025-04-29 17:00:00\n",
      "4      |  20250429160041004  |  1.1              |  5.1                      |  128.0                        |  22.1           |  35.0             |  1.5                        |  41004       |  2025-04-29 16:00:00\n",
      "5      |  20250429150041004  |  1.1              |  5.1                      |  128.0                        |  22.0           |  35.0             |  1.5                        |  41004       |  2025-04-29 15:00:00\n",
      "6      |  20250429140041004  |  1.1              |  5.1                      |  128.0                        |  21.8           |  35.0             |  1.5                        |  41004       |  2025-04-29 14:00:00\n",
      "7      |  20250429130041004  |  1.1              |  5.1                      |  128.0                        |  19.6           |  35.0             |  1.5                        |  41004       |  2025-04-29 13:00:00\n",
      "8      |  20250429120041004  |  1.1              |  5.1                      |  128.0                        |  19.6           |  35.0             |  1.5                        |  41004       |  2025-04-29 12:00:00\n",
      "9      |  20250429110041004  |  1.1              |  5.1                      |  128.0                        |  19.6           |  35.0             |  1.5                        |  41004       |  2025-04-29 11:00:00\n"
     ]
    }
   ],
   "source": [
    "print(f\"{df_facts_ocean.shape[0]}\\n\\n{df_facts_ocean.nunique()}\")\n",
    "display_row_values(df_facts_ocean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cr√©ation de la table facts_ocean et insertion des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ V√©rification de l'existence de la table...\n",
      "‚ö†Ô∏è La table 'facts_ocean' existe d√©j√†.\n",
      "üîé 1099 valeurs d√©j√† pr√©sentes dans la colonne 'Datetime'.\n",
      "üöÄ Insertion de 50 lignes dans 'facts_ocean'...\n",
      "‚úÖ Insertion termin√©e avec succ√®s.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    create_table_in_mysql(df=df_facts_ocean, engine=engine_DW, table_name=table_facts_marine_name)\n",
    "    insert_new_rows(df=df_facts_ocean, engine=engine_DW, table_name=table_facts_marine_name, ref='Datetime')\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
