{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "from imports import *\n",
    "from functions import *\n",
    "from database import engine, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Available Stations ID List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all stations and some metadata as a Pandas DataFrame\n",
    "stations_df = api.stations()\n",
    "# parse the response as a dictionary\n",
    "stations_df = api.stations(as_df=True)\n",
    "\n",
    "print(len(stations_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Buoys by Remarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_error_url_list = []\n",
    "\n",
    "# Liste de mots Ã  rechercher dans la colonne \"Remark\"\n",
    "blacklist = [\"Failure\", \"ceased\", \"failed\", \"recovered\", \"stopped\", 'adrift']\n",
    "stations_id_set = set()\n",
    "\n",
    "print(f'Avant Filtre: {stations_df.shape[0]}')\n",
    "\n",
    "# Liste pour collecter les indices Ã  supprimer\n",
    "indices_a_supprimer = []\n",
    "\n",
    "# Parcours des lignes de la DataFrame\n",
    "for idx, row in stations_df.iterrows():\n",
    "    station_id = row[\"Station\"]\n",
    "    station_Location = row[\"Hull No./Config and Location\"]  # Extraire la valeur de la cellule pour chaque ligne\n",
    "    \n",
    "    # Extraction du nom de la station si un \")\" est trouvÃ©\n",
    "    if \")\" in station_Location:\n",
    "        station_name = station_Location.split(')')[1].rstrip(\" )\")  # On enlÃ¨ve l'espace et la parenthÃ¨se en fin de chaÃ®ne\n",
    "    else:\n",
    "        station_name = station_Location.strip()  # Si pas de \")\", on garde toute la chaÃ®ne\n",
    "\n",
    "    station_name = station_name.rstrip(\" )\").replace(\"(\", \"\").replace(\")\", \"\").strip()\n",
    "\n",
    "    # Nettoyage final pour enlever toute parenthÃ¨se ou espace en fin de nom\n",
    "    station_name = station_name.rstrip(\" )\")\n",
    "\n",
    "    # VÃ©rifier si \"Remark\" n'est pas NaN et si un des Ã©lÃ©ments de blacklist est dans \"Remark\"\n",
    "    if isinstance(row[\"Remark\"], str) and any(blacklist_word.lower() in row[\"Remark\"].lower() for blacklist_word in blacklist):\n",
    "        # Ajouter l'index Ã  la liste\n",
    "        indices_a_supprimer.append(idx)\n",
    "        url = get_buoy_url(station_id)\n",
    "        access_error_url_list.append(url)\n",
    "    else:\n",
    "        pass\n",
    "# Supprimer les lignes aprÃ¨s la boucle\n",
    "stations_df.drop(index=indices_a_supprimer, inplace=True)\n",
    "\n",
    "print(f'AprÃ¨s Filtre: {stations_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Buoys_datas Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire pour stocker les DataFrames, clÃ© : ID de la bouÃ©e, valeur : DataFrame\n",
    "buoy_datas = {}\n",
    "buoy_list = []\n",
    "\n",
    "# Parcours de chaque bouÃ©e dans stations_df\n",
    "for index, row in stations_df.iterrows():\n",
    "    buoy_id = row['Station']\n",
    "    metadata = get_station_metadata(buoy_id)\n",
    "\n",
    "    # âœ… RÃ©cupÃ©rer les donnÃ©es sous forme de dictionnaire\n",
    "    buoy_info = parse_buoy_json(metadata)\n",
    "\n",
    "    # âœ… Stocker directement les donnÃ©es dans buoy_datas\n",
    "    buoy_datas[buoy_id] = buoy_info\n",
    "    buoy_list.append(buoy_id)\n",
    "\n",
    "# Affichage du nombre de bouÃ©es rÃ©ussies et Ã©chouÃ©es\n",
    "print(f\"Nombre de bouÃ©es traitÃ©es : {len(buoy_datas)}\\n\")\n",
    "\n",
    "# Afficher le contenu de buoy_datas\n",
    "\n",
    "first_key =next(iter(buoy_datas))\n",
    "first_key\n",
    "buoy_datas[first_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecte de donnÃ©es marines et mÃ©tÃ©os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸš€ DÃ©marrage du processus\n",
    "print(\"\\nðŸš€ DÃ©marrage du processus de collecte des donnÃ©es...\\n\")\n",
    "\n",
    "# Initialisation des compteurs\n",
    "marine_data_collected_successfully = marine_data_collected_failed = 0\n",
    "meteo_data_collected_successfully = meteo_data_collected_failed = 0\n",
    "\n",
    "success = False\n",
    "total_stations = stations_df.shape[0]\n",
    "count = 0\n",
    "\n",
    "# ðŸ”„ Parcours des bouÃ©es / stations\n",
    "for idx, row in stations_df.iterrows():\n",
    "    buoy_id = row[\"Station\"]\n",
    "\n",
    "    ######### ðŸŒŠ MARINE DATA #########\n",
    "    try:\n",
    "        df_marine = NDBC.realtime_observations(buoy_id)\n",
    "        if df_marine is None or df_marine.empty:\n",
    "            marine_data_collected_failed += 1\n",
    "            continue\n",
    "\n",
    "        marine_data_collected_successfully += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Erreur collecte marine {buoy_id}: {e}\")\n",
    "        marine_data_collected_failed += 1\n",
    "        continue\n",
    "\n",
    "    # Ajout des mÃ©tadonnÃ©es\n",
    "    try:\n",
    "        buoy_info = buoy_datas.get(buoy_id, {})\n",
    "        Lat, Lon = buoy_info.get('lat_buoy'), buoy_info.get('lon_buoy')\n",
    "        if Lat is None or Lon is None:\n",
    "            raise ValueError(f\"DonnÃ©es manquantes pour {buoy_id}\")\n",
    "\n",
    "        df_marine['Lat'] = Lat\n",
    "        df_marine['Lon'] = Lon\n",
    "        df_marine['Water_depth'] = buoy_info.get('Water_depth', None)\n",
    "        df_marine.columns = ['Datetime' if 'date' in col.lower() or 'time' in col.lower() else col for col in df_marine.columns]\n",
    "        df_marine['Datetime'] = df_marine['Datetime'].dt.tz_localize(None)\n",
    "\n",
    "        buoy_datas[buoy_id][\"Marine\"] = df_marine\n",
    "\n",
    "        station_zone = safe_get(parse_buoy_json(get_station_metadata(buoy_id)), \"station_zone\")\n",
    "        Bronze_Marine_table_Name = f\"br_{buoy_id}_marine_{station_zone}\".replace('.', '_').replace('-', '_').replace(' ', '_').lower()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Erreur mÃ©tadonnÃ©es marine {buoy_id}: {e}\")\n",
    "        marine_data_collected_failed += 1\n",
    "        continue\n",
    "\n",
    "    ######### â›… METEO DATA #########\n",
    "    try:\n",
    "        df_meteo = meteo_api_request([Lat, Lon])\n",
    "        if df_meteo is None or df_meteo.empty:\n",
    "            meteo_data_collected_failed += 1\n",
    "            continue\n",
    "        \n",
    "        rename_columns(df_meteo, {'date':'Datetime'})\n",
    "        df_meteo.columns = ['Datetime' if 'date' in col.lower() or 'time' in col.lower() else col for col in df_meteo.columns]\n",
    "        df_meteo['Datetime'] = df_meteo['Datetime'].dt.tz_localize(None)\n",
    "    \n",
    "        buoy_datas[buoy_id][\"Meteo\"] = df_meteo\n",
    "        meteo_data_collected_successfully += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Erreur collecte mÃ©tÃ©o {buoy_id}: {e}\")\n",
    "        meteo_data_collected_failed += 1\n",
    "        continue\n",
    "\n",
    "# Retirer les bouÃ©es avec des DataFrames vides ou None\n",
    "buoy_datas = {buoy_id: data for buoy_id, data in buoy_datas.items() \n",
    "              if \"Marine\" in data and data[\"Marine\"] is not None and not data[\"Marine\"].empty\n",
    "              and \"Meteo\" in data and data[\"Meteo\"] is not None and not data[\"Meteo\"].empty}\n",
    "\n",
    "# ðŸ”š RÃ©sumÃ© final\n",
    "\n",
    "print(\"\\nðŸ“ RÃ©sumÃ© final :\")\n",
    "print(f\"ðŸŒŠ Marine - Collecte âœ… {marine_data_collected_successfully} âŒ {marine_data_collected_failed}\")\n",
    "print(f\"â›… MÃ©tÃ©o - Collecte âœ… {meteo_data_collected_successfully} âŒ {meteo_data_collected_failed}\")\n",
    "\n",
    "# Afficher la longueur du dictionnaire (nombre de bouÃ©es avec des donnÃ©es valides)\n",
    "print(f\"\\nðŸ“Š Nombre de bouÃ©es avec des donnÃ©es valides : {len(buoy_datas)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Enrichment with MetaDatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_not_include = ['lon_buoy', \"lat_buoy\", \"url\"]\n",
    "for buoy_id, value in buoy_datas.items():\n",
    "    print(f\"\\nðŸ” Traitement de la Station ID: {buoy_id}\")\n",
    "\n",
    "    marine_df = buoy_datas[buoy_id][\"Marine\"]\n",
    "    meteo_df = buoy_datas[buoy_id][\"Meteo\"]\n",
    "\n",
    "    try:\n",
    "        # RÃ©cupÃ©rer les mÃ©tadonnÃ©es de la station\n",
    "        buoy_metadata = get_station_metadata(buoy_id)\n",
    "        parsed_data = parse_buoy_json(buoy_metadata)\n",
    "\n",
    "        # Mise Ã  jour du dictionnaire avec les mÃ©tadonnÃ©es\n",
    "        data = buoy_datas[buoy_id]\n",
    "        data.update(parsed_data)\n",
    "        \n",
    "        # Ajouter les mÃ©tadonnÃ©es comme nouvelles colonnes dans marine_df\n",
    "        if marine_df is not None:\n",
    "            marine_df[\"Station ID\"] = str(buoy_id)\n",
    "            for key, value in parsed_data.items():\n",
    "                # VÃ©rifier si la clÃ© n'est pas dans la liste des exclusions\n",
    "                if key not in list_not_include:\n",
    "                    marine_df[key] = value\n",
    "                    print(f\"âœ… Colonne '{key}' ajoutÃ©e au DataFrame de la station {buoy_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur pour la station {buoy_id}: {e}\")\n",
    "\n",
    "# VÃ©rification de l'ajout des colonnes en prenant un id au hasard\n",
    "station_id = random.choice(list(buoy_datas.keys()))\n",
    "marine_df = buoy_datas[station_id][\"Marine\"]\n",
    "\n",
    "if marine_df is not None:\n",
    "    print(\"\\nColonnes ajoutÃ©es au DataFrame de la station\", station_id)\n",
    "    print(marine_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_buoys_missing_df_counts(buoy_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_marine.columns)\n",
    "display(df_meteo.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_columns_oceanography = [\n",
    "    'wind_direction',             \n",
    "    'wind_speed',                 \n",
    "    'wave_height',                   \n",
    "    'pressure',                   \n",
    "    'air_temperature',            \n",
    "    'water_temperature',          \n",
    "    'Datetime',\n",
    "    'Lat',\n",
    "    'Lon'                 \n",
    "]\n",
    "\n",
    "important_columns_meteorology = [\n",
    "    'temperature_2m',             \n",
    "    'relative_humidity_2m',       \n",
    "    'dew_point_2m',               \n",
    "    'precipitation',              \n",
    "    'pressure_msl',               \n",
    "    'cloud_cover',                \n",
    "    'wind_speed_10m',             \n",
    "    'Datetime'\n",
    "]\n",
    "\n",
    "stations_depart = len(buoy_datas)\n",
    "ignored_buoys = {}  # Dictionary to track ignored buoys and their reasons\n",
    "\n",
    "for station_id, data in buoy_datas.items():\n",
    "    print(f\"\\nðŸ”„ Nettoyage des donnÃ©es pour la station {station_id}\")\n",
    "\n",
    "    marine_df = data.get(\"Marine\")\n",
    "    meteo_df = data.get(\"Meteo\")\n",
    "\n",
    "    if marine_df is None or meteo_df is None:\n",
    "        ignored_buoys[station_id] = \"Marine DataFrame ou Meteo DataFrame manquant(e)\"\n",
    "        print(f\"âš ï¸ Station {station_id} ignorÃ©e: Marine DataFrame ou Meteo DataFrame manquant(e)\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Nettoyage des DataFrames\n",
    "        cleaned_marine_df = handle_null_values(marine_df)\n",
    "        cleaned_meteo_df = handle_null_values(meteo_df)\n",
    "        # VÃ©rification des colonnes importantes aprÃ¨s nettoyage\n",
    "        marine_columns_ok = all(col in cleaned_marine_df.columns for col in important_columns_oceanography)\n",
    "        meteo_columns_ok = all(col in cleaned_meteo_df.columns for col in important_columns_meteorology)\n",
    "\n",
    "        # Track which columns are missing\n",
    "        missing_marine_columns = [col for col in important_columns_oceanography if col not in cleaned_marine_df.columns]\n",
    "        missing_meteo_columns = [col for col in important_columns_meteorology if col not in cleaned_meteo_df.columns]\n",
    "\n",
    "        if missing_marine_columns or missing_meteo_columns:\n",
    "            ignored_buoys[station_id] = f\"Colonnes manquantes: Marine: {missing_marine_columns}, Meteo: {missing_meteo_columns}\"\n",
    "            print(f\"âš ï¸ Station {station_id} ignorÃ©e: Colonnes manquantes - Marine: {missing_marine_columns}, Meteo: {missing_meteo_columns}\")\n",
    "            continue\n",
    "\n",
    "        # Ajouter le DataFrame nettoyÃ© au dictionnaire des rÃ©sultats\n",
    "        buoy_datas[station_id]['Cleaned Marine'] = cleaned_marine_df\n",
    "        buoy_datas[station_id]['Cleaned Meteo'] = cleaned_meteo_df\n",
    "        print(f\"âœ… Nettoyage rÃ©ussi pour la station {station_id} ({cleaned_marine_df.shape[0]} lignes)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        ignored_buoys[station_id] = f\"Erreur lors du nettoyage: {e}\"\n",
    "        print(f\"âŒ Erreur lors du nettoyage pour {station_id}: {e}\")\n",
    "\n",
    "# ðŸ”¥ Suppression des stations ignorÃ©es du dictionnaire principal\n",
    "for station_id in ignored_buoys:\n",
    "    buoy_datas.pop(station_id, None)\n",
    "\n",
    "len_cleaned_data = len([data for data in buoy_datas.values() if 'Cleaned Marine' in data and 'Cleaned Meteo' in data])\n",
    "\n",
    "# RÃ©sumÃ© final du nettoyage\n",
    "print(\"\\nðŸ“Š RÃ‰SUMÃ‰ DU NETTOYAGE:\")\n",
    "print(f\"ðŸ“Œ Stations au dÃ©part : {stations_depart}\")\n",
    "print(f\"âœ… Stations nettoyÃ©es : {len_cleaned_data}\")\n",
    "print(f\"ðŸ Stations restantes aprÃ¨s filtrage :\")\n",
    "\n",
    "for station_id, reason in ignored_buoys.items():\n",
    "    print(f\"ðŸ›‘ Station {station_id} ignorÃ©e: {reason}\")\n",
    "\n",
    "print(f\"\\nðŸ§¹ ClÃ©s restantes dans buoy_datas aprÃ¨s purge : {len(buoy_datas)} (attendu : {len_cleaned_data})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_buoys_missing_df_counts(buoy_datas, prefix=\"Cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_row_values(df_meteo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DÃ©finir la taille de la figure\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Utiliser seaborn pour crÃ©er la heatmap des valeurs manquantes\n",
    "sns.heatmap(df_meteo.isnull(), \n",
    "            cbar=False, \n",
    "            cmap='viridis', \n",
    "            yticklabels=False)\n",
    "\n",
    "plt.title('Carte thermique des valeurs manquantes dans df_meteo')\n",
    "plt.xlabel('Colonnes')\n",
    "plt.ylabel('EntrÃ©es')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusionner les df_meteo et df_marine sur 'Datetime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusion des DataFrames nettoyÃ©s\n",
    "print(\"\\nðŸ”— FUSION DES DONNÃ‰ES MARINE + METEO PAR STATION\")\n",
    "\n",
    "merged_success_count = 0  # Compteur de fusions rÃ©ussies\n",
    "total_merged_rows = 0     # Total de lignes fusionnÃ©es\n",
    "\n",
    "for station_id, data in buoy_datas.items():\n",
    "    print(f\"\\nðŸ”„ Fusion des donnÃ©es pour la station {station_id}\")\n",
    "\n",
    "    cleaned_marine_df = data.get(\"Cleaned Marine\")\n",
    "    cleaned_meteo_df = data.get(\"Cleaned Meteo\")\n",
    "\n",
    "    if cleaned_marine_df is None or cleaned_meteo_df is None:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        merged_df = pd.merge(cleaned_marine_df, cleaned_meteo_df, on=\"Datetime\", how=\"inner\")\n",
    "\n",
    "        if merged_df.empty:\n",
    "            print(f\"âš ï¸ Station {station_id} fusionnÃ©e, mais rÃ©sultat vide aprÃ¨s inner merge sur 'Datetime'\")\n",
    "        else:\n",
    "            buoy_datas[station_id][\"Merged\"] = merged_df\n",
    "            merged_success_count += 1\n",
    "            total_merged_rows += len(merged_df)\n",
    "            print(f\"âœ… Fusion rÃ©ussie pour la station {station_id} ({merged_df.shape[0]} lignes)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur lors de la fusion pour {station_id}: {e}\")\n",
    "\n",
    "# RÃ©sumÃ© des fusions\n",
    "print(f\"\\nðŸ“¦ Fusions rÃ©ussies : {merged_success_count}/{len_cleaned_data} stations\")\n",
    "print(f\"ðŸ“Š Total de lignes fusionnÃ©es : {total_merged_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConcatÃ©nation des DataFrames fusionnÃ©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConcatÃ©nation des DataFrames fusionnÃ©s\n",
    "print(\"\\nðŸ§¬ CONCATÃ‰NATION DES DONNÃ‰ES FUSIONNÃ‰ES EN UN SEUL DATAFRAME\")\n",
    "\n",
    "final_merged_df_list = []\n",
    "concat_success_count = 0\n",
    "concat_total_rows = 0\n",
    "\n",
    "for station_id, data in buoy_datas.items():\n",
    "    merged_df = data.get(\"Merged\")\n",
    "\n",
    "    if merged_df is None:\n",
    "        print(f\"âš ï¸ Station {station_id} ignorÃ©e pour concatÃ©nation: DonnÃ©es fusionnÃ©es manquantes\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        final_merged_df_list.append(merged_df)\n",
    "        concat_success_count += 1\n",
    "        concat_total_rows += len(merged_df)\n",
    "        print(f\"âœ… ConcatÃ©nation rÃ©ussie pour la station {station_id} ({len(merged_df)} lignes)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur lors de la concatÃ©nation pour {station_id}: {e}\")\n",
    "\n",
    "# CrÃ©ation du DataFrame final unique\n",
    "try:\n",
    "    df_final = pd.concat(final_merged_df_list, ignore_index=True)\n",
    "    print(f\"\\nðŸ§¾ DataFrame final crÃ©Ã© avec succÃ¨s ({df_final.shape[0]} lignes, {df_final.shape[1]} colonnes)\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Erreur lors de la crÃ©ation du DataFrame final: {e}\")\n",
    "    df_final = None\n",
    "\n",
    "# RÃ©sumÃ©\n",
    "print(f\"\\nðŸ“¦ ConcatÃ©nations rÃ©ussies : {concat_success_count}/{merged_success_count}\")\n",
    "print(f\"ðŸ“Š Total de lignes dans le DataFrame final : {concat_total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null values heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null value heatmap with sns\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "sns.heatmap(df_final.isnull(), cbar=False, cmap='viridis')\n",
    "\n",
    "plt.title('Null Values Heatmap', fontdict={'size': 20})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = handle_null_values(df_final)\n",
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hour Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_final = df_final[['Datetime', 'Lat', 'Lon'] + [col for col in df_final.columns if col not in ['Datetime', 'Lat', 'Lon']]]\n",
    "    # placer la colonne Datetime en %Y-%m-%d %H\n",
    "    \n",
    "    print(f\"ðŸš€ DataFrame filtrÃ©e pour ne garder que les lignes Ã  l'heure pile: {df_final.shape[0]} lignes\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "finally:\n",
    "    display(df_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns = [col.strip() for col in df_final.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renaming and deleting useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire de renommage des colonnes\n",
    "col_to_rename = {'temperature_2m': 'TÂ°(CÂ°)', \n",
    "                 'relative_humidity_2m': 'Relative Humidity (%)',\n",
    "                 'dew_point_2m': 'Dew Point (Â°C)', \n",
    "                 'precipitation': 'Precipitations (mm)',  \n",
    "                 'pressure_msl':'Sea Level Pressure (hPa)', \n",
    "                 'cloud_cover_low':'Low Clouds (%)',\n",
    "                 'cloud_cover_mid' : 'Middle Clouds (%)', \n",
    "                 'cloud_cover_high' : 'High Clouds (%)', \n",
    "                 'visibility' : 'Visibility (km)', \n",
    "                 'wind_direction': 'Wind Direction (Â°)',\n",
    "                 'wind_speed': 'Wind Speed (km/h)', \n",
    "                 'wind_gust': 'Wind Gusts (km/h)',\n",
    "                 'wind_speed_10m':'Wind Speed (10m)', \n",
    "                 'surface_pressure': 'Surface Pressure',\n",
    "                 'wave_height': 'Wave Height (m)', \n",
    "                 'average_wave_period': 'Average Wave Period (s)',\n",
    "                 'dominant_wave_direction': 'Dominant Wave Direction (Â°)', \n",
    "                 'pressure': 'Pressure (hPa)',\n",
    "                 'air_temperature': 'Air TÂ°', \n",
    "                 'water_temperature': 'Water TÂ°', \n",
    "                 'Water_depth': 'Water Depth (m)', \n",
    "                 \"Air_temp_height\": \"Air TÂ° Height\", \n",
    "                 \"Anemometer_height\": \"Anemometer Height (m)\", \n",
    "                 \"station_zone\": \"Station Zone\",\n",
    "                 \"Barometer_elevation\": \"Barometer Elevation\", \n",
    "                 \"sea_temp_depth\" : \"Sea Temperature Depth (m)\",\n",
    "                 \"cloud_cover\": \"Cloud Cover (%)\"\n",
    "                 }\n",
    "\n",
    "# Liste des colonnes Ã  supprimer\n",
    "cols_to_delete = ['soil_temperature_0cm', 'lat_buoy','lon_buoy', 'rain', \n",
    "                  'showers', 'is_day', 'soil_moisture_0_to_1cm']\n",
    "\t\n",
    "# Renommer les colonnes d'abord\n",
    "df_final = rename_columns(df_final, col_to_rename)\n",
    "# Ensuite, supprimer les colonnes non dÃ©sirÃ©es\n",
    "df_final = drop_columns_if_exist(df_final, cols_to_delete)\n",
    "try:\n",
    "    if df_final['Visibility (km)'].mean() > 1000:\n",
    "        df_final['Visibility (km)'] = df_final['Visibility (km)'] / 1000\n",
    "        print(\"Conversion de la visibilitÃ© de mÃ¨tres Ã  kilomÃ¨tres\")\n",
    "    df_final[\"TÂ°(CÂ°)\"] = round(df_final[\"TÂ°(CÂ°)\"], 2)\n",
    "    df_final[\"Wind Speed (10m)\"] = round(df_final[\"Wind Speed (10m)\"], 2)\n",
    "except Exception as e:\n",
    "    print(f\"ï¿½ï¿½ Erreur lors du traitement des colonnes :\\n {e}\")\n",
    "\n",
    "# Afficher les rÃ©sultats\n",
    "print(\"\\nColonnes aprÃ¨s renommage et suppression :\")\n",
    "print(\"\\n\")\n",
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer les coordonnÃ©es en format float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    df_final[['Lat', 'Lon']] = df_final.apply(\n",
    "        lambda row: pd.Series(convert_coordinates(row['Lat'], row['Lon'])),\n",
    "        axis=1\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Erreur : {e}\")\n",
    "finally:\n",
    "    display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pressure and air temperatures are very close\n",
    "We'll make the average of them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_final['TÂ°(CÂ°)'] = (df_final['Air TÂ°'] + df_final['TÂ°(CÂ°)']) / 2\n",
    "    df_final['TÂ°(CÂ°)'] = df_final['TÂ°(CÂ°)'].round(2)\n",
    "    df_final.drop(columns=['Air TÂ°'], inplace=True)\n",
    "    \n",
    "    df_final['Sea Level Pressure (hPa)'] = round((df_final['Sea Level Pressure (hPa)'] + df_final['Surface Pressure']) / 2, 2)\n",
    "    df_final.drop(columns=['Surface Pressure'], inplace=True)\n",
    "except Exception as e:\n",
    "    print(f\"Erreur :\\n {e}\")\n",
    "    \n",
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Categorical Time columns and delete the 'm' in Water Depth Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # CrÃ©er une colonne temporaire pour accÃ©der Ã  .dt\n",
    "\n",
    "    df_final['Year'] = df_final['Datetime'].dt.year\n",
    "    df_final['Month'] = df_final['Datetime'].dt.month_name()\n",
    "    df_final['DayOfWeek'] = df_final['Datetime'].dt.day_name()\n",
    "    df_final['DayPeriod'] = df_final['Datetime'].apply(\n",
    "        lambda x: 'Morning' if 6 <= x.hour < 12 else\n",
    "                  'Afternoon' if 12 <= x.hour < 18 else\n",
    "                  'Evening' if 18 <= x.hour < 22 else\n",
    "                  'Night'\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erreur :\\n {e}\")\n",
    "\n",
    "try:\n",
    "    # virer le m dans Water Depth avec regex lambda et passer la colonne en float\n",
    "    df_final['Water Depth (m)'] = df_final['Water Depth (m)'].apply(lambda x: re.sub(r'\\D', '', str(x)).strip())\n",
    "    df_final['Water Depth (m)'] = df_final['Water Depth (m)'].astype(float)\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "\n",
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check truth about Wind Speed Using another API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_42058 = df_final[df_final['Station ID'] == \"42058\"]\n",
    "df_42058.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RequÃªte Ã  l'API Visual Crossing pour les donnÃ©es de vÃ©rification (1 / 24h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Chargement de la clÃ© API ----\n",
    "vc_api_key_path = r\"c:\\Credentials\\visual_crossing_weather_api.json\"\n",
    "with open(vc_api_key_path, 'r') as file:\n",
    "    content = json.load(file)\n",
    "    vc_api_key = content[\"api_key\"]\n",
    "\n",
    "# ---- Extraire les coordonnÃ©es depuis la premiÃ¨re ligne du DataFrame ----\n",
    "lat_42058, lon_42058 = None, None\n",
    "\n",
    "if not df_42058.empty:\n",
    "    first_row = df_42058.iloc[0]\n",
    "    lat_42058, lon_42058 = first_row[\"Lat\"], first_row[\"Lon\"]\n",
    "\n",
    "# ---- DÃ©finir les dates pour la requÃªte ----\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "last_month = (datetime.now() - timedelta(days=31)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# ---- CrÃ©er le dossier de cache si nÃ©cessaire ----\n",
    "cache_dir = \"api_call_files\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# ---- DÃ©finir le fichier cache selon la position ----\n",
    "cache_file = os.path.join(cache_dir, f\"vc_meteo_{lat_42058}_{lon_42058}.csv\")\n",
    "\n",
    "# ---- VÃ©rifier si un cache rÃ©cent existe (moins de 24h) ----\n",
    "use_cache = False\n",
    "if os.path.exists(cache_file):\n",
    "    last_modified = datetime.fromtimestamp(os.path.getmtime(cache_file))\n",
    "    if datetime.now() - last_modified < timedelta(hours=24):\n",
    "        print(f\"ðŸ“¦ Cache dÃ©tectÃ© ({cache_file}), modifiÃ© le {last_modified.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        vc_meteo_df = pd.read_csv(cache_file)\n",
    "        print(\"âœ… DonnÃ©es mÃ©tÃ©o rechargÃ©es depuis le cache.\")\n",
    "        use_cache = True\n",
    "    else:\n",
    "        print(f\"âš ï¸ Cache trouvÃ© mais pÃ©rimÃ© (plus de 24h) â†’ nouvelle requÃªte API.\")\n",
    "\n",
    "# ---- Appel API si pas de cache valide ----\n",
    "if not use_cache and lat_42058 is not None and lon_42058 is not None:\n",
    "    url = f\"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/{lat_42058},{lon_42058}/{last_month}/{today}?unitGroup=metric&key={vc_api_key}&contentType=json\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            vc_meteo_data = response.json()\n",
    "            print(\"ðŸŒ DonnÃ©es mÃ©tÃ©o rÃ©cupÃ©rÃ©es depuis l'API Visual Crossing.\")\n",
    "\n",
    "            # ---- Extraire les donnÃ©es journaliÃ¨res et sauvegarder ----\n",
    "            if \"days\" in vc_meteo_data:\n",
    "                vc_meteo_df = pd.json_normalize(vc_meteo_data[\"days\"])\n",
    "                vc_meteo_df.to_csv(cache_file, index=False)\n",
    "                print(f\"ðŸ’¾ DonnÃ©es sauvegardÃ©es dans le cache : {cache_file}\")\n",
    "            else:\n",
    "                print(\"âš ï¸ Le champ 'days' est absent de la rÃ©ponse API.\")\n",
    "        else:\n",
    "            print(f\"âŒ Ã‰chec de lâ€™appel API â€” code de statut : {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Exception levÃ©e lors de la requÃªte API : {e}\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# ðŸ“ Charger les donnÃ©es du CSV Visual Crossing\n",
    "vc_csv_path = f\"api_call_files/vc_meteo_{lat_42058}_{lon_42058}.csv\"\n",
    "df_vc_meteo = pd.read_csv(vc_csv_path)\n",
    "try:\n",
    "    # mettre Datetime en index\n",
    "    df_vc_meteo.rename(columns={\"datetime\": \"Datetime\"}, inplace=True)  \n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erreur :\\n {e}\")\n",
    "\n",
    "df_vc_meteo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage du DataFrame retournÃ©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§¼ Nettoyage et transformation\n",
    "all_hours = []\n",
    "\n",
    "for i, row in df_vc_meteo.iterrows():\n",
    "    try:\n",
    "        hours_list = ast.literal_eval(row['hours'])\n",
    "\n",
    "        for hour_data in hours_list:\n",
    "            hour_data['Date'] = i  # âœ… On met l'index courant, i.e. la date du jour\n",
    "            all_hours.append(hour_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur parsing ligne {i}: {e}\")\n",
    "\n",
    "df_vc_flat = pd.DataFrame(all_hours)\n",
    "\n",
    "# ðŸ•’ Convertir le timestamp en datetime string\n",
    "df_vc_flat[\"Datetime\"] = pd.to_datetime(df_vc_flat[\"datetimeEpoch\"], unit=\"s\").dt.strftime(\"%Y-%m-%d-%H\")\n",
    "\n",
    "# ðŸ—“ï¸ Filtrer sur les 30 derniers jours\n",
    "today = datetime.now()\n",
    "thirty_days_ago = today - timedelta(days=30)\n",
    "\n",
    "df_vc_flat['Date'] = pd.to_datetime(df_vc_flat['Date'])  # ðŸ‘ˆ Assurer que c'est bien du datetime\n",
    "df_vc_last_month = df_vc_flat[\n",
    "    (df_vc_flat['Date'] >= thirty_days_ago) & \n",
    "    (df_vc_flat['Date'] <= today)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renommage des colonnes pour faciliter la comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtrer les colonnes nÃ©cessaires\n",
    "try:\n",
    "    df_vc_last_month = df_vc_last_month[[\"Datetime\", \"temp\", \"humidity\", \"precip\", \"dew\", \"windgust\", \n",
    "                                     \"windspeed\", \"winddir\", \"pressure\", \"visibility\"]]\n",
    "except Exception as e:\n",
    "        print(f\"Erreur lors du filtrage des colonnes:\\n {e}\\n\")\n",
    "\n",
    "try:\n",
    "        df_vc_last_month[\"Datetime\"] = pd.to_datetime(df_vc_last_month[\"Datetime\"], errors='coerce')\n",
    "except Exception as e:\n",
    "        print(f\"Erreur lors du reformatage de la colonne Datetime:\\n {e}\\n\")        \n",
    "\n",
    "for col in df_vc_last_month.columns:\n",
    "        try:\n",
    "            # \n",
    "            if not \"Datetime\" in col:\n",
    "                if not col.startswith(\"VC_\"):\n",
    "                        rename_columns(df_vc_last_month, {col: f\"VC_{col}\"})\n",
    "        except Exception as e:\n",
    "                print(f\"Erreur lors du renommage de la colonne {col}:\\n {e}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enforce Datetime Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the Datetime column is correctly converted\n",
    "print(df_42058[\"Datetime\"].dtype)\n",
    "print(df_vc_last_month[\"Datetime\"].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge and Compare DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    \n",
    "    df_compare = pd.merge(df_42058, df_vc_last_month, on=\"Datetime\", how=\"inner\")\n",
    "    display(df_compare.columns)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparaison Wind Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Wind Speed Comparison\n",
    "    df_windspeed_compare = df_compare[['Wind Speed (km/h)', 'Wind Speed (10m)','VC_windspeed']]\n",
    "    #  Pressure Comparison\n",
    "    df_pressure_compare = df_compare[['Sea Level Pressure (hPa)','Pressure (hPa)', 'VC_pressure']]\n",
    "    # Dew Point Comparison\n",
    "    df_dew_compare = df_compare[['dewpoint','Dew Point (Â°C)', 'VC_dew']]\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "df_windspeed_compare.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_col_to_delete = ['Wind Speed (km/h)', 'Anemometer Height (m)']\n",
    "df_final = drop_columns_if_exist(df_final, wind_col_to_delete)\n",
    "# Arrondir les valeurs de df_final\n",
    "\n",
    "df_final = df_final.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparaison Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pressure_compare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = drop_columns_if_exist(df_final, ['Pressure (hPa)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dew_compare.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Custom GPT Wisdom of crowd pour le DewPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    # Calcul des distances absolues entre chaque paire de mesures pour chaque ligne\n",
    "    # Ces distances nous permettent de savoir quelle mesure est la plus proche des autres\n",
    "    df_compare['dist_dewpoint_DewPoint'] = np.abs(df_compare['dewpoint'] - df_compare['Dew Point (Â°C)'])\n",
    "    df_compare['dist_dewpoint_VC'] = np.abs(df_compare['dewpoint'] - df_compare['VC_dew'])\n",
    "    df_compare['dist_DewPoint_VC'] = np.abs(df_compare['Dew Point (Â°C)'] - df_compare['VC_dew'])\n",
    "\n",
    "    # Pour chaque ligne, on dÃ©termine quelle mesure est la plus proche des deux autres :\n",
    "    # - Si 'dewpoint' est plus proche des autres mesures que 'Dew Point (Â°C)' et 'VC_dew', alors 'dewpoint' est marquÃ© comme plus proche.\n",
    "    df_compare['dewpoint_closer'] = (df_compare['dist_dewpoint_VC'] < df_compare['dist_dewpoint_DewPoint']) & (df_compare['dist_dewpoint_VC'] < df_compare['dist_DewPoint_VC'])\n",
    "\n",
    "    # - Si 'Dew Point (Â°C)' est plus proche des autres mesures que 'dewpoint' et 'VC_dew', alors 'Dew Point (Â°C)' est marquÃ© comme plus proche.\n",
    "    df_compare['DewPoint_closer'] = (df_compare['dist_dewpoint_DewPoint'] < df_compare['dist_dewpoint_VC']) & (df_compare['dist_dewpoint_DewPoint'] < df_compare['dist_DewPoint_VC'])\n",
    "\n",
    "    # - Si 'VC_dew' est plus proche des autres mesures que 'dewpoint' et 'Dew Point (Â°C)', alors 'VC_dew' est marquÃ© comme plus proche.\n",
    "    df_compare['VC_closer'] = (df_compare['dist_DewPoint_VC'] < df_compare['dist_dewpoint_VC']) & (df_compare['dist_DewPoint_VC'] < df_compare['dist_dewpoint_DewPoint'])\n",
    "\n",
    "    # Calculer les probabilitÃ©s que chaque mesure soit la plus proche des autres sur l'ensemble des lignes :\n",
    "    # La probabilitÃ© est simplement la proportion de fois oÃ¹ une mesure a Ã©tÃ© plus proche des autres.\n",
    "    prob_dewpoint_closer = df_compare['dewpoint_closer'].mean().round(3)\n",
    "    prob_dewpoint_c_closer = df_compare['DewPoint_closer'].mean().round(3)\n",
    "    prob_vc_closer = df_compare['VC_closer'].mean().round(3)\n",
    "\n",
    "    # Afficher les rÃ©sultats\n",
    "    # Ces rÃ©sultats indiquent la probabilitÃ© que chaque mesure soit la plus proche des autres sur toutes les lignes de donnÃ©es\n",
    "    print(f\"Probability that 'dewpoint' is closer to the truth: {prob_dewpoint_closer}\")\n",
    "    print(f\"Probability that 'Dew Point (Â°C)' is closer to the truth: {prob_dewpoint_c_closer}\")\n",
    "    print(f\"Probability that 'VC_dew' is closer to the truth: {prob_vc_closer}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_final['Dew Point (Â°C)'] = df_final['dewpoint']\n",
    "    df_final = drop_columns_if_exist(df_final, ['dewpoint'])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename Final Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_final.rename(columns={'Air TÂ° Height': 'Air TÂ° Height (m)',\n",
    "                             'Barometer Elevation': 'Barometer Elevation (m)',\n",
    "                             'Water TÂ°': 'Water TÂ° (Â°C)'}, inplace=True)\n",
    "\n",
    "    print(\"Column 'Air TÂ° Height' renamed to 'Air TÂ° Height (m)'\")\n",
    "\n",
    "    print(\"Column 'Barometer Elevation' renamed to 'Barometer Elevation (m)'\")\n",
    "    print(\"Column 'Water TÂ°' renamed to 'Water TÂ° (Â°C)'\")\n",
    "    df_final['TÂ°(CÂ°)'] = df_final['TÂ°(CÂ°)'].round(2)\n",
    "\n",
    "    print(\"Column 'TÂ°(CÂ°)' rounded to 2 decimal places\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction des types des colonnes\n",
    "try:\n",
    "    df_final['Lat'] = df_final['Lat'].astype(str)\n",
    "    df_final['Lon'] = df_final['Lon'].astype(str)\n",
    "    df_final['Year'] = df_final['Year'].astype(str)\n",
    "    print(df_final.dtypes)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_silver = \"silver_table\"\n",
    "table_dim_station = \"dim_station\"\n",
    "table_dim_time = \"dim_time\"\n",
    "table_facts_meteo = \"facts_meteo\"\n",
    "table_facts_ocean = \"facts_ocean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[\"Date ID\"] = df_final[\"Datetime\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Supprimer la colonne 'Datetime'\n",
    "    \n",
    "    create_table_in_mysql(df=df_final, engine=engine, table_name=table_silver)\n",
    "except Exception as e:\n",
    "    print(f\"Error when creating table:\\n {str(e)}\")\n",
    "try:\n",
    "    insert_new_rows(df=df_final, engine=engine, table_name=table_silver, ref= 'Date ID')\n",
    "except Exception as e:\n",
    "    print(f\"Error when inserting new rows:\\n {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrÃ©ation des DataFrames pour les tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colonnes dÃ©rivÃ©es\n",
    "df_final['Date ID'] = df_final['Datetime'].dt.strftime('%Y%m%d%H')\n",
    "df_final['Unique ID'] = df_final['Datetime'].dt.strftime('%Y%m%d%H%M') + df_final['Station ID']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_concat_csv(df_final, csv_folder=\"csv\", base_filename=\"Cleaned_Data_Ocean_Meteo_ETL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprimer toutes les colonnes numeriquesq\n",
    "num_cols = df_final.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "#imprimer toutes les colonnes non-numeriques\n",
    "non_num_cols = df_final.select_dtypes(exclude=[np.number]).columns\n",
    "non_num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_row_values(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### DimStation ##################################################################################################\n",
    "\n",
    "df_station = df_final[[\n",
    "    'Station ID', 'Station Zone', 'Lat', 'Lon'\n",
    "]].copy().drop_duplicates()\n",
    "\n",
    "###################################### DimTime ##################################################################################################\n",
    "\n",
    "df_time = df_final[['Datetime', 'Year', 'Month', 'DayOfWeek', 'DayPeriod']].copy().drop_duplicates()\n",
    "\n",
    "########################################## Facts Meteo #########################################################################\n",
    "\n",
    "df_facts_meteo = df_final[[\n",
    "    'Unique ID', # PK\n",
    "    'TÂ°(CÂ°)', 'Relative Humidity (%)', 'Dew Point (Â°C)', 'Precipitations (mm)',\n",
    "    'Sea Level Pressure (hPa)', 'Low Clouds (%)', 'Middle Clouds (%)', 'High Clouds (%)',\n",
    "    \"Cloud Cover (%)\", 'Visibility (km)', 'Wind Speed (10m)', 'Wind Direction (Â°)',\n",
    "    'Wind Gusts (km/h)', 'Barometer Elevation (m)', 'Air TÂ° Height (m)',\n",
    "    \n",
    "    'Station ID', # FK\n",
    "    'Datetime' # FK\n",
    "]].copy().drop_duplicates()\n",
    "\n",
    "########################################## Facts Ocean #########################################################################\n",
    "\n",
    "df_facts_ocean = df_final[[\n",
    "    'Unique ID', # PK\n",
    "    'Wave Height (m)', 'Average Wave Period (s)', 'Dominant Wave Direction (Â°)',\n",
    "    'Water TÂ° (Â°C)', 'Water Depth (m)', 'Sea Temperature Depth (m)',\n",
    "    \n",
    "    'Station ID', # FK\n",
    "    'Datetime'  # FK\n",
    "]].copy().drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_row_values(df_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_row_values(df_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_row_values(df_facts_meteo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_row_values(df_facts_ocean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VÃ©rifier les doublons\n",
    "print(f\"Doublons dans df_station : {df_station.duplicated().sum()}\")\n",
    "print(f\"Doublons dans df_time : {df_time.duplicated().sum()}\")\n",
    "print(f\"Doublons dans df_facts_meteo : {df_facts_meteo.duplicated().sum()}\")\n",
    "print(f\"Doublons dans df_facts_ocean : {df_facts_ocean.duplicated().sum()}\")\n",
    "\n",
    "# VÃ©rification des valeurs de base\n",
    "print(f\"\\nDescription des donnÃ©es de df_station : \\n{df_station.describe()}\")\n",
    "print(f\"\\nDescription des donnÃ©es de df_time : \\n{df_time.describe()}\")\n",
    "print(f\"\\nDescription des donnÃ©es de df_facts_meteo : \\n{df_facts_meteo.describe()}\")\n",
    "print(f\"\\nDescription des donnÃ©es de df_facts_ocean : \\n{df_facts_ocean.describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking dim_station DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique values\n",
    "print(f\"{df_station.shape[0]}\\n\\n{df_station.nunique()}\")\n",
    "display_row_values(df_station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrÃ©ation de la table dim_station et insertion des donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    create_table_in_mysql(df=df_station, engine=engine, table_name=table_dim_station)\n",
    "    insert_new_rows(df=df_station, engine=engine, table_name=table_dim_station, ref='Station ID')\n",
    "\n",
    "    save_concat_csv(df_station, csv_folder=\"csv\", base_filename=\"Dim_Station_Data_Ocean_Meteo_ETL\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking dim_time DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{df_time.shape[0]}\\n\\n{df_time.nunique()}\")\n",
    "display_row_values(df_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrÃ©ation de la table dim_time et insertion des donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    create_table_in_mysql(df=df_time, engine=engine, table_name=table_dim_time)\n",
    "    insert_new_rows(df=df_time, engine=engine, table_name=table_dim_time, ref='Datetime')\n",
    "\n",
    "    save_concat_csv(df_station, csv_folder=\"csv\", base_filename=\"Dim_Time_Data_Ocean_Meteo_ETL\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Meteo Facts DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{df_facts_meteo.shape[0]}\\n\\n{df_facts_meteo.nunique()}\")\n",
    "display_row_values(df_facts_meteo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrÃ©ation de la table facts_meteo et insertion des donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    create_table_in_mysql(df=df_facts_meteo, engine=engine, table_name=table_facts_meteo)\n",
    "    insert_new_rows(df=df_facts_meteo, engine=engine, table_name=table_facts_meteo, ref='Datetime')\n",
    "\n",
    "    save_concat_csv(df_station, csv_folder=\"csv\", base_filename=\"Facts_Meteo_Data_Ocean_Meteo_ETL\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Ocean Facts DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{df_facts_ocean.shape[0]}\\n\\n{df_facts_ocean.nunique()}\")\n",
    "display_row_values(df_facts_ocean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrÃ©ation de la table facts_ocean et insertion des donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    create_table_in_mysql(df=df_facts_ocean, engine=engine, table_name=table_facts_ocean)\n",
    "    insert_new_rows(df=df_facts_ocean, engine=engine, table_name=table_facts_ocean, ref='Datetime')\n",
    "\n",
    "    save_concat_csv(df_station, csv_folder=\"csv\", base_filename=\"Facts_Ocean_Data_Ocean_Meteo_ETL\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     # Initialiser le metadata\n",
    "#     metadata = MetaData()\n",
    "#     metadata.reflect(bind=engine)\n",
    "\n",
    "#     # RÃ©cupÃ©rer les tables existantes\n",
    "#     dim_station = metadata.tables['dim_station']\n",
    "#     dim_time = metadata.tables['dim_time']\n",
    "#     facts_meteo = metadata.tables['facts_meteo']\n",
    "#     facts_ocean = metadata.tables['facts_ocean']\n",
    "\n",
    "#     # Ajouter les clÃ©s Ã©trangÃ¨res aux tables de faits\n",
    "#     ForeignKeyConstraint(['Station ID'], [dim_station.c['Station ID']], name='fk_meteo_station').create(facts_meteo, bind=engine)\n",
    "#     ForeignKeyConstraint(['Datetime'], [dim_time.c['Datetime']], name='fk_meteo_time').create(facts_meteo, bind=engine)\n",
    "\n",
    "#     ForeignKeyConstraint(['Station ID'], [dim_station.c['Station ID']], name='fk_ocean_station').create(facts_ocean, bind=engine)\n",
    "#     ForeignKeyConstraint(['Datetime'], [dim_time.c['Datetime']], name='fk_ocean_time').create(facts_ocean, bind=engine)\n",
    "# except Exception as e:\n",
    "#     print(str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
