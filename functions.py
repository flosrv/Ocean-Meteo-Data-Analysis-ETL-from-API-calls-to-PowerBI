from imports import *

Base = declarative_base()


def convert_df_columns(df):
    """
    Convertit chaque colonne en son type appropri√© sans modifier les donn√©es
    ou introduire des NaN.
    
    Args:
    - df: pd.DataFrame. Le DataFrame √† traiter.
    
    Returns:
    - pd.DataFrame: Le DataFrame avec les types de donn√©es convertis.
    """
    
    # Traitement des colonnes avec les types appropri√©s
    for col in df.columns:
        # Convertir les colonnes num√©riques
        if df[col].dtype == 'object':
            # Tenter de convertir en float si c'est un nombre repr√©sent√© par des strings
            try:
                # Convertir en float pour les colonnes qui peuvent l'√™tre (ex: "Wind Speed (km/h)", "Pressure (hPa)", etc.)
                df[col] = pd.to_numeric(df[col], errors='raise')
            except ValueError:
                # Si la conversion √©choue, laisser la colonne intacte
                pass
                
        # Convertir des dates si la colonne contient des cha√Ænes de caract√®res repr√©sentant des dates
        if df[col].dtype == 'object' and 'date' in col.lower():
            try:
                df[col] = pd.to_datetime(df[col], errors='raise')
            except ValueError:
                pass
        
        # Convertir les bool√©ens (is_day) en int
        if col == "is_day":
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)

    # Assurer les types num√©riques pour les colonnes d√©j√† num√©riques mais mal typ√©es
    for col in df.select_dtypes(include=['float64', 'int64']).columns:
        df[col] = df[col].astype(pd.Float64Dtype())  # Garantir une gestion correcte des NaN dans les colonnes num√©riques

    return df

def get_buoy_url(station_id):
            station_id_str = str(station_id)
            url = f"https://www.ndbc.noaa.gov/station_page.php?station={station_id_str}"
            return url

def fetch_table_data(engine, table_name: str) -> pd.DataFrame:
    """
    R√©cup√®re les donn√©es d'une table MySQL dans une DataFrame Pandas.
    
    Params:
        engine: SQLAlchemy engine pour la connexion MySQL.
        table_name (str): Nom de la table √† r√©cup√©rer.
    
    Returns:
        pd.DataFrame: DataFrame contenant les donn√©es de la table.
    """
    try:
        query = text(f"SELECT * FROM `{table_name}`;")
        with engine.connect() as connection:
            df = pd.read_sql(query, connection)

        print(f"‚úÖ Donn√©es r√©cup√©r√©es depuis '{table_name}' ({len(df)} lignes).")
        return df

    except Exception as e:
        print(f"‚ùå Erreur lors de la r√©cup√©ration de '{table_name}' : {e}")
        return pd.DataFrame()  # Retourne une DataFrame vide en cas d'erreur

def create_table_in_mysql(df: pd.DataFrame, table_name: str, engine):
    # V√©rifier si la table existe d√©j√†
    check_table_sql = f"SHOW TABLES LIKE '{table_name}';"

    try:
        with engine.connect() as connection:
            result = connection.execute(text(check_table_sql))
            if result.fetchone():
                print(f"‚ö†Ô∏è La table '{table_name}' existe d√©j√† dans MySQL.")
                return  # Ne pas recr√©er la table si elle existe d√©j√†

            # D√©finition des colonnes
            column_definitions = []
            for column_name, dtype in df.dtypes.items():
                if dtype == 'object': 
                    column_type = "VARCHAR(255)"
                elif dtype == 'int64':  
                    column_type = "INT"
                elif dtype == 'float64': 
                    column_type = "FLOAT"
                elif dtype == 'datetime64[ns]': 
                    column_type = "DATETIME"
                elif dtype == 'datetime64[ns, UTC]': 
                    column_type = "DATETIME"
                elif dtype == 'timedelta64[ns]':  
                    column_type = "TIME"
                elif dtype == 'bool':  
                    column_type = "BOOLEAN"
                elif dtype == 'time64[ns]': 
                    column_type = "TIME"
                else:
                    column_type = "VARCHAR(255)" 

                column_definitions.append(f"`{column_name}` {column_type}")

            # Cr√©ation de la table
            create_table_sql = f"CREATE TABLE `{table_name}` ({', '.join(column_definitions)});"
            connection.execute(text(create_table_sql))
            print(f"‚úÖ Table '{table_name}' cr√©√©e avec succ√®s dans MySQL.")

    except Exception as e:
        print(f"‚ùå Erreur lors de la cr√©ation de la table : {e}")

def insert_new_rows(engine, df: pd.DataFrame, table_name: str):
    """
    Ins√®re uniquement les nouvelles lignes de df dans MySQL en comparant la colonne 'Datetime'.
    """
    try:
        # V√©rifier si la table existe et r√©cup√©rer la valeur max de Datetime
        query = f"SELECT MAX(Datetime) FROM `{table_name}`;"
        with engine.connect() as connection:
            result = connection.execute(text(query)).scalar()  # R√©cup√©rer la valeur max
        
        # Si la table est vide, ins√©rer tout le DataFrame
        if result is None:
            print(f"üíæ Aucune donn√©e dans '{table_name}', insertion de toutes les lignes.")
            df.to_sql(name=table_name, con=engine, if_exists='append', index=False)
            print(f"‚úÖ {len(df)} nouvelles lignes ins√©r√©es dans '{table_name}'.")
            return
        
        # Filtrer les nouvelles lignes (celles avec un Datetime plus grand que la valeur max en base)
        df["Datetime"] = pd.to_datetime(df["Datetime"])  # S'assurer que la colonne est bien en datetime
        new_rows = df[df["Datetime"] > result]

        if new_rows.empty:
            print(f"‚úÖ Aucune nouvelle ligne √† ins√©rer dans '{table_name}'.")
            return

        # Ins√©rer uniquement les nouvelles lignes
        new_rows.to_sql(name=table_name, con=engine, if_exists='append', index=False)
        print(f"‚úÖ {len(new_rows)} nouvelles lignes ins√©r√©es dans '{table_name}'.")

    except Exception as e:
        print(f"‚ùå Erreur lors de l'insertion des nouvelles lignes : {e}")

def show_null_counts(df):
    row_count = df.shape[0]
    null_counts = df.isnull().sum()
    
    formatted_output = "\n".join(
        f"{col:<40}{count:<4}/ {row_count}" for col, count in null_counts.items()
    )
    
    print(formatted_output)

def drop_columns_if_exist(df, columns_to_drop):
    print(f" Nombre initial de colonnes: {len(df.columns)}")  # Afficher les colonnes existantes
    existing_columns = []
    for col in columns_to_drop:
        if col in df.columns:
            existing_columns.append(col)
            print(f"Colonne '{col}' Supprim√©e")
        else: 
            print(f"Colonne '{col}' Non Trouv√©e")
    
    # Si des colonnes √† supprimer existent, les supprimer
    if existing_columns:
        df = df.drop(columns=existing_columns)
    
    print(f" Nombre final de colonnes: {len(df.columns)}")
    return df

def convert_to_datetime(date_value):
    try:
        # Si l'entr√©e est d√©j√† un objet datetime, on le retourne directement
        if isinstance(date_value, datetime):
            return date_value
        
        # Si l'entr√©e est un objet pandas.Timestamp, on le convertit en datetime
        if isinstance(date_value, pd.Timestamp):
            return date_value.to_pydatetime()
        
        # Si l'entr√©e est une cha√Æne de caract√®res, on tente de la convertir en datetime
        if isinstance(date_value, str):
            return datetime.fromisoformat(date_value)
        
    except ValueError as e:
        # Si la conversion √©choue, on retourne la valeur d'origine (sans la modifier)
        return date_value  # Retourne la valeur d'origine sans la modifier

def handle_null_values(df):
    # Calcul du nombre de lignes du DataFrame
    num_rows = len(df)

    # Attribution du tag et seuil en fonction du nombre de lignes
    if num_rows > 100000:
        tag = 'green'  # Vert pour les DataFrames de plus de 100 000 lignes
        threshold = 70  # Plus souple pour les DataFrames verts (jusqu'√† 70% de donn√©es manquantes)
    elif 10000 < num_rows <= 100000:
        tag = 'yellow'  # Jaune pour les DataFrames entre 10 000 et 100 000 lignes
        threshold = 60  # Seuil interm√©diaire pour les DataFrames jaunes (jusqu'√† 60% de donn√©es manquantes)
    elif 2000 < num_rows <= 10000:
        tag = 'orange'  # Orange pour les DataFrames entre 2 000 et 10 000 lignes
        threshold = 55  # Seuil mod√©r√© pour les DataFrames entre 2K et 10K (jusqu'√† 55% de donn√©es manquantes)
    else:
        tag = 'red'  # Rouge pour les DataFrames de moins de 2 000 lignes
        threshold = 50  # Plus strict pour les DataFrames rouges (jusqu'√† 50% de donn√©es manquantes)

    print(f"\nTag: {tag} - Nombre de lignes: {num_rows}")

    # Calcul du pourcentage de valeurs manquantes par colonne
    missing_percent = (df.isnull().sum() / len(df)) * 100

    # Gestion des valeurs manquantes en fonction du tag
    for column in df.columns:
        null_percentage = missing_percent[column]  # Calcul du pourcentage de valeurs manquantes

        if null_percentage == 0:
            print(f"Colonne '{column}' non modifi√©e (0% de valeurs manquantes)")
            continue  # Skip this column if there are no missing values

        if null_percentage > threshold:  # Si + de X% de valeurs manquantes (d√©pend du tag)
            df = df.drop(columns=[column]) 
            print(f"Colonne '{column}' Supprim√©e (plus de {threshold}% de valeurs manquantes)")

        else:  # Si moins de X% de valeurs manquantes
            if pd.api.types.is_numeric_dtype(df[column]):
                median_value = df[column].median()
                df[column] = df[column].fillna(median_value)  # Imputation par la m√©diane pour les colonnes num√©riques
                print(f"Colonne '{column}' Imput√©e par la m√©diane ({round(null_percentage,2)}% de valeurs manquantes)")

            else:
                mode_value = df[column].mode()[0]  # Imputation par le mode pour les colonnes non num√©riques
                df[column] = df[column].fillna(mode_value)
                print(f"Colonne '{column}' Imput√©e par le mode ({round(null_percentage,2)}% de valeurs manquantes)")

    return df

def meteo_api_request(coordinates, mode='historical', days=92, interval='hourly'):
    """
    Fonction pour r√©cup√©rer les donn√©es m√©t√©o depuis l'API Open-Meteo avec cache et r√©essayer en cas d'erreur.
    
    Param√®tres:
        coordinates (list) : liste de coordonn√©es [latitude, longitude]
        mode (str) : intervalle de donn√©es ('forecast' ou 'historical', par d√©faut 'historical')
        days (int) : nombre de jours dans le pass√© ou dans le futur (par d√©faut : 92 pour historique)
        interval (str) : intervalle des donn√©es ('hourly' ou 'daily', par d√©faut 'hourly')

    Retourne :
        pd.DataFrame : un DataFrame avec les donn√©es m√©t√©o
    """
    # Fonction utilitaire pour convertir les coordonn√©es avec ou sans suffixe (ex: '45.5W', '-45.5')
    def parse_coordinates(coord):
        # V√©rifie si la coordonn√©e a un suffixe de direction (W, E, N, S)
        pattern = r"^([-+]?\d+(\.\d+)?)([NSEW]?)$"
        match = re.match(pattern, str(coord))
        if match:
            value = float(match.group(1))
            direction = match.group(3)
            
            if direction == 'W' or direction == 'S':
                value = -abs(value)  # Si direction est Ouest ou Sud, on inverse la valeur
            
            return value
        else:
            raise ValueError(f"Coordonn√©e invalide : {coord}")

    # Convertir les coordonn√©es
    latitude = parse_coordinates(coordinates[0])
    longitude = parse_coordinates(coordinates[1])

    # Setup de l'API client avec retry et cache
    cache_session = requests_cache.CachedSession('.cache', expire_after=3600)
    openmeteo = openmeteo_requests.Client(session=cache_session)

    url = "https://api.open-meteo.com/v1/forecast"
    
    # Param√®tres de base
    params = {
        "latitude": latitude,
        "longitude": longitude,
        "past_days": days if mode == 'historical' else None,  # Si historique, utiliser 'past_days'
        "forecast_days": days if mode == 'forecast' else None,  # Si forecast, utiliser 'forecast_days'
        "hourly" if interval.lower() == 'hourly' else "daily": [
            "temperature_2m", "relative_humidity_2m", "dew_point_2m", "precipitation", "rain", "showers", 
            "pressure_msl", "surface_pressure", "cloud_cover", "cloud_cover_low", "cloud_cover_mid", 
            "cloud_cover_high", "visibility", "wind_speed_10m", "soil_temperature_0cm", "soil_moisture_0_to_1cm", 
            "is_day"
        ]
    }

    # Faire l'appel API
    responses = openmeteo.weather_api(url, params=params)

    # Traiter la r√©ponse pour le premier emplacement
    response = responses[0]  # On prend la premi√®re r√©ponse si plusieurs lieux sont fournis

    # Initialisation du dictionnaire pour les donn√©es √† retourner
    data = {}

    # Processus des donn√©es en fonction du mode s√©lectionn√©
    if mode == 'historical':
        # Traitement pour donn√©es historiques
        if interval == 'hourly':
            hourly = response.Hourly()
            hourly_data = {
                "date": pd.date_range(
                    start=pd.to_datetime(hourly.Time(), unit="s", utc=True),
                    end=pd.to_datetime(hourly.TimeEnd(), unit="s", utc=True),
                    freq=pd.Timedelta(seconds=hourly.Interval()),
                    inclusive="left"
                )
            }
            hourly_variables = [
                "temperature_2m", "relative_humidity_2m", "dew_point_2m", "precipitation", "rain", "showers", 
                "pressure_msl", "surface_pressure", "cloud_cover", "cloud_cover_low", "cloud_cover_mid", 
                "cloud_cover_high", "visibility", "wind_speed_10m", "soil_temperature_0cm", "soil_moisture_0_to_1cm", 
                "is_day"
            ]
            
            for i, var in enumerate(hourly_variables):
                hourly_data[var] = [round(value, 2) for value in hourly.Variables(i).ValuesAsNumpy()]

            return pd.DataFrame(hourly_data)

        elif interval == 'daily':
            daily = response.Daily()
            daily_data = {
                "date": pd.date_range(
                    start=pd.to_datetime(daily.Time(), unit="s", utc=True),
                    end=pd.to_datetime(daily.TimeEnd(), unit="s", utc=True),
                    freq=pd.Timedelta(seconds=daily.Interval()),
                    inclusive="left"
                )
            }
            daily_variables = [
                "temperature_2m_max", "temperature_2m_min", "apparent_temperature_max", "apparent_temperature_min", 
                "sunrise", "sunset", "daylight_duration", "sunshine_duration", "uv_index_max", "uv_index_clear_sky_max", 
                "precipitation_sum", "rain_sum", "showers_sum", "precipitation_hours", "precipitation_probability_max", 
                "wind_speed_10m_max", "wind_gusts_10m_max", "wind_direction_10m_dominant", "shortwave_radiation_sum"
            ]
            
            for i, var in enumerate(daily_variables):
                daily_data[var] = [round(value, 2) for value in daily.Variables(i).ValuesAsNumpy()]

            return pd.DataFrame(daily_data)

    # If Forecast is chosen
    elif mode == 'forecast':
        # Traitement pour pr√©visions
        if interval == 'hourly':
            hourly = response.Hourly()
            hourly_data = {
                "date": pd.date_range(
                    start=pd.to_datetime(hourly.Time(), unit="s", utc=True),
                    end=pd.to_datetime(hourly.TimeEnd(), unit="s", utc=True),
                    freq=pd.Timedelta(seconds=hourly.Interval()),
                    inclusive="left"
                )
            }
            hourly_variables = [
                "temperature_2m", "relative_humidity_2m", "dew_point_2m", "precipitation", "rain", "showers", 
                "pressure_msl", "surface_pressure", "cloud_cover", "cloud_cover_low", "cloud_cover_mid", 
                "cloud_cover_high", "visibility", "wind_speed_10m", "soil_temperature_0cm", "soil_moisture_0_to_1cm", 
                "is_day"
            ]
            
            for i, var in enumerate(hourly_variables):
                hourly_data[var] = [round(value, 2) for value in hourly.Variables(i).ValuesAsNumpy()]

            return pd.DataFrame(hourly_data)

        elif interval == 'daily':
            daily = response.Daily()
            daily_data = {
                "date": pd.date_range(
                    start=pd.to_datetime(daily.Time(), unit="s", utc=True),
                    end=pd.to_datetime(daily.TimeEnd(), unit="s", utc=True),
                    freq=pd.Timedelta(seconds=daily.Interval()),
                    inclusive="left"
                )
            }
            daily_variables = [
                "temperature_2m_max", "temperature_2m_min", "apparent_temperature_max", "apparent_temperature_min", 
                "sunrise", "sunset", "daylight_duration", "sunshine_duration", "uv_index_max", "uv_index_clear_sky_max", 
                "precipitation_sum", "rain_sum", "showers_sum", "precipitation_hours", "precipitation_probability_max", 
                "wind_speed_10m_max", "wind_gusts_10m_max", "wind_direction_10m_dominant", "shortwave_radiation_sum"
            ]
            
            for i, var in enumerate(daily_variables):
                daily_data[var] = [round(value, 2) for value in daily.Variables(i).ValuesAsNumpy()]

            return pd.DataFrame(daily_data)

def get_station_metadata(station_id):
    return api.station(station_id=station_id)

def parse_buoy_json(buoy_metadata):
    print("\nüîç D√©but du parsing de la bou√©e...")

    # V√©rifier la pr√©sence des cl√©s requises
    if not isinstance(buoy_metadata, dict):
        raise ValueError("‚ùå Les donn√©es fournies ne sont pas un dictionnaire valide.")

    if 'Name' not in buoy_metadata or 'Location' not in buoy_metadata:
        raise ValueError("‚ùå Les cl√©s 'Name' et 'Location' doivent √™tre pr√©sentes dans les donn√©es.")

    Name = buoy_metadata['Name']

    # Trouver tout ce qui vient apr√®s le premier tiret
    name_parts = Name.split(' - ', 2)
    station_zone = name_parts[1].strip().lower() if len(name_parts) > 1 else "Unknown"
    print(f"üåç Zone de la station : {station_zone}")

    # Extraction de l'ID
    name_split = Name.split()
    station_id = name_split[1] if len(name_split) > 1 else "Unknown"
    print(f"üÜî Station ID : {station_id}")

    # Extraction des coordonn√©es depuis "Location"
    location_parts = buoy_metadata["Location"].split()

    if len(location_parts) < 4:
        raise ValueError("‚ùå Format de 'Location' invalide (au moins 4 √©l√©ments attendus).")

    try:
        lat_buoy = f"{float(location_parts[0]):.2f}{location_parts[1]}"
        lon_buoy = f"{float(location_parts[2]):.2f}{location_parts[3]}"
        print(f"‚úÖ Coordonn√©es extraites : Latitude = {lat_buoy}, Longitude = {lon_buoy}")
    except ValueError:
        raise ValueError("‚ùå Impossible de convertir les coordonn√©es en nombres.")

    # Extraire les autres valeurs avec des valeurs par d√©faut si non pr√©sentes
    Water_depth = buoy_metadata.get("Water depth", "N/A")
    print(f"üåä Water Depth : {Water_depth}")
    
    sea_temp_depth = clean_numeric(buoy_metadata.get("Sea temp depth"))
    print(f"üå°Ô∏è Sea Temp Depth : {sea_temp_depth}")
    
    Barometer_elevation = clean_numeric(buoy_metadata.get("Barometer elevation"))
    print(f"üå¨Ô∏è Barometer Elevation : {Barometer_elevation}")
    
    Anemometer_height = clean_numeric(buoy_metadata.get("Anemometer height"))
    print(f"üí® Anemometer Height : {Anemometer_height}")
    
    Air_temp_height = clean_numeric(buoy_metadata.get("Air temp height"))
    print(f"üå§Ô∏è Air Temp Height : {Air_temp_height}")

    # R√©cup√©rer l'URL de la bou√©e
    url = get_buoy_url(station_id)
    print(f"üîó URL de la bou√©e : {url}")

    # Cr√©ation du dictionnaire final
    data = {
        "station_zone": station_zone,
        "lat_buoy": lat_buoy,
        "lon_buoy": lon_buoy,
        "Water_depth": Water_depth,
        "sea_temp_depth": sea_temp_depth,
        "Barometer_elevation": Barometer_elevation,
        "Anemometer_height": Anemometer_height,
        "Air_temp_height": Air_temp_height,
        "url": url
    }

    print("‚úÖ Parsing termin√© !\n")
    return data

def extract_lat_lon_from_station_list(location):

    # Expression r√©guli√®re pour capturer la latitude et la longitude
    lat_match = re.search(r'([+-]?\d+\.\d+|\d+)([NS])', location)
    lon_match = re.search(r'([+-]?\d+\.\d+|\d+)([EW])', location)
    
    if lat_match and lon_match:
        # Extraction des valeurs
        lat = float(lat_match.group(1))
        lon = float(lon_match.group(1))
        
        # Inverser la direction de la latitude et longitude si n√©cessaire
        if lat_match.group(2) == 'S':  # Si la latitude est au Sud
            lat = -lat
        if lon_match.group(2) == 'W':  # Si la longitude est √† l'Ouest
            lon = -lon
        
        lat = round(lat, 2)
        lon = round(lon, 2)
        return lat, lon
    return None, None

def print_with_flush(message):

    sys.stdout.write(f'\r{message}  ')  # \r permet de revenir au d√©but de la ligne
    sys.stdout.flush()  # Force l'affichage imm√©diat

# Extraction des valeurs avec s√©curit√©
def safe_get(dico, key):
    value = dico.get(key, "N/A")  # Valeur par d√©faut si cl√© absente
    print(f"üìä {key} : {value}")
    return value

# üîπ Nettoyer les valeurs num√©riques (supprimer tout sauf chiffres et ".")
def clean_numeric(value):
    if value is None:
        return None  # ‚ö†Ô∏è √âvite de faire `.replace()` sur None !
    return re.sub(r"[^\d.]", "", value).strip()  # Supprime tout sauf chiffres et "."

def convert_coordinates(lat, lon):
    # Conversion de la latitude
    lat_value = float(lat[:-1])  # On enl√®ve la lettre 'n' ou 's' et on garde la valeur num√©rique
    if lon[-1].lower() == 's':  # Si la latitude est dans l'h√©misph√®re sud
        lat_value = -lat_value

    # Conversion de la longitude
    lon_value = float(lon[:-1])  # On enl√®ve la lettre 'e' ou 'w' et on garde la valeur num√©rique
    if lon[-1].lower() == 'w':  # Si la longitude est dans l'h√©misph√®re ouest
        lon_value = -lon_value

    return round(lat_value, 2), round(lon_value, 2)

def drop_tables(conn, schema_name, drop_schema=False, table_name_filter=None):
    try:
        # Cr√©er un inspecteur pour v√©rifier les sch√©mas
        inspector = inspect(conn)
        schemas = inspector.get_schema_names()
        
        if schema_name not in schemas:
            print(f"Schema '{schema_name}' does not exist.")
            return
        
        # Cr√©er un objet metadata et charger toutes les tables
        metadata = MetaData(bind=conn)
        metadata.reflect(schema=schema_name)
        
        # Obtenir la liste des tables du sch√©ma
        print(f"Fetching tables from schema '{schema_name}'...")
        tables = metadata.tables
        
        # Filtrer les tables si un filtre de nom est fourni
        if table_name_filter:
            filtered_tables = {name: table for name, table in tables.items() if table_name_filter.lower() in name.lower()}
            if not filtered_tables:
                print(f"No tables found matching the filter '{table_name_filter}' in schema '{schema_name}'.")
                return
            print(f"Tables matching the filter '{table_name_filter}':\n{filtered_tables.keys()}")
        else:
            filtered_tables = tables
            print(f"Tables found in schema '{schema_name}':\n{filtered_tables.keys()}")
        
        # V√©rifier les verrous existants avant de continuer
        print("Checking for existing locks...")
        query_locks = """
            SELECT
                pg_stat_activity.pid,
                pg_stat_activity.state,
                pg_locks.mode,
                pg_class.relname,
                pg_stat_activity.query
            FROM
                pg_stat_activity
            JOIN
                pg_locks ON pg_stat_activity.pid = pg_locks.pid
            JOIN
                pg_class ON pg_locks.relation = pg_class.oid
            WHERE
                pg_stat_activity.state = 'idle in transaction';
        """
        locks = pd.read_sql(query_locks, conn)
        
        if not locks.empty:
            print(f"Found active locks:\n{locks}")
            print("Waiting 5 seconds before continuing...")
            time.sleep(5)  # Attendre 5 secondes avant de tenter la suppression des tables
        
        # Supprimer les tables une par une dans des transactions distinctes
        with conn.begin():  # Utilisation du bloc `with conn` pour garantir la gestion des transactions
            for table_name, table in filtered_tables.items():
                try:
                    # Supprimer la table
                    print(f"\nDropping table '{table_name}'...")
                    conn.execute(text(f'DROP TABLE IF EXISTS "{schema_name}"."{table_name}" CASCADE'))
                    print(f"Table '{table_name}' dropped.")
                    time.sleep(1)  # D√©lai d'une seconde entre chaque suppression
                except Exception as e:
                    print(f"Error dropping table '{table_name}': {e}")

            # Si l'argument drop_schema est True, supprimer √©galement le sch√©ma
            if drop_schema:
                print(f"Dropping schema '{schema_name}'...")
                conn.execute(text(f'DROP SCHEMA IF EXISTS "{schema_name}" CASCADE'))
                print(f"Schema '{schema_name}' and all its objects have been dropped.")
            else:
                print(f"\nTables dropped from schema '{schema_name}', but schema not removed.")

    except Exception as e:
        print(f"Error dropping tables in schema '{schema_name}': {e}")

def count_files_in_directory(output_dir):
    try:
        # V√©rifier si le dossier existe
        if not os.path.exists(output_dir):
            print(f"Le dossier {output_dir} n'existe pas.")
            return
        
        # Liste des fichiers dans le dossier
        files = [f for f in os.listdir(output_dir) if os.path.isfile(os.path.join(output_dir, f))]
        
        # Si le dossier est vide
        if not files:
            print(f"Aucun fichier trouv√© dans le dossier {output_dir}.")
            return
        
        print(f"Nombre de fichiers dans le dossier '{output_dir}': {len(files)}\n")
        
        # Analyser chaque fichier
        for file in files:
            file_path = os.path.join(output_dir, file)
            file_name, file_extension = os.path.splitext(file)
            
            # V√©rifier si c'est un fichier CSV
            if file_extension.lower() == '.csv':
                try:
                    # Lire le fichier CSV avec pandas
                    df = pd.read_csv(file_path)
                    num_rows, num_cols = df.shape
                    
                    # Afficher les informations sur le fichier
                    print(f"Nom du fichier: {file_name}")
                    print(f"Format: {file_extension}")
                    print(f"Nombre de lignes: {num_rows}, Nombre de colonnes: {num_cols}\n")
                except Exception as e:
                    print(f"Erreur lors de la lecture du fichier {file}: {e}")
            else:
                print(f"Fichier {file} n'est pas un fichier CSV.\n")
    
    except Exception as e:
        print(f"Erreur dans la fonction count_files_in_directory: {e}")

def get_day_time(col):
    # Extraire l'heure et le mois directement de l'objet Timestamp
    hour = col.hour
    
    # Initialiser une variable pour le moment de la journ√©e
    if 0 <= hour < 6:
        daytime = 'Night'
    elif 6 <= hour < 12:
        daytime = 'Morning'
    elif 12 <= hour < 18:
        daytime = 'Afternoon'
    elif 18 <= hour < 24:
        daytime = 'Evening'
    
    # Extraire le mois directement
    month = col.month
    
    # Retourner un tuple avec le moment de la journ√©e et le mois
    return daytime, month

def process_datetime_column(df, column):
    # Convertir la colonne en cha√Æne de caract√®res au tout d√©but
    df[column] = df[column].astype(str)
    print(f"üìå La colonne '{column}' est maintenant convertie en cha√Æne de caract√®res.")

    # Essayer de convertir la colonne en datetime en for√ßant la conversion avec errors='coerce'
    try:
        df[column] = pd.to_datetime(df[column], errors='coerce', utc=True)
        print(f"üìå Conversion r√©ussie de '{column}' en datetime.")

        # Renommer la colonne AVANT d'appliquer floor()
        df.rename(columns={column: 'Datetime'}, inplace=True)
        print('‚úÖ Successfully renamed column to "Datetime"!')

        # Appliquer l'arrondi sur la nouvelle colonne renomm√©e et supprimer le fuseau horaire
        df['Datetime'] = df['Datetime'].dt.floor('H').dt.tz_localize(None)
        
    except Exception as e:
        print(f"üö® ERREUR lors de la conversion de '{column}' : {e}")
    
    return df  # Toujours retourner le DataFrame modifi√©

def clean_dataframe(df, cols_to_convert, verbose=False):

    # Faire une copie du DataFrame pour √©viter les modifications sur une vue
    df = df.copy()

    # Supprimer les colonnes 100% vides
    df.dropna(axis=1, how="all", inplace=True)

    # Conversion des colonnes en float
    for col in cols_to_convert:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")
    
            # Remplacement des NaN par des valeurs adapt√©es
            df.fillna(df.median(numeric_only=True), inplace=True)

    # Cas sp√©cifiques : visibilit√© et temp√©ratures
    if "visibility" in df.columns:
        df["visibility"].fillna(df["visibility"].mean(), inplace=True)

    if "water_level_above_mean" in df.columns:
        df["water_level_above_mean"].fillna(0, inplace=True)

    if "air_temperature" in df.columns:
        df["air_temperature"].fillna(df["air_temperature"].median(), inplace=True)

    if "water_temperature" in df.columns:
        df["water_temperature"].fillna(df["water_temperature"].median(), inplace=True)

    # V√©rification finale
    if verbose:
        if df.isnull().sum().sum() > 0:
            print("‚ö†Ô∏è Il reste encore des NaN !")
        else:
            print("‚úÖ Toutes les valeurs ont √©t√© remplac√©es avec succ√®s !")
        print(df.dtypes)

    return df

def convert_columns_to_numeric(df, cols_to_convert):


    df = df.copy()  # Ne pas modifier l'original

    print("Columns before conversion:", df.columns)

    for col in cols_to_convert:
        if col in df.columns:
            try:
                # Essayer de convertir la colonne en num√©rique
                df[col] = pd.to_numeric(df[col], errors="raise")  # 'raise' l√®ve une erreur si la conversion √©choue
            except ValueError:
                print(f"‚ö†Ô∏è Impossible de convertir la colonne {col}, elle reste inchang√©e.")

    # Si 'is_day' existe, convertissons-le en entier proprement
    if "is_day" in df.columns:
        try:
            df["is_day"] = pd.to_numeric(df["is_day"], errors="raise").astype(int)
        except ValueError:
            print("‚ö†Ô∏è Impossible de convertir 'is_day' en entier, elle reste inchang√©e.")

    print("Columns after conversion:", df.columns)
    return df

def display_row_values(df):
    # Calculer la largeur maximale pour chaque colonne afin de bien aligner les valeurs
    column_widths = [max(df[col].astype(str).apply(len).max(), len(col)) for col in df.columns]

    # Afficher les noms des colonnes, en tenant compte des largeurs maximales
    column_headers = [col.ljust(column_widths[i]) for i, col in enumerate(df.columns)]
    print("  |  ".join(column_headers))
    print("-" * (sum(column_widths) + (len(df.columns) - 1) * 4))  # Ligne de s√©paration

    # Afficher les 10 premi√®res valeurs sous chaque colonne, align√©es
    for i in range(min(10, len(df))):  # Affiche jusqu'√† 10 lignes ou le nombre de lignes disponibles
        row_values = [str(df.iloc[i, col]).ljust(column_widths[col]) for col in range(len(df.columns))]
        print("  |  ".join(row_values))

def rename_columns(df, columns):
    # Assurez-vous que l'entr√©e est bien un DataFrame
    if not isinstance(df, pd.DataFrame):
        raise ValueError("Input must be a pandas DataFrame.")
    
    # Si `columns` est un dictionnaire, le convertir en une liste de dictionnaires pour un traitement uniforme
    if isinstance(columns, dict):
        columns = [columns]  # Convertir en liste de dictionnaires pour uniformit√©
    
    # Parcourir chaque dictionnaire de renommage
    if isinstance(columns, list):
        for rename_dict in columns:
            # Cr√©er un dictionnaire de colonnes existantes √† renommer
            existing_columns = {col: rename_dict[col] for col in rename_dict if col in df.columns}
            
            if existing_columns:
                # Renommer les colonnes si elles existent dans le DataFrame
                for old_name, new_name in existing_columns.items():
                    print(f"üîÑ Colonne '{old_name}' renomm√©e en '{new_name}'")
                
                df.rename(columns=existing_columns, inplace=True)
                print(f"‚úÖ Colonnes renomm√©es : {existing_columns}")
            else:
                print(f"‚ö†Ô∏è Aucune colonne √† renommer pour ce dictionnaire : {rename_dict}")
    
    return df




